[{"authors":["admin"],"categories":null,"content":"About me As of September 2018, I am an assistant professor at the Econometric Institute of the Erasmus University Rotterdam. I became a candidate fellow of the Tinbergen Institute in May 2019.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://HannoReuvers.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"About me As of September 2018, I am an assistant professor at the Econometric Institute of the Erasmus University Rotterdam. I became a candidate fellow of the Tinbergen Institute in May 2019.","tags":null,"title":"Hanno Reuvers","type":"authors"},{"authors":[],"categories":null,"content":"Schematic representation Some neural network notation will be introduced using Figure 1. This schematic representation visualizes how the three inputs $x_1$, $x_2$ and $x_3$ (on the far left) are propagated through a series of layers and create the three outputs $\\hat{y}_1$, $\\hat{y}_2$ and $\\hat{y}_3$ (on the far right). The neurons of the input layer contain input features only. As this layer is typically uncounted, Figure 1 shows a neural network of three layers. A superscript ``$[\\ell]$\u0026rsquo;\u0026rsquo; will be used to refer to quantities in the $\\ell$th layer. We will subsequently discuss the connections between the layers of the network.\nFigure 1: A simplified neural network with $L=3$ layers. For this particular layout of the neural network we have: $n^{[0]}=3$, $n^{[1]}=5$, $n^{[2]}=2$, and $n^{[L]}=3$. The value of neuron $a_1^{[1]}$ has similarities to a logistic regression. The three incoming arrows from $x_1$, $x_2$ and $x_3$ tell us to linearly combine these three inputs and to add the bias term $b_1^{[1]}$.1 Stacking $\\mathbf{x}=(x_1,x_2,x_3)^\\prime$ and defining the weight vector $\\mathbf{w}_{1}^{[1]}=(w_{11}^{[1]}, w_{12}^{[1]},w_{13}^{[1]})^\\prime$, the linear combination is expressed compactly as $z_1^{[1]}=\\mathbf{w}_{1}^{[1]\\prime}\\mathbf{x}+b_1^{[1]}$. Applying the sigmoid function $\\Lambda(x)=\\frac{1}{1+\\exp(-x)}$, the result is $a_1^{[1]}=\\Lambda(z_1^{[1]})$. The other neurons in hidden layer 1 are computed in the same way. Each neuron will receive its own weight vector and bias. For $i=1,2,\\ldots,5$, we have $z_i^{[1]}=\\mathbf{w}_i^{[1]\\prime}\\mathbf{x}+ b_i^{[1]}$ and $a_i^{[1]}=\\Lambda(z_i^{[1]})$.\nHidden layer 2 takes the values from the neurons in hidden layer 1 as inputs. For ease of notation, define $\\mathbf{a}^{[1]}=(a_1^{[1]},a_2^{[1]},\\ldots,a_5^{[1]})$. To compute $a_1^{[2]}$, calculate $z_1^{[2]}=\\mathbf{w}_{1}^{[2]\\prime}\\mathbf{a}^{[1]}+b_1^{[2]}$ and $a_1^{[2]}=\\Lambda(z_1^{[2]})$. Similarly, $a_2^{[2]}=\\Lambda(z_2^{[2]})$ with $z_2^{[2]}=\\mathbf{w}_{2}^{[2]\\prime}\\mathbf{a}^{[1]}+b_2^{[2]}$.\nThe two-step computation for the output layer is somewhat different. As a start, we can still calculate the usual linear combinations $$ \\begin{aligned} z_1^{[3]} \u0026amp;=\\mathbf{w}_{1}^{[3]\\prime}\\mathbf{a}^{[2]}+b_1^{[3]}, \\\\ \u0026amp; \\vdots \\\\ z_3^{[3]} \u0026amp;=\\mathbf{w}_{3}^{[3]\\prime}\\mathbf{a}^{[2]}+b_3^{[3]}, \\end{aligned} $$ with $\\mathbf{a}^{[2]}=(a_1^{[2]},a_2^{[2]})$. The subsequent transformation however should provide outcomes matching the type of data that is being modeled. For example, if $\\hat{y}_1,\\hat{y}_2,\\hat{y}_3 \\in \\mathbb{R}$ (a multivariate regression), then no subsequent transformation is needed and we can simply use $\\hat{y}_1=z_1^{[3]}$, $\\hat{y}_2=z_2^{[3]}$ and $\\hat{y}_3=z_3^{[3]}$. We pursue this case in the main text of this post. If the neural network output is used for classification, then a softmax output layer is more appropriate (Appendix C). What are the benefits of neural networks? The main advantage of a neural network is its flexible link between input features and output variable(s). That is, even highly nonlinear relationships in the data can be modelled throught these subsequent cascades of (1) linear combinations and (2) nonlinear transforms (through the sigmoid function).2 This is particularly useful in data-rich environments when parametric relationship are just too restrictive.\nTraining a neural network The values of the weight vectors and bias terms are still to be selected. We perform this selection using a cost function $\\mathcal{C}$. This cost function quantifies the quality of a particular choice of weights and bias values. A higher cost indicates a worse choice of parameters. The evaluation of the cost function happens in the forward-propagation step. Backward-propagation suggests the direction in which the cost function is lower.\nForward-propagation The forward propagation step evaluates the cost function $\\mathcal{C}$. The procedure becomes more concise if we use matrix notation. For each layer $\\ell=1,\\ldots,L$, we stack the results for each neuron: $$ \\mathbf{z}^{[\\ell]} = \\begin{bmatrix} z_1^{[\\ell]} \\\\ \\vdots \\\\ z_{n^{[\\ell]}}^{[\\ell]} \\end{bmatrix} , \\mathbf{b}^{[\\ell]} = \\begin{bmatrix} b_1^{[\\ell]} \\\\ \\vdots \\\\ b_{n^{[\\ell]}}^{[\\ell]} \\end{bmatrix} , \\mathbf{a}^{[\\ell]} = \\begin{bmatrix} a_1^{[\\ell]} \\\\ \\vdots \\\\ a_{n^{[\\ell]}}^{[\\ell]} \\end{bmatrix} ,\\text{and } \\mathbf{W}^{[\\ell]} = \\begin{bmatrix} \\mathbf{w}_1^{[\\ell]\\prime} \\\\ \\vdots \\\\ \\mathbf{w}_{n^{[\\ell]}}^{[\\ell]\\prime}, \\end{bmatrix} $$ where $n^{[\\ell]}$ denotes the number of neurons in layer $\\ell$. If we agree to apply the function $\\Lambda$ elements-wise to vectors, then all computations of the previous section can be summarized by $$ \\begin{aligned} \\mathbf{z}^{[1]} \u0026amp;= \\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b}^{[1]}, \u0026amp;\\mathbf{a}^{[1]} = \\Lambda(\\mathbf{z}^{[1]}), \\\\ \\mathbf{z}^{[2]} \u0026amp;= \\mathbf{W}^{[2]} \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}, \u0026amp;\\mathbf{a}^{[2]} = \\Lambda(\\mathbf{z}^{[2]}), \\\\ \\hat{\\mathbf{y}} = \\mathbf{z}^{[3]} \u0026amp;= \\mathbf{W}^{[3]} \\mathbf{a}^{[2]} + \\mathbf{b}^{[3]}. \\end{aligned} \\label{eq:NNlayout} \\tag{1} $$ It remains to compare the neural network output to the actual data. We let $\\mathbf{y}=(y_1,y_2,y_3)^\\prime$ denote the vector of observed outcomes. A possible cost function for this single-observation setting is the squared error $\\| \\hat{\\mathbf{y}} - \\mathbf{y} \\|^2$.\nIn practice, we have multiple observations to determine $\\mathbf{W}^{[1]}$, $\\mathbf{W}^{[2]}$, $\\mathbf{W}^{[3]}$, $\\mathbf{b}^{[1]}$, $\\mathbf{b}^{[2]}$, and $\\mathbf{b}^{[3]}$. We assume a sample size of $T$ observations and enumerate the input-output pairs as $(\\mathbf{x}^{(1)},\\mathbf{y}^{(1)}),\\ldots,(\\mathbf{x}^{(T)},\\mathbf{y}^{(T)})$. For each $t\\in\\{1,\\ldots,T\\}$, we now propagate the input $\\mathbf{x}^{(t)}$ through the recursions in \\eqref{eq:NNlayout} and find the implied prediction $\\hat{\\mathbf{y}}^{(t)}$. The cost function that takes all the samples into account is the Mean Square Error (MSE) defined as $$ MSE = \\frac{1}{T} \\sum_{t=1}^T \\|\\hat{\\mathbf{y}}^{(t)} -\\mathbf{y}^{(t)} \\|^2. $$\nNOTE: Instead of iterating through \\eqref{eq:NNlayout} for each $\\mathbf{x}^{(t)}$, a more efficient algorithm vectorizes this process. For example, after stacking $\\mathbf{X}=[\\mathbf{x}^{(1)} \\cdots \\mathbf{x}^{(T)}] \\in \\mathbb{R}^{3\\times T}$, the matrix product $\\mathbf{W}^{[1]}\\mathbf{X}$ will immediately hold $\\mathbf{W}^{[1]}\\mathbf{x}^{(t)}$ in its $t$th column.\nBackward-propagation The learning in the neural network is nothing but a series of successively better updates for $\\mathbf{W}^{[1]}$, $\\mathbf{W}^{[2]}$, $\\mathbf{W}^{[3]}$, $\\mathbf{b}^{[1]}$, $\\mathbf{b}^{[2]}$, and $\\mathbf{b}^{[3]}$. A popular method to perform these updates is gradient descent. Using a subscript ``$\\{k\\}$\u0026rsquo;\u0026rsquo; to denote the $k$th parameter update, we perform:\nInitialize $\\mathbf{W}_{\\{0\\}}^{[1]}$, $\\mathbf{W}_{\\{0\\}}^{[2]}$, $\\mathbf{W}_{\\{0\\}}^{[3]}$, $\\mathbf{b}_{\\{0\\}}^{[1]}$, $\\mathbf{b}_{\\{0\\}}^{[2]}$, and $\\mathbf{b}_{\\{0\\}}^{[3]}$ (more details later). For $\\ell=1,\\ldots,3$, update the parameters according to \\begin{equation} \\begin{aligned} \\mathbf{W}_{\\{k+1\\}}^{[\\ell]} \u0026amp;= \\mathbf{W}_{\\{k\\}}^{[\\ell]} - \\alpha \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{W}^{[\\ell]}} \\\\ \\mathbf{b}_{\\{k+1\\}}^{[\\ell]} \u0026amp;= \\mathbf{b}_{\\{k\\}}^{[\\ell]} - \\alpha \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{b}^{[\\ell]}} \\end{aligned} \\label{eq:GradientDescent} \\tag{2} \\end{equation} where The learning rate $\\alpha$ is a hyperparameter to be set by the researcher. The terms $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{W}^{[\\ell]}}$ and $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{b}^{[\\ell]}}$ are the gradients of the cost function with respect to the weight matrices and bias vectors, respectively (see Appendix A for explanations on matrix and vector derivatives). The gradients in \\eqref{eq:GradientDescent} are evaluated at the parameter values of iteration $k$. Stop updating upon convergence. Quantitative convergence criteria are difficult to formulate because we are generally not trying to find the global minimum of the cost function. The neural network is typically heavily parametrised and the global minimum is thus likely to overfit the data.3 We want to continue decreasing the training cost function as long as this also leads to decreases in the validation cost function. The crucial components in \\eqref{eq:GradientDescent} are $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{W}^{[\\ell]}}$ and $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{b}^{[\\ell]}}$ ($\\ell=1,2,3$). These gradients are computed during backward-propagation. We will work our way backwards and start with the derivatives with respect to $\\mathbf{W}^{[3]}$ and $\\mathbf{b}^{[3]}$. For brevity, a single feature vector $\\mathbf{x}$ is propagated as in \\eqref{eq:NNlayout} to produce a single output vector $\\hat{\\mathbf{y}}$. The cost function is the squared error.\nThe cost function $\\mathcal{C}=\\| \\mathbf{z}^{[3]} - \\mathbf{y} \\|^2$ depends on $\\mathbf{W}^{[3]}$ and $\\mathbf{b}^{[3]}$ through $\\mathbf{z}^{[3]} = \\mathbf{W}^{[3]} \\mathbf{a}^{[2]} + \\mathbf{b}^{[3]}$. We will have to use the chain rule. First, write $\\mathcal{C}= \\sum_{i=1}^{[n^L]} (z_{i}^{[3]}-y_i)^2$, calculate \\begin{equation} \\frac{\\partial \\mathcal{C}}{\\partial z_i^{[3]}} = 2(z_i^{[3]}-y_i) = \\left[ 2 (\\mathbf{z}^{[3]} - \\mathbf{y}) \\right]_i \\end{equation} and conclude that $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[3]}}= 2 (\\mathbf{z}^{[3]} - \\mathbf{y})$. The gradient with respect to $\\mathbf{b}^{[3]}$ is also $2 (\\mathbf{z}^{[3]} - \\mathbf{y})$ because $$ \\begin{aligned} \\frac{\\partial \\mathcal{C}}{\\partial b_i^{[3]}} \u0026amp;= \\sum_{j=1}^{[n^L]} \\frac{\\partial \\mathcal{C}}{\\partial z_j^{[3]}} \\frac{\\partial z_j^{[3]}}{\\partial b_i^{[3]}} = \\sum_{j=1}^{[n^L]} \\frac{\\partial \\mathcal{C}}{\\partial z_j^{[3]}} \\frac{\\partial}{\\partial b_i^{[3]}} \\left(\\mathbf{w}_{j}^{[3]\\prime} \\mathbf{a}^{[2]}+b_j^{[3]}\\right) \\\\ \u0026amp;= \\sum_{j=1}^{[n^L]} \\frac{\\partial \\mathcal{C}}{\\partial z_j^{[3]}} \\delta_{ij} = \\frac{\\partial \\mathcal{C}}{\\partial z_i^{[3]}} = \\left[ 2 (\\mathbf{z}^{[3]} - \\mathbf{y}) \\right]_i \\end{aligned} $$ where we used the Kronecker delta \\begin{equation} \\delta_{ij} = \\begin{cases} 1 \u0026amp; \\quad \\text{if } i=j,\\\\ 0 \u0026amp; \\quad{ otherwise.} \\end{cases} \\end{equation} The calculation of $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{W}^{[3]}}$ is somewhat similar. Relabeling the summation index to avoid confusion with the subscripts of $w_{ij}$, we find $$ \\begin{aligned} \\frac{\\partial \\mathcal{C}}{\\partial w_{ij}^{[3]}} \u0026amp;= \\sum_{m=1}^{[n^L]} \\frac{\\partial \\mathcal{C}}{\\partial z_m^{[3]}} \\frac{\\partial z_m^{[3]}}{\\partial b_i^{[3]}} = \\sum_{m=1}^{[n^L]} \\frac{\\partial \\mathcal{C}}{\\partial z_m^{[3]}} \\frac{\\partial}{\\partial w_{ij}^{[3]}} \\left(\\mathbf{w}_{m}^{[3]\\prime} \\mathbf{a}^{[2]}+b_m^{[3]}\\right) \\\\ \u0026amp;= \\sum_{m=1}^{[n^L]} \\frac{\\partial \\mathcal{C}}{\\partial z_m^{[3]}} \\delta_{im} a_j^{[2]} = \\frac{\\partial \\mathcal{C}}{\\partial z_i^{[3]}} a_j^{[2]} = \\left[ \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[3]}} \\mathbf{a}^{[2]\\prime} \\right]_{ij}. \\end{aligned} $$ For the parameters in layer 3, we find the following gradients: $$ \\begin{aligned} \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{b}^{[3]}} = \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[3]}} \u0026amp;= 2 (\\mathbf{z}^{[3]} - \\mathbf{y}), \\\\ \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{W}^{[3]}} \u0026amp;= \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[3]}} \\mathbf{a}^{[2]\\prime}. \\end{aligned} $$\nMoving back one more layer, we are in layer 2. Four equations now determine the gradients with respect to $\\mathbf{W}^{[2]}$ and $\\mathbf{b}^{[2]}$, namely: $$ \\begin{aligned} \\mathcal{C} \u0026amp;=\\| \\mathbf{z}^{[3]} - \\mathbf{y} \\|^2, \\\\ \\mathbf{z}^{[3]} \u0026amp;= \\mathbf{W}^{[3]} \\mathbf{a}^{[2]} + \\mathbf{b}^{[3]}, \\\\ \\mathbf{a}^{[2]} \u0026amp;= \\Lambda(\\mathbf{z}^{[2]})\\\\ \\mathbf{z}^{[2]} \u0026amp;= \\mathbf{W}^{[2]} \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}. \\end{aligned} $$ Because we derived $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[3]}}$ before, we can continue with $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{a}^{[2]}}$. Defining $w_{ij}^{[3]}$ as the $(i,j)$th element of $\\mathbf{W}^{[3]}$, the chain rule implies $$ \\begin{aligned} \\frac{\\partial \\mathcal{C}}{\\partial a_i^{[2]}} \u0026amp;= \\sum_{j=1}^{[n^L]} \\frac{\\partial \\mathcal{C}}{\\partial z_j^{[3]}} \\frac{\\partial z_j^{[3]}}{\\partial a_i^{[2]}} = \\sum_{j=1}^{[n^L]} \\frac{\\partial \\mathcal{C}}{\\partial z_j^{[3]}} \\frac{\\partial}{\\partial a_i^{[2]}} \\left(\\mathbf{w}_{j}^{[3]\\prime} \\mathbf{a}^{[2]}+b_j^{[3]}\\right) \\\\ \u0026amp;= \\sum_{j=1}^{[n^L]} \\frac{\\partial \\mathcal{C}}{\\partial z_j^{[3]}} w_{ji}^{[3]} = \\left[ \\mathbf{W}^{[3]\\prime} \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[3]}}\\right]_i. \\end{aligned} $$ We subsequently need to take care of $\\mathbf{a}^{[2]}= \\Lambda(\\mathbf{z}^{[2]})$. This transformation is elements-wise implying that $a_i^{[2]}=\\Lambda(z_{i}^{[2]})$ for $i\\in\\{1,2\\}$ and thus $$ \\begin{aligned} \\frac{\\partial \\mathcal{C}}{\\partial z_i^{[2]}} \u0026amp;= \\frac{\\partial \\mathcal{C}}{\\partial a_i^{[2]}} \\frac{\\partial a_i^{[2]}}{\\partial z_i^{[2]}} = \\frac{\\partial \\mathcal{C}}{\\partial a_i^{[2]}} \\Lambda\u0026rsquo;(z_i^{[2]}) \\\\ \u0026amp;= \\frac{\\partial \\mathcal{C}}{\\partial a_i^{[2]}} \\Lambda(z_i^{[2]})\\big(1-\\Lambda(z_i^{[2]}) \\big), \\end{aligned} \\label{eq:NonlinearTransformGradient} \\tag{3} $$ where we used $\\Lambda\u0026rsquo;(x)=\\Lambda(x)\\big(1-\\Lambda(x)\\big)$ (see this post on logistic regression). In matrix notation, we can use the Hadamard product (i.e. the elements-wise multiplication symbol \u0026ldquo;$\\odot$\u0026rdquo;) to express \\eqref{eq:NonlinearTransformGradient} as $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[2]}} = \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{a}^{[2]}} \\odot \\Lambda(\\mathbf{z}^{[2]})\\odot \\big(\\mathbf{1}-\\Lambda(\\mathbf{z}^{[2]})\\big)$. Having computed the gradient with respect to $\\mathbf{z}^{[2]}$, the calculations for $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{b}^{[2]}}$ and $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{W}^{[2]}}$ are exactly as in layer 3 because only another linear transformation $\\mathbf{z}^{[2]} = \\mathbf{W}^{[2]} \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}$ remains. For the parameters in layer 2, we find the following gradients: $$ \\begin{aligned} \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{a}^{[2]}} \u0026amp;= \\mathbf{W}^{[3]\\prime} \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[3]}}, \\\\ \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{b}^{[2]}}=\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[2]}} \u0026amp;= \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{a}^{[2]}} \\odot \\Lambda(\\mathbf{z}^{[2]})\\odot \\big(\\mathbf{1}-\\Lambda(\\mathbf{z}^{[2]})\\big), \\\\ \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{W}^{[2]}} \u0026amp;= \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[2]}} \\mathbf{a}^{[1]\\prime}. \\end{aligned} $$\nLayer 1 is a fully-connected layer just like layer 2. Because the mathematical structure is identical to layer 2, the gradients follow immediately. We should only realized that the input variables to the first layer are $\\mathbf{a}^{[0]}=\\mathbf{x}$. For the parameters in layer 1, we find the following gradients: $$ \\begin{aligned} \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{a}^{[1]}} \u0026amp;= \\mathbf{W}^{[2]\\prime} \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[2]}}, \\\\ \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{b}^{[1]}}=\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[1]}} \u0026amp;= \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{a}^{[1]}} \\odot \\Lambda(\\mathbf{z}^{[1]})\\odot \\big(\\mathbf{1}-\\Lambda(\\mathbf{z}^{[1]})\\big), \\\\ \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{W}^{[1]}} \u0026amp;= \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[1]}} \\mathbf{a}^{[0]\\prime}= \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{z}^{[1]}} \\mathbf{x}^{\\prime}. \\end{aligned} $$\nNOTE: Our single-observation derivations generalize easily to a multiple-observation setting due to linearity of derivatives. Consider the input-output pairs $(\\mathbf{x}^{(1)},\\mathbf{y}^{(1)}),\\ldots,(\\mathbf{x}^{(T)},\\mathbf{y}^{(T)})$ and $MSE = \\frac{1}{T} \\sum_{t=1}^T \\|\\hat{\\mathbf{y}}^{(t)} -\\mathbf{y}^{(t)} \\|^2$. The gradient of the MSE with respect to say $\\mathbf{W}^{[1]}$ is $$ \\frac{\\partial}{\\partial \\mathbf{W}^{[1]}} MSE = \\frac{1}{T}\\sum_{T=1}^T\\frac{\\partial}{\\partial \\mathbf{W}^{[1]}}\\|\\hat{\\mathbf{y}}^{(t)} -\\mathbf{y}^{(t)} \\|^2. $$ The overall gradient is clearly just the average of all single-observation gradients. Theoretically, we can thus run the back-propagation algorithm for each observation and average. A practical implementation vectorizes this process to leverage the performance benefits of linear algebra libraries.\nRemarks It is standard to initialize the gradient descent algorithm with $\\mathbf{b}^{[1]}=\\mathbf{b}^{[2]}=\\mathbf{b}^{[3]}=\\mathbf{0}$. This zero initialization should not be used for the weight matrix. Such a choice would imply $\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{a}^{[2]}}=\\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{a}^{[1]}}=\\mathbf{0}$ and all gradients would be zero. Gradient descent would not update any parameter. It is instead common practice to initialize the weight matrices with (small) random numbers. Concrete examples are the heuristic and normalized initializations in (1) and (16) of Glorot and Bengio (2010). The learning rate $\\alpha$ is specified by the user. Common values are 0.1, 0.01, and 0.001. Determining a good choice for $\\alpha$ typically requires some trial-and-error. If the learning rate is too high, then the cost function can oscillate strongly and even fail to converge. If the learning rate is too low, then cost function improvements are small and many iterations are needed. The nonlinear functional transform in our neural network was the sigmoid function $\\Lambda(x)=\\frac{1}{1+\\exp(-x)}$. Other activation functions can be used (Table 1). An inspection of the derivations shows that a change in activation function only impacts the backward propagation algorithm through \\eqref{eq:NonlinearTransformGradient}. That is, if the new relationship is $\\mathbf{a}^{[2]}=g(\\mathbf{z}^{[2]})$, then \\eqref{eq:NonlinearTransformGradient} becomes $\\frac{\\partial \\mathcal{C}}{\\partial z_i^{[2]}} = \\frac{\\partial \\mathcal{C}}{\\partial a_i^{[2]}} \\frac{\\partial a_i^{[2]}}{\\partial z_i^{[2]}} = \\frac{\\partial \\mathcal{C}}{\\partial a_i^{[2]}} g\u0026rsquo;(z_i^{[2]})$. Table 1: Several activation functions and their gradients. The derivative of the rectified linear unit is undefined at $x=0$ but the non-existence of the derivative at this single isolated point is usually ignored. Several extensions of the baseline gradient descent algorithm have been proposed. Two of the most popular examples are: The gradient descent updates can be smoothed across iterations. This can dampen oscilations and thus allow for higher learning rates (read: faster convergence). An example is the Adam optimizer by Kingma and Ba (2015) which keeps track of an exponential moving average of the gradient and squared gradient to smoothen the parameter update. Mini batch gradient descent is particularly popular on larger datasets. With gradients being averages of the single-observation gradients, the gradient based on say 1,000 observations will already give a good indication of the direction of steepest descent. It is thus natural to update the parameters after evaluating the gradient based on a subset of the data (a mini batch). This effectively increases the parameter update speed. Small illustration We use a neural network to classify the artificial data shown in Figure 2. The input feature vector $\\mathbf{x}$ is 2-dimensional and there are three categories (the letter H, the letter R, and the background). This simulation design clearly generates a highly nonlinear mapping from input features to labels and a flexible classifier is thus needed. This justifies the use of a neural network. The details of the neural network implementation are listed in Table 2.\nFigure 2: The artifical data for the neural network illustration with (a) training data (5,000 observations), (b) validation data (1,000 observations), and (c) test data (1,000 observations). Table 2: The neural network architecture. We initialize the weight matrices according to the ``normalized initialization\u0026rsquo;\u0026rsquo; in Glorot and Bengio (2010) and start all bias vectors as $\\boldsymbol{0}$. We subsequently use gradient descent with a learning rate of 0.2 to update the 2,853 parameters. In combination with the hyperbolic tangent activation functions, this results in a steady decrease of the cost function (Figure 3). The decrease is nearly monotonic except for a temporary spike in the cost function around learning epoch 1,260. Further debugging shows that this coincides with a sudden large increases of points in the left of the figure being classified as belonging to the blue category. This overshoot is corrected in subsequent iterations.\nThe learning process is further illustrated in Figure 4. The first 2,000 learning epochs provide a rather blurry outline of the letters H and R but subsequent parameter updates add more and more detail. As there are only tiny improvements during learning epochs 6,000 to 8,000 (both in the decision regions and in the cost function of Figure 3), we stop the learning process after 8,000 epochs.4\nFigure 3: The training cost as a function of learning epoch. Figure 4: The evolution of the decision regions (as determined by majority voting) for increasing number of learning epochs. Finally, we verify the performance on the test data set. The overall performance is satisfactory with an accuracy of 93.9%. The confusion matrix (Table 3) shows that there is little mislabeling between the classes that define the H and R. Most errors are made when distinguishing the letters from the background.\nTable 3: The confusion matrix for the test data of the HR-classification example. The encoding of the labels is: 1 (H; blue), 2 (R; red), and 3 (background, grey). Appendix A: Vector and matrix derivatives As an $m$-dimensional vector can be interpreted as an $m\\times 1$ matrix, it suffices to center this discussion around matrix derivatives. Defining the $m\\times n$ matrix $\\mathbf{A}$, \\begin{equation} \\mathbf{A} =\\begin{bmatrix} a_{11} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} \\end{equation} and the function $f:\\mathbb{R}^{m\\times n}\\to\\mathbb{R}$, the matrix derivative $\\frac{\\partial f}{\\partial \\mathbf{A}}\\in \\mathbb{R}^{m\\times n}$ is defined as \\begin{equation} \\frac{\\partial f}{\\partial \\mathbf{A}} =\\begin{bmatrix} \\frac{\\partial f}{\\partial a_{11}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial a_{1n}} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f}{\\partial a_{m1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial a_{mn}} \\end{bmatrix}. \\end{equation} Two consequences of this definition should be stressed: (1) the dimensions of $\\mathbf{A}$ and $\\frac{\\partial f}{\\partial \\mathbf{A}}$ are identical, and (2) the $(i,j)$th element of $\\frac{\\partial f}{\\partial \\mathbf{A}}$ is $\\frac{\\partial f}{\\partial a_{ij}}$. The second property is particularly interesting as it connects the calculation of a matrix derivative to the calculation of an ordinary partial derivative. The general recipe to compute matrix derivatives is therefore as follows:\nCompute $\\frac{\\partial f}{\\partial a_{ij}}$ for arbitrary $i$ and $j$. The matrix derivative $\\frac{\\partial f}{\\partial \\mathbf{A}}$ is the matrix having $\\frac{\\partial f}{\\partial a_{ij}}$ as its $(i,j)$th element. For example, let $f(\\mathbf{A})=\\mathbf{x}^\\prime \\mathbf{A}\\mathbf{x}$ for $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ and some fixed vector $\\mathbf{x}\\in\\mathbb{R}^n$. As $\\mathbf{x}^\\prime \\mathbf{A}\\mathbf{x}= \\sum_{k=1}^n \\sum_{l=1}^n a_{kl} x_k x_l$, we have $\\frac{\\partial f}{\\partial a_{ij}}= x_i x_j$. We conclude that $\\frac{\\partial}{\\partial \\mathbf{A}} \\mathbf{x}^\\prime \\mathbf{A}\\mathbf{x} = \\mathbf{x} \\mathbf{x}^\\prime$, because \\begin{equation} \\left[\\frac{\\partial f}{\\partial \\mathbf{A}} \\right]_{ij}= \\frac{\\partial f}{\\partial a_{ij}} = x_i x_j = \\left[\\mathbf{x} \\mathbf{x}^\\prime \\right]_{ij}. \\end{equation}\nAppendix B: Cross-entropy loss function Consider a random sample $Y_1, Y_2,\\ldots,Y_n$ of categorical data.5 For ease of notation, we will label these categories as $1,2,\\ldots,K$ and denote the corresponding probabilities as $p_1,p_2,\\ldots,p_K$. The associated likelihood is: $$ L(p_1,p_2,\\ldots,p_K) = \\prod_{i=1}^n p_1^{\\mathbb{1}_{ \\{ y_i=1 \\} }} p_2^{\\mathbb{1}_{ \\{ y_i=2 \\} }}\\times \\cdots \\times p_K^{\\mathbb{1}_{ \\{ y_i=K \\} }} $$ with $\\mathbb{1}_{ \\{ \\mathcal{A} \\} }$ denoting the indicator function for condition $\\mathcal{A}$.6 The maximization of the likelihood is equal to the minimization of the cross-entropy loss function (the negative and logarithm of the previous likelihood) $$ -\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{1}_{ \\{ y_i = k \\} } \\log(p_k). $$ The intuition behind this formula is as follows. For any $y_i$, the indicator function inside the summation over $k$ will be equal to 1 exactly once (namely when $k$ equals the class label stored in $y_i$). The contribution of $- \\sum_{k=1}^K \\mathbb{1}_{ \\{ y_i = k \\} } \\log(p_k)$ to the loss function is lowest if the probability for this class is as high as possible.\nAppendix C: Classification output layer We consider a neural network in which the output layer, layer $L$, should act as a classifier for $K$ classes. The combination of softmax activation and cross-entropy evaluation metric is very popular because (1) it works well in practice,7 and (2) it results in concise mathematical expressions for the back-propagation step.\nThe concrete implementation details are as follows. First, we match the number of neurons in the output layer to the number of output classes, i.e. $n^{[L]}=K$. The linear transformation towards the $L$th layer is thus $\\mathbf{z}^{[L]}= \\mathbf{W}^{[L]} \\mathbf{a}^{[L-1]} + \\mathbf{b}^{[L]}$ with $\\mathbf{W}^{[L]}\\in\\mathbb{R}^{K\\times n^{[L-1]}}$ and $\\mathbf{b}\\in\\mathbb{R}^{K}$. Second, we convert $\\mathbf{z}^{[L]}$ into a $K$-dimensional vector of probabilities using the softmax activation. That is, we calculate \\begin{equation} p_i^{[L]} = \\frac{ \\exp(z_i^{[L]}) }{ \\sum_{k=1}^{K} \\exp(z_k^{[L]}) }, \\qquad\\text{for }i=1,2,\\ldots,K, \\label{eq:SoftmaxActivation} \\tag{2} \\end{equation} and stack $\\mathbf{p}^{[L]} = (p_1^{[L]},\\ldots,p_K^{[L]})^\\prime$. The crucial component of \\eqref{eq:SoftmaxActivation} is the use of the exponential function to obtain positive outcomes and the standardization in the denominator to ensure that probabilities add to $1$.\nFinally, let us look at the back-propagation step. For brevity of notation, we will consider a single outcome observation $y$ and omit superscripts referring to the final layer. The cost function and softmax activation are thus $C=-\\sum_{k=1}^K \\mathbb{1}_{ \\{ y = k \\} } \\log(p_k)$ and $p_i = \\exp(z_i) / \\sum_{\\kappa=1}^{K} \\exp(z_\\kappa)$ $(i=1,2,\\ldots,K)$, respectively. We are interested in $\\frac{\\partial C}{\\partial \\mathbf{z}}$.\nMotivated by Appendix A, we derive the $j$th component of $\\frac{\\partial C}{\\partial \\mathbf{z}}$. We have $$ \\begin{aligned} \\frac{\\partial C}{\\partial z_j} \u0026amp;=-\\sum_{k=1}^K \\mathbb{1}_{ \\{ y = k \\} } \\frac{1}{p_k} \\frac{ \\partial p_k}{\\partial z_j} \\\\ \u0026amp;=-\\sum_{k=1}^K \\mathbb{1}_{ \\{ y = k \\} } \\frac{1}{p_k} \\left(\\frac{ \\exp(z_j) }{ \\sum_{\\kappa=1}^{K} \\exp(z_\\kappa) }\\mathbb{1}_{ \\{ j=k \\}} - \\frac{ \\exp(z_k) \\exp(z_j) }{ ( \\sum_{\\kappa=1}^{K} \\exp(z_\\kappa) )^2 } \\right) \\\\ \u0026amp;=-\\sum_{k=1}^K \\mathbb{1}_{ \\{ y = k \\} } \\left(\\mathbb{1}_{ \\{ j=k \\}} - p_j \\right) = p_j- \\mathbb{1}_{ \\{ y=j \\}} . \\end{aligned} $$ In conclusion, if we use one-hot encoding for the outcome $y$, then $\\frac{\\partial C}{\\partial \\mathbf{z}} = \\mathbf{p}-\\mathbf{y}$. Having calculated the derivative with respect to $\\mathbf{z}$, the resulting derivates for $\\mathbf{W}$ and $\\mathbf{b}$ take the usual form.\nReferences X. Glorot and Y. Bengio (2010), Understanding the Difficulty of Training Deep Feedforward Neural Networks, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)\nN.J. Guliyevand V.E. Ismailov (2018), On the Approximation by Single Hidden Layer Feedforward Neural Networks with Fixed Weights, Neural Networks\nD.P. Kingma and J. Ba (2015), Adam: A Method for Stochastic Optimization, arXiv:1412.6980\nS.A. Solla, E. Levin and M. Fleisher (1988), Accelerated Learning in Layered Neural Networks, Complex Systems\nNotes The bias terms are sometimes explicitly included in the graphical representation of the neural network. For improved readability, we omit the biases in Figure 1.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTheoretically, there are many results establishing that a neural network with a single hidden layer can approximate any continuous function on a compact set with arbitrary precision (see Guliyev and Ismailov (2018) and references therein). Empirically, neural networks with several layers work better.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt can easily happen that the amount of parameters exceeds the number of training instances. The global minimum of the cost function could coincide with a perfect fit implying that the neural network is no longer distinguishing signal from noise.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe cost as computed on the validation set is very similar to the graph in Figure 4. That is, this cost is also steadily decreasing until epoch 6,000 and stabilizing afterwards. With 8,000 learning epochs we are not yet over-fitting the data.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn this section we use upper case $Y_1, Y_2,\\ldots,Y_n$ to denote the random variables and lower case $y_1, y_2,\\ldots,y_n$ to denote the observations.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExplicitly, we have $\\mathbb{1}_{ \\{ \\mathcal{A} \\} }=1$ if $\\mathcal{A}$ is true, and $\\mathbb{1}_{ \\{ \\mathcal{A} \\} }=0$ if $\\mathcal{A}$ is false.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSolla, Levin and Fleisher (1988) compares the performance of $L_2$-loss and logarithmic error functions. The authors conclude that a logarithmic loss function has steeper gradients which improves learning. The finding was re-emphasized in Glorot and Bengio (2010).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1658102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658102400,"objectID":"2be81440aeb4567426a457cebc4257b7","permalink":"https://HannoReuvers.github.io/post/neuralnetwork/","publishdate":"2022-07-18T00:00:00Z","relpermalink":"/post/neuralnetwork/","section":"post","summary":"Mathematical details on forward and backward propagation and a small illustration","tags":[],"title":"The math of neural networks","type":"post"},{"authors":[],"categories":null,"content":"Model specification The logistic regression is a popular binary classifier and arguably the elementary building block of a neural network. Specifically, given a feature vector $\\mathbf{x} \\in \\mathbb{R}^p$, the logistic regression models the binary outcome $y\\in \\{0,1\\}$ as \\begin{equation} \\mathbb{P}(y=1 | \\mathbf{x} ; b, \\boldsymbol \\theta) = \\frac{1}{1+ \\exp\\big( -(b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}) \\big)} = : \\Lambda ( b + \\boldsymbol \\theta\u0026rsquo;\\mathbf{x}), \\label{eq:LogisticModel} \\tag{1} \\end{equation} where we introduced the sigmoid function $\\Lambda(x) = \\frac{1}{1+\\exp(-x)}$ in the last equality. The step-wise explanation of the logistic regression is: (1) introduce the parameter vector $\\boldsymbol{\\theta}\\in\\mathbb{R}^p$ to linearly combine the input features into the scalar $\\boldsymbol \\theta\u0026rsquo;\\mathbf{x}$, (2) add the intercept/bias parameter $b$, and (3) transform $b+\\boldsymbol \\theta\u0026rsquo;\\mathbf{x}$ into a probability by guiding it through the sigmoid function.1 Figure 1 clearly shows how the sigmoid function maps any input into the interval (0,1) thereby resulting in a valid probability.\nFigure 1: The sigmoid function $\\Lambda(x)= \\frac{1}{1+\\exp(-x)}$. To provide a geometrical interpretation of the logistic regression, we note that $\\Lambda(x)\\geq0.5$ whenever $x\\geq 0$. If we label an outcome as 1 whenever its probability $\\mathbb{P}(y=1 | \\mathbf{x} ; b, \\boldsymbol \\theta)$ exceeds 0.5 (majority voting), then the label 1 is assigned whenever $b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}\\geq 0$. Two illustrations are provided in Figure 2.\nFigure 2: A dataset of $n=50$ i.i.d. observations with two features ($x_1$ and $x_2$) and outcomes being visualized as 1 (blue circle) or 0 (red circle). The background colours specify the estimated decision regions. (a) A logistic regression using the vector $\\mathbf{x}=[x_1, x_2]^\\prime$ as input features gives parameter estimates $\\hat b=0.40$ and $\\hat{\\boldsymbol{\\theta}}=[-2.07,2.65]^\\prime$. The estimated decision boundary $0.40-2.07 x_1 +2.65 x_2= 0$ separates the $(x_1,x_2)$-space into two half-planes. (b) The decision boundary becomes nonlinear if the feature vector of the logistic regression includes nonlinear transformations of the input variables. The feature vector for this illustration is $\\mathbf{x}^*=[x_1, x_2, x_1^2, x_1^3, x_2^2, x_2^3]^\\prime$. General remarks on maximum likelihood estimation The following two identities will be especially convenient when developing the maximum likelihood framework: (1) $1-\\Lambda(x)=\\frac{1}{1+\\exp(x)}=\\Lambda(-x)$, and (2) $\\frac{d}{dx} \\Lambda(x) =\\big(1+\\exp(-x)\\big)^{-2}\\times \\exp(-x)= \\Lambda(x)\\big( 1 -\\Lambda(x) \\big)$. Exploiting the first identity, the likelihood function for $n$ independent observations $\\big(y_i, \\mathbf{x}_i \\big)_{i=1,\\ldots,n}$ from model \\eqref{eq:LogisticModel} can be written as $$ \\begin{aligned} L(b,\\boldsymbol\\theta) \u0026amp;= \\prod_{i=1}^n \\Big(\\Lambda ( b + \\boldsymbol \\theta\u0026rsquo;\\mathbf{x}_i)\\Big)^{y_i} \\Big(1-\\Lambda ( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i)\\Big)^{1-y_i} \\\\ \u0026amp;= \\prod_{i=1}^n \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big). \\end{aligned} $$ Clearly, the implied log-likelihood function is $$ \\log L(b,\\boldsymbol \\theta) = \\sum_{i=1}^n \\log \\Lambda \\Big( (2y_i -1) \\big( b + \\boldsymbol \\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big). $$ The second identity is subsequently used to easily compute the derivatives of this scaled log-likelihood. Repeated application of the chain rule gives the gradients $$ \\begin{aligned} \\frac{\\partial \\log L(b,\\boldsymbol\\theta) }{\\partial b} \u0026amp;= \\sum_{i=1}^n \\frac{1}{\\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big)} \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big)\\Big[1 - \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big)\\Big) \\Big] (2y_i -1) \\\\ \u0026amp;= \\sum_{i=1}^n (2y_i -1) \\Big[1 - \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big) \\Big], \\\\ \\frac{\\partial \\log L(b,\\boldsymbol \\theta) }{\\partial \\boldsymbol\\theta} \u0026amp;= \\sum_{i=1}^n (2y_i -1) \\mathbf{x}_i \\Big[1 - \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big) \\Big]. \\end{aligned} $$ The stack of these two gradients will be denoted as $\\mathbf{S}(b,\\boldsymbol\\theta)= \\sum_{i=1}^n (2y_i -1) \\left[\\begin{smallmatrix} 1 \\\\ \\mathbf{x}_i \\end{smallmatrix}\\right] \\Big[1 - \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big) \\Big]$. As these gradients are linear in $\\Lambda\\big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big) \\big)$, the $(p+1)\\times(p+1)$ Hessian matrix follows as\n\\begin{equation} \\begin{aligned} \\mathbf{H}(b, \\boldsymbol\\theta) \u0026amp;= \\left[\\begin{smallmatrix} \\frac{\\partial^2 \\log L(b,\\boldsymbol\\theta) }{\\partial b^2} \u0026amp; \\frac{\\partial^2 \\log L(b,\\boldsymbol\\theta) }{\\partial b \\partial \\boldsymbol\\theta\u0026rsquo;} \\\\ \\frac{\\partial^2 \\log L(b,\\boldsymbol\\theta) }{\\partial b \\partial \\boldsymbol\\theta} \u0026amp; \\frac{\\partial^2 \\log L(b,\\boldsymbol \\theta) }{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta\u0026rsquo;} \\end{smallmatrix}\\right] \\\\ \u0026amp;= -\\sum_{i=1}^n \\left[\\begin{smallmatrix} 1 \u0026amp; \\mathbf{x}_i\u0026rsquo; \\\\ \\mathbf{x}_i \u0026amp; \\mathbf{x}_i \\mathbf{x}_i\u0026rsquo;\\end{smallmatrix}\\right] \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol \\theta\u0026rsquo;\\mathbf{x}_i \\big)\\Big) \\Big[1 - \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big) \\Big], \\end{aligned} \\end{equation} where $(2 y_i-1)^2=1$ has been used. Finally, take an arbitrary $\\mathbf{a} = (a_0,a_1,\\ldots, a_p)\u0026rsquo;\\in \\mathbb{R}^{p+1}$, then \\begin{equation} \\mathbf{a}\u0026rsquo; \\mathbf{H}(b, \\boldsymbol\\theta)\\mathbf{a} = - \\sum_{i=1}^n (a_0 + \\mathbf{a}_{1:p}\u0026rsquo;\\mathbf{x}_i)^2 \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big)\\Big) \\Big[1 - \\Lambda\\Big( (2y_i -1) \\big( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big) \\Big], \\label{eq:PosDefHession} \\tag{2} \\end{equation} with $\\mathbf{a}_{1:p}= (a_1,\\ldots,a_p)^\\prime$. We make several observations related to \\eqref{eq:PosDefHession}:\nThe sigmoid function satisfies $0 \u0026lt; \\Lambda(x) \u0026lt; 1$ for all $x$. Each term within the summation is thus non-negative. The equality $\\mathbf{a}\u0026rsquo; \\mathbf{H}(b, \\boldsymbol\\theta) \\mathbf{a} =0$ holds if and only if $a_0 + \\mathbf{a}_{1:p}\u0026rsquo;\\mathbf{x}_i=0$ for all $i\\in{1,2,\\ldots,n}$. Or equivalently, $\\mathbf{a}\u0026rsquo; \\mathbf{H}(b, \\boldsymbol\\theta) \\mathbf{a} =0$ if and only if $\\mathbf{a}$ is orthogonal to all vectors in the collection \\begin{equation} \\left\\{\\begin{bmatrix} 1 \\\\ \\mathbf{x}_1 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ \\mathbf{x}_2 \\end{bmatrix}, \\ldots, \\begin{bmatrix} 1 \\\\ \\mathbf{x}_n \\end{bmatrix} \\right\\}. \\label{eq:vectorset} \\tag{3} \\end{equation} This analysis naturally leads to two important regimes. Low-dimensional: If $p$ is small, then $\\mathbf{a}\u0026rsquo; \\mathbf{H}(b, \\boldsymbol\\theta) \\mathbf{a}$ is strictly negative. The objective function is strictly concave because the Hessian matrix is negative-definite. The likelihood function has an unique minimizer and asymptotically valid inference is possible.\nHigh-dimensional: If $p$ is large, then we enter a high-dimensional regime with (1) many parameters to estimate, and (2) a Hessian matrix with eigenvalues (close to) 0. The dimensionality of the problem requires faster algorithms and parameter regularization.\nThe low-dimensional regime Estimation Let us denote the maximum likelihood estimators by $\\hat b$ and $\\hat{\\boldsymbol\\theta}$. These estimators are the solutions of $$ \\mathbf{S}(\\hat b, \\hat{\\boldsymbol\\theta})= \\sum_{i=1}^n (2y_i -1)\\begin{bmatrix} 1 \\\\ \\mathbf{x}_i \\end{bmatrix} \\Big[1 - \\Lambda\\Big( (2y_i -1) \\big( \\hat b + \\hat{\\boldsymbol \\theta}\u0026rsquo;\\mathbf{x}_i \\big) \\Big) \\Big] = \\mathbf{0}. $$ The solution to this set of equations is not available in closed form and we thus need to resort to numerical methods. The maximization of strictly concave objective functions is well-studied and numerical solution schemes are readily available. Especially for small $p$ (say $p\u0026lt;50$ and $p\u0026laquo;n$), we can rely on the Newton-Rhapson algorithm. The sketch of the algorithm is as follows:\nMake a starting guess for the parameters, say $b_{\\{0\\}}$ and $\\boldsymbol\\theta_{\\{0\\}}$.\nIteratively update the parameters based on a quadratic approximation of log-likelihood. That is, being located at the point $\\big\\{b_{\\{i\\}},\\boldsymbol\\theta_{\\{i\\}}\\big\\}$, the local quadratic approximation of the log-likelihood (read: its second order Taylor expansion) is \\begin{equation} L(b,\\boldsymbol \\theta) \\approx \\log L(b_{\\{i\\}},\\boldsymbol \\theta_{\\{i\\}}) + \\mathbf{S}(b_{\\{i\\}},\\boldsymbol\\theta_{\\{i\\}})^\\prime \\begin{bmatrix} b - b_{\\{i\\}} \\\\ \\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{\\{i\\}} \\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} b - b_{\\{i\\}}\\\\ \\boldsymbol\\theta - \\boldsymbol\\theta_{\\{i\\}} \\end{bmatrix}^\\prime \\mathbf{H}(b_{\\{i\\}}, \\boldsymbol\\theta_{\\{i\\}})\\begin{bmatrix} b - b_{\\{i\\}}\\\\ \\boldsymbol\\theta - \\boldsymbol\\theta_{\\{i\\}} \\end{bmatrix}. \\end{equation}\nThe updates $b_{\\{i+1\\}}$ and $\\boldsymbol{\\theta}_{\\{i+1\\}}$ are the optimizers of this quadratic approximation, or \\begin{equation} \\begin{bmatrix} b_{\\{i+1\\}}\\\\ \\boldsymbol \\theta_{\\{i+1\\}} \\end{bmatrix} = \\begin{bmatrix} b_{\\{i\\}}\\\\ \\boldsymbol \\theta_{\\{i\\}} \\end{bmatrix} - \\big[ \\mathbf{H}(b_{[i]},\\boldsymbol\\theta_{\\{i\\}}) \\big]^{-1} \\mathbf{S}(b_{\\{i\\}},\\boldsymbol\\theta_{\\{i\\}}). \\label{eq:NewtonRhapsonUpdate} \\tag{4} \\end{equation}\nRepeatly update the parameter values using \\eqref{eq:NewtonRhapsonUpdate} until (relative) parameter changes become negligible.\nIn typical settings the Newton-Rhapson algorithm converges after a small number of iterations.\nAsymptotically valid inference Define $\\hat{\\boldsymbol \\gamma} = \\left[\\begin{smallmatrix} \\hat{b} \\\\ \\hat{\\boldsymbol\\theta} \\end{smallmatrix}\\right]$ and $\\boldsymbol \\gamma = \\left[\\begin{smallmatrix} b \\\\ \\boldsymbol\\theta \\end{smallmatrix}\\right]$. Results as in McFadden and W.K. Newey (1994) imply: $$ \\sqrt{n}\\left( \\hat{\\boldsymbol \\gamma} - \\boldsymbol \\gamma \\right) \\stackrel{D}{\\to} \\mathcal{N}(\\mathbf{0}, \\mathbf{J}^{-1}) \\quad \\text{as} \\quad T\\to\\infty, \\label{eq:AsymptoticDistr} \\tag{5} $$ where $\\mathbf{J}= - \\mathbb{E}\\left[ \\frac{\\partial^2 \\log \\Lambda\\big( (2y-1) (b+\\boldsymbol\\theta\u0026rsquo;\\mathbf{x}) \\big) }{\\partial \\boldsymbol \\gamma \\partial \\boldsymbol \\gamma\u0026rsquo;} \\right]$ (the expectation is computed w.r.t. the joint distribution of $(y,\\mathbf{x})$). A consistent estimator for $\\mathbf{J}$, for example $$ \\hat{\\mathbf{J}} = \\frac{1}{n}\\mathbf{H}(\\hat b, \\hat{\\boldsymbol\\theta}) =-\\frac{1}{n}\\sum_{i=1}^n \\left[\\begin{smallmatrix} 1 \u0026amp; \\mathbf{x}_i\u0026rsquo; \\\\ \\mathbf{x}_i \u0026amp; \\mathbf{x}_i \\mathbf{x}_i\u0026rsquo;\\end{smallmatrix}\\right] \\Lambda\\Big( (2y_i -1) \\big( \\hat{b} + \\hat{\\boldsymbol \\theta}\u0026rsquo;\\mathbf{x}_i \\big)\\Big) \\Big[1 - \\Lambda\\Big( (2y_i -1) \\big( \\hat b + \\hat{\\boldsymbol\\theta}\u0026rsquo;\\mathbf{x}_i \\big) \\Big) \\Big], $$ is an almost immediate byproduct of the Newton-Rhapson algorithm. Asymptotically valid hypothesis tests are thus easy to construct in this low-dimensional regime.\nIllustration 1: Haberman\u0026rsquo;s survival data set The data is freely available here, or you can readily download the SQL-file $\\texttt{HabermanDataSet.sqlite}$ from my GitHub page. The binary variable in this study is the survival status of $n=306$ patients who had undergone surgery for breast cancer. For $i=1,\\ldots,n$, we recode this variable into $y_i=1$ (patient $i$ survived 5 years or longer) and $y_i=0$ (patient $i$ died within 5 years). There are three explanatory variables:\n$Age_i$: age of patient $i$ at the time of operation $Year_i$: year of operation for patient $i$ with offset of 1900 (i.e. the year 1960 is recorded as 60) $AxilNodes_i$: number of axillary nodes detected in patient $i$ Following Landwehr et al. (1984), we entertain the model $$ \\mathbb{P}(y_i=1 | \\mathbf{x}_i ; b, \\boldsymbol\\theta) = \\Lambda\\Big( b + \\theta_1 z_{1i} + \\theta_2 z_{1i}^2 + \\theta_3 z_{1i}^3 + \\theta_4 z_{2i} + \\theta_5 z_{1i} z_{2i} + \\theta_6 \\log(1+AxilNodes_i) \\Big), $$ where $z_{1i}=Age_{i}-52$ and $z_{2i} = Year_{i} - 63$.2 The estimation results are listed in Tables 2\u0026ndash;3 and Figure 3. Some short comments are:\nThe levels of $z_1$ and $z_2$ do not significantly influence the 5-year survival probability. These features can be omitted from the model. The sigmoid function is monotonically increasing. An increase in $b + \\theta_1 z_{1i} + \\theta_2 z_{1i}^2 + \\theta_3 z_{1i}^3 + \\theta_4 z_{2i} + \\theta_5 z_{1i} z_{2i} + \\theta_6 \\log(1+AxilNodes_i)$ will thus imply a higher survival probabiliy. The estimation results point towards nonlinear effect in $Age$. Generally speaking, older people are at a higher risk but this effect is not strictly monotone (Figure 3).3 The accuracy, the fraction of correctly classified cases, is $235/305\\approx 77.1\\%$. Further classification details are shown in Table 3. Table 2: The estimation output for the Haberman data set. The standard errors (Std. Error) and $p$-values are computed using the asymptotic distribution. Figure 3: The estimated contribution of the age component to the 5-year survival probability. The blue line corresponds to $\\hat \\theta_1 z_1 + \\hat \\theta_2 z_1^2 + \\hat \\theta_3 z_1^3$. The red line is the age contribution after the two insignificant regressors $z_1$ and $z_2$ have been omitted and the model has been re-estimated. That is, we show $\\tilde \\theta_2 z_1^2 + \\tilde \\theta_3 z_1^3$ with $\\tilde{\\boldsymbol{\\theta}}$ denoting the vector of MLEs under the restricted model. Table 3: The confusion matrix for the Haberman data set. The label $\\hat y =1$ is assigned if $\\mathbb{P}(y=1| \\mathbf{x};\\hat b, \\hat{\\boldsymbol \\theta})\u0026gt;0.5$ (majority voting). Illustration 2: A Monte Carlo study to compare implementations We also study the low-dimensional logistic regression through two small Monte-Carlo studies. The settings are outlined below.\nDGP 1: Comparing languages The data features two regressors, $\\mathbf{x}_i = (x_{1i},x_{2i})^\\prime$, generated as $\\mathbf{x}_i\\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ with $\\boldsymbol{\\Sigma}= \\left[ \\begin{smallmatrix} 1 \u0026amp; \\rho \\\\ \\rho \u0026amp;1 \\end{smallmatrix} \\right]$. The binary outcome $y_i\\in\\{0,1\\}$ takes the value 1 with probability $\\Lambda(b + \\boldsymbol\\theta\u0026rsquo; \\mathbf{x}_i)$. The selected parameters are: $\\rho=0.3$, $b=0.5$, $\\theta_1=0.3$ and $\\theta_2=0.7$. For $n\\in\\{200,500\\}$, we compare the following methods:\nDirect Newton-Rhapson implementations in Matlab, Numpy, R and TensorFlow. The implementations and convergence criteria are exactly the same across programming languages. The logistic regression implementations from GLM (R) and Scikit-learn (Python). For each setting, the code will (1) generate the data, (2) optimize the log-likelihood to compute the MLE, and (3) construct the asymptotically valid t-statistic based on the limiting distribution in \\eqref{eq:AsymptoticDistr}.4 We repeat the procedure $N_{sim}=1000$ times.\nTable 4: Computational times in seconds for the Newton-Rhapson algorithm and built-in logistic regression estimators. Computational times are reported in Table 4. Using $a\u0026gt;b$ to denote $a$ being faster than $b$, we have $Matlab\u0026gt;R\u0026gt;Python\u0026gt;TensorFlow$. The ranking of the first three has been the same in my other posts as well. As the Newton-Rhapson algorithm relies mainly on linear algebra, the good performance of Matlab (originally a linear algebra platform) might not come as a surpise. The poor performance of TensorFlow is perhaps rather unexpected. Possible explanations are: the manual implementation through $\\texttt{tf.linalg}$ not allowing pieces of the code to be executed in faster lower level languages, and inefficiencies in the TensorFlow linear algebra implementation. The latter has been reported by Sankaran et al. (2022).\nThe built-in function have similar performances to their manually implemented counterparts. Their implementation is probably faster but the built-in function incur some overhead costs.\nDGP 2: Adding irrelevant features to increase $p$ Consider $\\mathbf{x}_i = (x_{1i},\\ldots,x_{pi})^\\prime\\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\mathbf{0}, \\boldsymbol \\Sigma)$. The $(p\\times p)$ matrix $\\boldsymbol \\Sigma$ takes value 1 on the main diagonal and $\\rho$ on each off-diagonal element. The additional parameters are $b=0.5$, $\\theta_1=0.3$, $\\theta_2=0.7$ and $\\theta_3=\\ldots=\\theta_p=0$. Again, the binary outcome $y_i\\in\\{0,1\\}$ takes the value 1 with probability $\\Lambda(b + \\boldsymbol\\theta\u0026rsquo; \\mathbf{x}_i)$. We fix $n=200$ but vary $\\rho\\in\\{0.3,0.8 \\}$ and $p\\in\\{2,3,\\ldots,20\\}$.\nFigure 4: The effect of increasing $p$ for $\\rho=0.3$ (blue) and $\\rho=0.8$ (red). (a) The mean squared error (MSE) of the logistic regression estimator for $\\theta_1$. (b) Computational time increases steadily with $p$. Some comments on Figure 4 are:\nMore features make it more difficult to determine which explanatory variables are driving the success probability. The mean squared error of the parameter estimators increases. This effect amplifies if the regressors are more correlated (increasing $\\rho$). Computational times increase faster than linear in $p$. The correlation parameter $\\rho$ does not influence computational times. The high-dimensional regime Two issues arise when $p$ becomes larger:\nIssue 1: If $p\u0026gt;n$, then there exist vectors that are orthogonal to all vectors in \\eqref{eq:vectorset}. The likelihood is flat in these directions and the maximum likelihood estimator is no longer uniquely defined. In practice, performance typically deteriorates earlier. As soon as $p$ and $n$ are of similar orders of magnitude, then decreased curvature in the objective function causes additional noise to start leaking into the estimator. These effects are more pronounced when the regressors are more correlated.\nIssue 2: The computational costs of the objective function do not scale well with increasing $p$. That is, calculating $\\big[ \\mathbf{H}(b_{[i]},\\boldsymbol\\theta_{\\{i\\}}) \\big]^{-1}$ becomes increasingly time-consuming because standard matrix inversion algorithms have a computational complexity of about $\\mathcal{O}(p^3)$.\nRegularization is the typical solution to the first issue. In this post we will consider $L_1$- and $L_2$-regularization. The algorithmic modifications originate from findings in Tseng (2001). The results in this paper guarantee the convergence of a cyclic coordinate descent method whenever the objective function is a sum of a convex function and a seperable function.5 These repeated updates of single coordinates/parameters will decrease computational costs by avoiding issue 2.\nRegularization with $L_2$-penalty We start with $L_2$-regularization because it is easiest to explain. The objective function adds an additional penalty term $\\lambda \\|\\boldsymbol \\theta\\|_2^2=\\lambda \\sum_{j=1}^p \\theta_j^2$ to the previous negative log-likelihood. The estimators for $b$ and $\\boldsymbol \\theta$ are now the minimizers of \\begin{equation} Q_\\lambda(b,\\boldsymbol \\theta) = \\sum_{i=1}^n \\log \\Lambda \\Big( (2y_i -1) \\big( b + \\boldsymbol \\theta\u0026rsquo;\\mathbf{x}_i \\big) \\Big) + \\lambda \\|\\boldsymbol \\theta\\|_2^2. \\label{eq:L2ObjectiveFunction} \\tag{6} \\end{equation} The intercept parameter $b$ is not penalized.6 There are (at least) two intuitive explanations for the penalty term $\\lambda \\|\\boldsymbol \\theta\\|_2^2$:\nThe Hessian of the penalty term with respect to the parameter vector $\\boldsymbol \\theta$ is $\\frac{\\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^\\prime} \\lambda \\|\\boldsymbol \\theta\\|_2^2 = 2 \\lambda \\boldsymbol{I}_p$. The penalty term adds additional curvature to the objective function and thus provides a better defined minimum. The $L_2$-penalty discourages parameter solutions with a large norm. This avoids overfitting. For example, with strongly positively correlated variables it occurs that excessively positive and excessively negative parameters mostly cancel out to only describe small and noisy effects. These large parameter vectors marginally improve the fit but cause high variance. This situation is avoided if parameter vectors with a high norm are penalized. Given the validity of cyclic coordinate descent, the solution algorithm for optimization problem is relatively straightforward:\nMake a starting guess for the parameters, say $b_{\\{0\\}}$ and $\\boldsymbol\\theta_{\\{0\\}}$. We cycle through each $k\\in\\{1,2,\\ldots,p\\}$ and determine the parameter value $\\theta_k$ with the lowest value for $Q_\\lambda(b,\\boldsymbol \\theta)$. To clarify, we split the vector $\\boldsymbol \\theta$ into its $k$th component $\\theta_k$, and a vector with its $k$th component being omitted, i.e. $\\boldsymbol \\theta_{-k}=(\\theta_1,\\ldots,\\theta_{k-1},\\theta_{k+1},\\ldots,\\theta_p)^\\prime$ (and similar notational conventions for $\\boldsymbol x_i$). The new single-parameter objective function reads \\begin{equation} Q_\\lambda^*(\\theta_k) = \\sum_{i=1}^n \\log \\Lambda \\Big( (2y_i -1) \\big( b + \\boldsymbol \\theta_{-k}\u0026rsquo;\\mathbf{x}_{i,-k}+\\theta_k x_{i,k} \\big)\\Big)+\\lambda \\theta_k^2 +\\lambda\\|\\boldsymbol\\theta_{-k} \\|_2^2, \\end{equation} where the last contribution $\\lambda\\|\\boldsymbol\\theta_{-k} \\|_2^2$ is irrevelvant to the solution. This univariate problem is easily solved using the Newton-Rhapson algorithm. We stress that any parameter update continues to be used in the subsequent coordinate descents. Update the intercept parameter $b$ using Newton-Rhapson. Repeat steps 2 and 3 until the (relative) changes in all parameters are negligible. References J.M. Landwehr, D. Pregibon and A.C. Shoemaker (1984), Graphical Methods for Assessing Logistic Regression Models, Journal of the American Statistical Association\nD. McFadden and W.K. Newey (1994), Large Sample Estimation and Hypothesis Testing, Chapter 36, Handbook of Econometrics\nA. Sankaran, N.A. Alashti and C. Psarras (2022), Benchmarking the Linear Algebra Awareness of TensorFlow and PyTorch, arXiv:2202.09888\nP. Tseng (2001), Convergence of a Block Coordinate Descent Method for Nondifferentiable Minimization, Journal of Optimization Theory and Applications\nNotes Foreshadowing the discussion on penalized estimation, we prefer a model representation with an explicit bias term instead of implicitly assuming a specific element in $\\mathbf{x}$ (typically the first) to always take the value 1.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFollowing Landwehr et al. (1984), we center (but not rescale) $Age_i$ and $Year_i$. Such rescaling is probably advisable because $z_1^3$ attains very high values compared to the other explanatory variables.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAs the logistic regression model is nonlinear, the numerical decrease in 5-year survival probability with increasing $Age$ depends on the specific values of all other explanatory variables.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe asymptotically valid t-statistics are not automatically available from GLM (R) and Scikit-learn (Python). I added some extra lines of code to estimate the asymptotic covariance matrix.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe results in Tseng (2001) are more general. We focus on results that are most relevant for the continuation of this post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOne possible motivation for not penalizing $b$ is as follows. Abbreviating $\\Lambda ( b + \\boldsymbol\\theta\u0026rsquo;\\mathbf{x}_i)$ as $\\Lambda_i$, $Q_\\lambda(b,\\boldsymbol \\theta)$ can be written as \\begin{equation} Q_\\lambda(b,\\boldsymbol \\theta) = \\sum_{i=1}^n y_i \\log \\Lambda_i + (1-y_i)\\log\\big(1-\\Lambda_i \\big)+\\lambda \\|\\boldsymbol \\theta\\|_2^2. \\end{equation} With $b$ not being penalized, the derivative with respect to $b$ does not involve the penalty and \\begin{equation} \\begin{aligned} \\frac{\\partial}{\\partial b}Q_\\lambda(b,\\boldsymbol \\theta) \u0026amp;= \\sum_{i=1}^n y_i (1-\\Lambda_i) - (1-y_i) \\Lambda_i \\\\ \u0026amp;= \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\Lambda_i. \\end{aligned} \\end{equation} The first order condition for an optimum implies $\\frac{\\partial}{\\partial b}Q_\\lambda(b,\\boldsymbol \\theta)=0$ or $\\frac{1}{n}\\sum_{i=1}^n y_i = \\frac{1}{n}\\sum_{i=1}^n \\Lambda_i$. This latter equation states that the fraction of observations with $y=1$ matches the average success probability. By not penalizing $b$, we ensure that this intuitive equality holds.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1655251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655251200,"objectID":"d61dbb6ad96488e21931f92ee7307780","permalink":"https://HannoReuvers.github.io/post/logisticregression/","publishdate":"2022-06-15T00:00:00Z","relpermalink":"/post/logisticregression/","section":"post","summary":"Interpretation, implementation and computational comparison","tags":[],"title":"Low- and high-dimensional logistic regression","type":"post"},{"authors":[],"categories":null,"content":"The GARCH(1,1) model was proposed in Bollerslev (1986). The early publication date of this paper might give the impression that the model is somewhat outdated. Regardless of whether or not this last sentence is true, there are still two important reasons to discuss this model: (1) a discussion of the GARCH(1,1) model facilitates the understanding of more complex univariate GARCH specifications, and (2) many multivariate GARCH models follow a similar methodology. The GARCH(1,1) specification reads:\n\\begin{equation} \\begin{aligned} y_t \u0026amp;= \\sigma_t \\eta_t,\\qquad\\qquad\\qquad\\qquad \\eta_t \\stackrel{i.i.d.}{\\sim} (0,1), \\\\ \\sigma_t^2 \u0026amp;= \\omega + \\alpha y_{t-1}^2 + \\beta \\sigma_{t-1}^2, \\end{aligned} \\tag{1} \\label{eq:GARCHspec} \\end{equation} where $\\eta_t \\stackrel{i.i.d.}{\\sim} (0,1)$ is shorthand indicating an independent and identically distributed series ${\\eta_t}$ with mean $\\mathbb{E}[\\eta_t]=0$ and variance $\\mathbb{V}\\mathrm{ar}[\\eta_t]=1$. The unknown parameters in the model are $\\omega\u0026gt;0$, $\\alpha\\geq 0$, and $\\beta\\geq 0$. For convenience, we stack all parameters in the $(3\\times 1)$ vector $\\boldsymbol{\\theta}=(\\omega,\\alpha,\\beta)^\\prime$.\nThe GARCH(1,1) model defines the volatility process ${\\sigma_t^2}$ recursively. To see this, we note that $y_{t-1}^2=\\sigma_{t-1}^2 \\eta_{t-1}^2$ and rewrite the volatility recursion in \\eqref{eq:GARCHspec} as $\\sigma_t^2=\\omega+\\big(\\alpha \\eta_{t-1}^2+ \\beta \\big)\\sigma_{t-1}^2$. By repeated substitution, the volatility process $\\sigma_t^2$ is subsequently rewritten as \\begin{equation} \\begin{aligned} \\sigma_t^2 \u0026amp;= \\omega + \\big( \\alpha \\eta_{t-1}^2+ \\beta \\big) \\sigma_{t-1}^2 \\\\ \u0026amp;= \\omega + \\big( \\alpha \\eta_{t-1}^2+ \\beta \\big) \\Bigg( \\omega + \\big( \\alpha \\eta_{t-2}^2+ \\beta \\big) \\sigma_{t-2}^2 \\Bigg) \\\\ \u0026amp;= \\ldots = \\omega \\left(1 + \\sum_{j=1}^\\infty \\prod_{i=1}^j \\big( \\alpha \\eta_{t-i}^2+ \\beta \\big) \\right), \\end{aligned} \\tag{2} \\label{eq:sigma2expr} \\end{equation} whenever the infinite sum $ \\sum_{j=1}^\\infty \\prod_{i=1}^j \\big( \\alpha \\eta_{t-i}^2+ \\beta \\big)$ converges in a statistically meaningful sense. A sufficient condition for convergence (see, e.g. section 2.2 in Francq and Zakoian (2019)) is \\begin{equation} \\mathbb{E}\\Big[\\log(\\alpha \\eta_t^2 + \\beta) \\Big] \u0026lt; 0. \\tag{3} \\label{eq:stationaritycondition} \\end{equation} This convergence criterion depends on: the parameter $\\alpha$, the parameter $\\beta$, and the distribution of $\\eta_t$. Overall, if condition \\eqref{eq:stationaritycondition} is fulfilled, then the GARCH(1,1) process is uniquely defined and it will be strictly stationary.1 More stringent parameter conditions are needed to guarantee finite higher moments. An illustration is provided in Figure 1.\nFigure 1: The coloured parameter regions indicate strict stationarity (blue+red+gray), second-order stationarity (blue+red), and stationarity with finite fourth moments (blue). Expectations have been simulated using $1\\times 10^6$ Monte Carlo replications. Statistical Properties of the GARCH(1,1) Process We first investigate the covariance structure of the levels of a GARCH(1,1) process. Let $\\mathcal{F}_t=\\sigma(y_u; u\\leq t)$ denote the $\\sigma$-algebra containing all information up to and including time $t$. Since $\\sigma_t$ depends solely on observations prior to time $t$, the law of iterated expectations provides\n\\begin{equation} \\mathbb{E}[y_t] = \\mathbb{E}\\Big[ \\mathbb{E}[y_t \\mathcal{F}_{t-1}] \\Big]= \\mathbb{E}\\Big[ \\sigma_{t} \\mathbb{E}[ \\eta_t \\mathcal{F}_{t-1}] \\Big]=0. \\end{equation}\nSimilarly, since $\\mathbb{V}\\text{ar}[\\eta_t]=\\mathbb{E}[\\eta_t^2]=1$, we have $\\mathbb{V}\\text{ar}[y_t]=\\mathbb{E}[\\sigma_t^2]$. This last expectation is rather easily computed using \\eqref{eq:sigma2expr}. Indeed, the i.i.d. property of the series ${\\eta_t}$ combined with $\\mathbb{E}[a(\\eta_{t-i};\\boldsymbol \\theta)]=\\alpha+\\beta$ (for any $i\\in\\mathbb{Z}$) provides\n\\begin{equation} \\begin{aligned} \\mathbb{E}[\\sigma_t^2] \u0026amp;= \\omega \\left(1 + \\sum_{i=1}^\\infty \\mathbb{E}\\big[a(\\eta_{t-1}^2;\\boldsymbol \\theta)\\big] \\cdots \\mathbb{E}\\big[ a(\\eta_{t-i}^2;\\boldsymbol \\theta) \\big] \\right) \\\\ \u0026amp;= \\omega \\left(1 + \\sum_{i=1}^\\infty (\\alpha+\\beta)^i \\right)= \\frac{\\omega}{1-\\alpha-\\beta}, \\end{aligned} \\label{eq:expsigma2} \\end{equation}\nassuming that the geometric series converges, i.e. assuming $\\alpha+\\beta\u0026lt;1$.2 The covariance structure of the levels of the GARCH(1,1) process is now clear. If $\\alpha+\\beta\u0026lt;1$, then $\\mathbb{V}\\text{ar}[y_t]=\\omega/(1-\\alpha-\\beta)$ and $$ \\begin{aligned} \\mathbb{C}\\text{ov}[y_t, y_{t-j}] \u0026amp;= \\mathbb{E}[y_t y_{t-j}] \\\\ \u0026amp;= \\mathbb{E}\\big[ \\sigma_t y_{t-j} \\mathbb{E}[\\eta_t|\\mathcal{F}_{t-1}] \\big] = 0,\\qquad\\text{for }j=1,2,\\ldots. \\end{aligned} $$ Overall, this shows that the levels of a GARCH(1,1) process have an unconditional variance of $\\mathbb{V}\\text{ar}[y_t]=\\omega/(1-\\alpha-\\beta)$ and do not exhibit autocorrelation.\nBecause the GARCH(1,1) process is designed to model volatility (the second moment), all interesting properties are related to the covariance structure of $y_t^2$. The kurtosis of $y_t$, $\\kappa_y = \\mathbb{E}[y_t^4]/\\big(\\mathbb{V}\\text{ar}[y_t]\\big)^2$, is computed along the way. As full derivations are somewhat tedious, we relegate all details to the appendix. The two most important findings are:\nIf $\\lambda = \\mathbb{E}\\big[(\\alpha \\eta_t^2+ \\beta)^2\\big] \u0026lt;1$, then $\\kappa_y$ exists. Denoting the kurtosis of the innovations by $\\kappa_\\eta = \\mathbb{E}[\\eta_t^4]$, its mathematical expression is $$ \\begin{aligned} \\kappa_y \u0026amp;= \\left( 1 + \\frac{\\mathbb{V}\\mathrm{ar}[\\alpha\\eta_t^2+\\beta]}{1-\\lambda} \\right) \\kappa_\\eta \\\\ \u0026amp;= \\frac{1-(\\alpha+\\beta)^2}{1-(\\alpha+\\beta)^2 - \\alpha^2(\\kappa_\\eta -1)} \\kappa_\\eta. \\end{aligned} $$ The recursive nature of the volatility process thus causes the kurtosis of the GARCH(1,1) process to exceed the kurtosis of its innovations. The autocorrelations of the squares of a GARCH(1,1) process decay geometrically, i.e. $\\rho_k = \\mathbb{C}\\mathrm{or}[y_t^2,y_{t-k}^2] = \\rho_1(\\boldsymbol \\theta) (\\alpha+\\beta)^{k-1}$ where $\\rho_1(\\boldsymbol \\theta)$ is a function of the parameters $\\alpha$ and $\\beta$ but not of $k$. Estimation GARCH(1,1) models are frequently estimated by conditional Gaussian quasi-maximum likelihood. The second part of this terminology, Gaussian quasi-maximum likelihood, refers to the gaussianity assumption we place upon the distribution of ${\\eta_t}$. That is, our likelihood derivation simply assumes $\\eta_t \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0,1)$ even when the true innovation density is different. Our inference is conditional because we select some fixed initial values for $y_0$ and $\\sigma_0^2$ ourselves. The precise estimation steps are enumerated below.\nStep 1. Set initial values for $y_0$ and $\\tilde\\sigma_0^2$. A common choice is the unconditional variance for the volatility, $\\tilde\\sigma_0^2=\\omega/(1-\\alpha-\\beta)$, and $y_0=\\tilde\\sigma_0$.\nStep 2. Given the observations $y_1,\\ldots,y_T$ and a parameter vector $\\boldsymbol \\theta=(\\omega,\\alpha,\\beta)$\u0026rsquo;, we recursively compute $\\{\\tilde\\sigma_t^2(\\boldsymbol \\theta)\\}_{1\\leq t\\leq T}$ using \\begin{equation} \\tilde\\sigma_t^2(\\boldsymbol \\theta) = \\tilde\\sigma_t^2 = \\omega + \\alpha y_{t-1}^2 + \\beta \\tilde\\sigma_{t-1}^2. \\label{eq:ApproximateVolatility} \\end{equation} The value of $\\tilde\\sigma_t^2(\\boldsymbol \\theta)$ is determined by all observations up to and including time $t-1$.\nStep 3. Approximating the volatility process $\\sigma_t^2$ by $\\tilde\\sigma_t^2(\\boldsymbol \\theta) $ and recalling the gaussianity assumption on the innovations, we have $y_t|y_{t-1},\\ldots,y_1\\sim \\mathcal{N}(0,\\tilde\\sigma_t^2)$. The quasi-likelihood is thus $$ L_T(\\boldsymbol \\theta) = \\prod_{i=1}^T \\frac{1}{\\sqrt{2\\pi \\tilde\\sigma_t^2(\\boldsymbol \\theta)}} \\exp\\left( - \\frac{y_t^2}{2 \\tilde\\sigma_t^2(\\boldsymbol \\theta)} \\right). $$ The maximisation of the quasi-likelihood is identical to the minimisation of $$ \\bar\\ell_T(\\boldsymbol \\theta) = \\frac{1}{T} \\sum_{t=1}^T \\tilde\\ell_t(\\boldsymbol \\theta),\\text{ where }\\tilde\\ell_t(\\boldsymbol \\theta) = \\frac{y_t^2}{\\tilde\\sigma_t^2(\\boldsymbol \\theta)} + \\log\\big( \\tilde\\sigma_t^2(\\boldsymbol \\theta) \\big). $$ This latter representation is more suitable when numerically solving this nonlinear optimisation problem. Stationarity conditions can be imposed during estimation.\nParameter Inference Under suitable regularity conditions, the quasi-maximum likelihood estimator (QMLE) $\\hat{\\boldsymbol \\theta}_T$ is asymptotically normally distributes:\n\\begin{equation} \\sqrt{T} \\big( \\hat{\\boldsymbol \\theta}_T - \\boldsymbol\\theta \\big) \\stackrel{D}{\\rightarrow} \\mathcal{N}\\Big( \\boldsymbol 0,(\\kappa_\\eta-1) \\mathbf{J}^{-1}\\Big) \\qquad\\text{as}\\qquad T\\to\\infty, \\label{eq:AsymptoticDistribution} \\tag{4} \\end{equation}\nwhere $\\kappa_\\eta=\\mathbb{E}[\\eta_t^4]$ (the kurtosis of the innovations) and $\\boldsymbol J = \\mathbb{E}\\left[\\frac{\\partial^2 \\ell_t}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta\u0026rsquo;}\\right]$ with $\\ell_t=\\frac{y_t^2}{\\sigma_t^2(\\boldsymbol \\theta)} + \\log\\big( \\sigma_t^2(\\boldsymbol \\theta) \\big)$. The full details of the derivation can be found in Chapter 7 of Francq and Zakoian (2019). Broadly speaking, the proof consists of three steps:\nEstablishing that $\\{\\tilde{\\sigma}_t^2(\\boldsymbol \\theta)\\}$ is close to $\\{\\sigma_t^2(\\boldsymbol \\theta)\\}$. That is, the initialisation of $y_0$ and $\\tilde\\sigma_0^2$ in Step 1 should have a negligible influence asymptotically. This is expected to hold because stationarity requirements such as $\\alpha+\\beta\u0026lt;1$ imply both $\\alpha\u0026lt;1$ and $\\beta\u0026lt;1$. Looking at the recursion $\\sigma_t^2 = \\omega + \\alpha y_{t-1}^2 + \\beta \\sigma_{t-1}^2$, we see that the impact of the initialisation will decay rapidly in $t$.\nProving consistency of the QMLE, i.e. proving $\\hat{\\boldsymbol \\theta}_T \\stackrel{P}{\\to} \\boldsymbol\\theta$ as $T\\to\\infty$. This is the most difficult step because it requires (uniform) convergence of the objective function to a limiting criterion function with a unique optimum.\nAs in the usual likelihood framework, a mean-value expansion of the gradient of the quasi-likelihood around $\\boldsymbol \\theta$ provides the asymptotic distribution.\nThe result in \\eqref{eq:AsymptoticDistribution} is rather easily used for (asymptotically) valid inference because consistent estimators for $\\boldsymbol J$ and $\\kappa_\\eta$ are readily available. A consistent estimator for $\\boldsymbol J$ is the Hessian of the average quasi-likelihood evaluated at $\\hat{\\boldsymbol\\theta}_T$:\n$$ \\hat{\\boldsymbol J}_T = \\frac{\\partial^2\\bar\\ell_T(\\hat{\\boldsymbol\\theta}_T)}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta\u0026rsquo;} = \\frac{1}{T} \\sum_{t=1}^T\\frac{\\partial \\tilde\\ell_t(\\hat{\\boldsymbol\\theta}_T)}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta\u0026rsquo;}. $$\nThis Hessian matrix is usually immediately available from the numerical optimiser. A subsequent run of the volatility recursions at the estimated parameters gives estimated innovations $\\hat\\eta_t = y_t/ \\tilde\\sigma_t(\\hat{\\boldsymbol \\theta}_T)$ ($t=1,\\ldots,T$). The sample kurtosis of these estimated innovations consistently estimates $\\kappa_\\eta$.\nIllustration We illustrate the model using log-returns from the AEX index for 2000/08/08 until 2021/08/04. The $T=5400$ observations are visualised in Figure 2(a).3 The Gaussian quasi-maximum likelihood estimates are: $\\hat\\omega=2.33\\times 10^{-6}$, $\\hat\\alpha=0.109$, and $\\hat\\beta=0.876$. With these parameter estimates, we can initialise the estimated volatility process as $\\hat\\sigma_0=\\hat\\omega/(1-\\hat\\alpha-\\hat\\beta)$ and iterate through $\\hat\\sigma_t = \\hat\\omega + \\hat\\alpha y_{t-1}^2+\\hat\\beta \\hat\\sigma_{t-1}^2$ to approximate the volatility over time (Figure 2(b)). Clearly, the estimated volatility peaks during the Global Financial crisis (2008/09/29) and around the introduction of the preventive Coronavirus regulations in the Netherlands (2020/03/12). The ability to model the conditional volatility as time-varying and thereby capture volatility clusters is one of the successes of GARCH modelling.\nFigure 2: The log returns of the AEX index (left) and estimated volatility (right). This empirical example can also demonstrate how the GARCH(1,1) model can successfully replicate other stylised facts from financial data. First, we look at the autocorrelation function (ACF) of both $y_t$ and $y_t^2$ (Figure 3). The absence of autocorrelation in $y_t$ and the slow decay of the autocorrelations in $y_t^2$ align well with the theoretical properties of the GARCH(1,1) model. Second, some statistical characteristics of the innovations can be explored through the residuals $\\hat\\eta_t = y_t/\\hat\\sigma_t$ ($t=1,\\ldots,T$). The sample kurtosis of the log-returns and residuals are $\\hat\\kappa_y = 10.55$ and $\\hat\\kappa_\\eta=4.10$, respectively. The mismatch between these numbers indicates that the recursive structure of the GARCH(1,1) model should amplify the tails of the innovation distribution considerably.\nFigure 3: The autocorrelations for the levels and squares of the data. The dashed red lines at $\\pm1.96/\\sqrt{n}$ are the approximate 95% confidence interval of a strong white noise. Extensions The main idea behind the GARCH(1,1) process, i.e. the volatility update based on past volatility and past realisations, can be generalised beyond the current GARCH(1,1) setting. A non-exhaustive list of generalisations is presented below.\nThe digits in the GARCH(1,1) specification indicate the number of lags of the volatility and squared observations influencing $\\sigma_t^2$, respectively. That is, the general GARCH($p$,$q$) process reads \\begin{equation} \\begin{aligned} y_t \u0026amp;= \\sigma_t \\eta_t,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad \\eta_t \\stackrel{i.i.d.}{\\sim} (0,1). \\\\ \\sigma_t^2 \u0026amp;= \\omega + \\sum_{i=1}^q \\alpha y_{t-i}^2 + \\sum_{j=1}^p \\beta_j \\sigma_{t-j}^2, \\end{aligned} \\label{eq:GARCHpq} \\end{equation} Clearly, the GARCH(1,1) process is simply a GARCH($p$,$q$) process with $p=q=1$. The additional lags in the GARCH($p$,$q$) allow for a better fit to the data while keeping the developed intuition from the GARCH(1,1) model intact. The paper by P. R. Hansen and A. Lunde \u0026ndash; A Forecast Comparison of Volatility Models: Does Anything Beat a GARCH(1,1)? \u0026ndash; probably explains why the GARCH(1,1) model has become somewhat of a benchmark.\nThe volatility of the GARCH(1,1) model does not distinguish positive and negative past returns. That is, only $y_{t-1}^2$ (note the square) affects the volatility. However, if volatility is interpreted as riskiness, then we might expect a past negative return to have a larger impact on volatility than a past positive return. A Threshold GARCH (TGARCH) model allows for this leverage effect. Defining $y_t^+=\\max(y_t,0)$ and $y_t^- = \\max(-y_t,0)$, the TGARCH(1,1) specification is \\begin{equation} \\begin{aligned} y_t \u0026amp;= \\sigma_t \\eta_t,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad \\eta_t \\stackrel{i.i.d.}{\\sim} (0,1). \\\\ \\sigma_t^2 \u0026amp;= \\omega + \\alpha_{1,+} y_{t-1}^+ + \\alpha_{1,-} y_{t-1}^- + \\beta \\sigma_{t-1}^2, \\end{aligned} \\label{eq:TGARCHspec} \\end{equation}\nMultivariate extensions of the GARCH(1,1) model are needed if multiple return series are to be investigated simultaneously. Let $\\mathbf{y}_t\\in \\mathbb{R}^m$ denote the $m$-dimensional return vector at time $t$. There are two popular modelling strategies. The first strategy is a direct multivariate generalisation of \\eqref{eq:GARCHspec}. That is, we let $\\boldsymbol \\eta_t\\stackrel{i.i.d.}{\\sim} (\\boldsymbol 0, \\boldsymbol{I}_m)$ denote an i.i.d. sequence of $m$-dimensional innovations with mean vector $\\boldsymbol 0$ and an identity matrix as covariance matrix. The model assumes $$ \\mathbf{y}_t = \\boldsymbol H_t^{1/2} \\boldsymbol\\eta_t, $$ with $\\boldsymbol H_t \\in \\mathbb{R}^{m\\times m}$ following a recursive structure. Different recursive structures will lead to different multivariate GARCH (MGARCH) models. One example is the BEKK-GARCH(1,1), $$ \\boldsymbol H_t = \\boldsymbol\\Omega + \\boldsymbol A \\boldsymbol{y}_{t-1} \\boldsymbol{y}_{t-1}\u0026rsquo; \\boldsymbol A\u0026rsquo; + \\boldsymbol B \\boldsymbol H_{t-1} \\boldsymbol B\u0026rsquo;, $$ as introduced in Engle and Kroner (1995).4 The similarities to the GARCH(1,1) specification should be evident as the outer product $\\boldsymbol{y}_{t-1} \\boldsymbol{y}_{t-1}^\\prime$ and conditional covariance matrix $\\boldsymbol H_t$ are the natural multivariate extensions to respectively $y_{t-1}^2$ and $\\sigma_t^2$ of the GARCH(1,1) model. The main issue with this class of MGARCH models is parameter interpretability and the difficulties in ensuring stationarity and positive-definiteness of $\\boldsymbol H_t$.\nThe second strategy, the so-called Cholesky GARCH models, starts from a collection of $m$ univariate processes. For $i=1,\\ldots,m$, we thus consider \\begin{equation} \\begin{aligned} v_{it} \u0026amp;= \\sqrt{g_{it}} \\eta_{it},\\\\ g_{it} \u0026amp;= \\omega_i + \\alpha_i v_{i,t-1}^2 + \\beta_i g_{i,t-1}, \\end{aligned} \\label{eq:CholeskyGARCHspec} \\end{equation} or in vector form as $\\boldsymbol v_{t} = \\boldsymbol G_t^{1/2} \\boldsymbol \\eta_t$ after defining $\\boldsymbol{v}_t = (v_{1t},\\ldots,v_{mt})^\\prime$ and $\\boldsymbol{G}_t = \\text{diag}(g_{1t},\\ldots, g_{mt})$. A lower unitriangular matrix $\\boldsymbol L$, that is a matrix of the form $$ \\boldsymbol L = \\begin{bmatrix} 1 \\\\ l_{2,1} \u0026amp; 1 \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\ddots \\\\ l_{m-1,1} \u0026amp; \u0026amp; \\ddots \u0026amp; 1\\\\ l_{m,1} \u0026amp; l_{m,2} \u0026amp; \\cdots \u0026amp; l_{m,m-1} \u0026amp; 1 \\end{bmatrix}, $$ subsequently connects the univarariate GARCH(1,1) process with the return series through $\\boldsymbol y_t = \\boldsymbol L \\boldsymbol v_t = \\boldsymbol L \\boldsymbol G_t^{1/2} \\boldsymbol \\eta_t$.5 The advantages of Cholesky GARCH are: (1) the conditional variance $\\boldsymbol L \\boldsymbol{G}_t \\boldsymbol{L}^\\prime$ is positive-definite by construction, and (2) the elements in $\\boldsymbol L^{-1}$ are interpretable as conditional betas. The main disadvantage of Cholesky GARCH is that the model is not invariant to changes in the ordering of the return series.\nA Computation Comparison Finally, we conduct a Monte Carlo simulation to compare computational speeds across Matlab, Python and R. The computational tasks are: (1) generating a GARCH(1,1) process with standard Gaussian innovations and $(\\omega,\\alpha,\\beta)^{\\prime}= (0.1, 0.05, 0.8)^{\\prime}$ (ensuring stationarity), (2) estimating the parameters by conditional quasi-maximum likelihood using the generated data series, and (3) computing the asymptotically valid t-statistic based on the limiting distribution in \\eqref{eq:AsymptoticDistribution}. We use $N_{sim}=1000$ Monte Carlo replications. To see the code, click the corresponding button at the top of this blog.\nFigure 4: A histogram of the standarised test statistics related to the parameter $\\alpha$ for sample sizes: (a) $T=2500$, (b) $T=5000$, and (c) $T=10000$. Table 1: Absolute and relative (using C++ as the benchmark) computational times in seconds. The results are shown in Figure 4 and Table 1. We make the following two observations:\nTheoretically, the standardised statistics converge in distribution to a standard normal as $n\\to\\infty$. The finite sample distribution of the test statistics (Figure 4) indeed seems to approach the $\\mathcal{N}(0,1)$ distribution but truly large sample sizes are needed. The Matlab and Python implementation are fastest and slowest, respectively. The same ranking was also observed in this previous post. The overall speed of the implementation is probably determined by the speed of for-loops (to recursively compute the volatility process) and the numerical optimiser (fmincon() for Matlab, the minimize() function from scipy for Python, and optim() for R). The good performance of Matlab is thus perhaps unsurprising. Its for-loops are known to be quick and fmincon() is a pretty robust nonlinear optimisation routine.6 Appendix Calculating $\\mathbb{E}[y_t^4]$ By the law of iterated expectations, we have $\\mathbb{E}[y_t^4]= \\mathbb{E}[\\sigma_t^4] \\mathbb{E}[\\eta_t^4]$. Clearly, $\\mathbb{E}[\\eta_t^4]$ is implied by the distributional assumption on $\\eta_t$ and we only need to calculate $\\mathbb{E}[\\sigma_t^4]$. Using \\eqref{eq:sigma2expr}, we find \\begin{equation} \\begin{aligned} \\mathbb{E}\\big[\\sigma_t^4\\big] \u0026amp;= \\mathbb{E}\\left[ \\omega^2 \\left(1 + \\sum_{j=1}^\\infty \\prod_{i=1}^j \\big( \\alpha \\eta_{t-i}^2+ \\beta \\big) \\right)^2 \\right] \\\\ \u0026amp;= \\omega^2 \\Bigg( 1 + 2 \\sum_{j=1}^\\infty \\mathbb{E}\\Bigg[ \\prod_{i=1}^j \\big( \\alpha \\eta_{t-i}^2+ \\beta \\big) \\Bigg] \\\\ \u0026amp;\\qquad\\qquad+ \\sum_{j=1}^\\infty \\sum_{k=1}^\\infty \\mathbb{E}\\Bigg[\\prod_{i=1}^j \\prod_{l=1}^k \\big( \\alpha \\eta_{t-i}^2+ \\beta \\big) \\big( \\alpha \\eta_{t-l}^2+ \\beta \\big) \\Bigg] \\Bigg). \\end{aligned} \\label{eq:expsigma4} \\end{equation} For brevity, we define $\\zeta = \\mathbb{E}\\big[\\alpha \\eta_t^2+ \\beta\\big]=\\alpha+\\beta$ and $\\lambda = \\mathbb{E}\\big[(\\alpha \\eta_t^2+ \\beta)^2\\big]=\\alpha^2 \\mathbb{E}\\big[\\eta_t^4 \\big]+2\\alpha\\beta + \\beta^2$. Standard results on geometric series imply \\begin{equation} \\begin{aligned} \\mathbb{E}\\big[\\sigma_t^4\\big] \u0026amp;= \\omega^2 \\left( 1 + 2 \\sum_{j=1}^\\infty \\zeta^j + \\sum_{j=1}^\\infty \\sum_{k=1}^\\infty \\lambda^{\\min(j,k)} \\zeta^{\\max(j,k)-\\min(j,k)} \\right) \\\\ \u0026amp;= \\omega^2 \\left( 1 + 2 \\sum_{j=1}^\\infty \\zeta^j + \\sum_{j=1}^\\infty \\lambda^j + 2 \\sum_{j=1}^\\infty \\sum_{k=j+1}^\\infty \\lambda^j \\zeta^{k-j} \\right) \\\\ \u0026amp;= \\omega^2 \\left( 1 + \\frac{2\\zeta}{1-\\zeta}+ \\frac{\\lambda}{1-\\lambda}+ \\frac{2 \\zeta}{1-\\zeta} \\frac{\\lambda}{1-\\lambda} \\right) \\\\ \u0026amp;= \\omega^2 \\left( \\frac{1}{1-\\lambda} + \\frac{2\\zeta}{(1-\\zeta)(1-\\lambda)} \\right) = \\omega^2 \\frac{1+\\zeta}{(1-\\zeta)(1-\\lambda)} \\\\ \u0026amp;= \\big( \\mathbb{E}[\\sigma_t^2] \\big)^2 \\frac{1-\\zeta^2}{1-\\lambda} = \\big( \\mathbb{E}[\\sigma_t^2] \\big)^2 \\left( 1 + \\frac{\\mathbb{V}\\mathrm{ar}[\\alpha\\eta_t^2+\\beta]}{1-\\lambda} \\right) , \\end{aligned} \\tag{5} \\label{eq:sigmat4} \\end{equation} where the convergence of the geometric series requires $\\lambda\u0026lt;1$. We also used $\\mathbb{E}[\\sigma_t^2] = \\omega/(1-\\alpha-\\beta) = \\omega/(1-\\zeta)$ and $\\mathbb{V}\\mathrm{ar}[\\alpha\\eta_t^2+\\beta]=\\lambda-\\zeta^2$.\nKurtosis For any random variable $X$, its kurtosis is defined as its standardised fourth moment $$ \\kappa_X = \\mathbb{E}\\left[\\left(\\frac{X-\\mu}{\\sqrt{\\mathbb{V}\\mathrm{ar}[X]}}\\right)^4\\right] . $$ Clearly, we have $\\kappa_\\eta=\\mathbb{E}[\\eta_t^4]$ and $\\kappa_y=\\mathbb{E}[y_t^4]/\\big( \\mathbb{E}[y_t^2]\\big)^2$. The derivations in \\eqref{eq:sigmat4} directly imply $$ \\kappa_y = \\frac{\\mathbb{E}[y_t^4]}{\\big( \\mathbb{E}[y_t^2]\\big)^2} = \\frac{\\mathbb{E}[\\sigma_t^4]}{\\big( \\mathbb{E}[\\sigma_t^2]\\big)^2} , \\mathbb{E}[\\eta_t^4] = \\left( 1 + \\frac{\\mathbb{V}\\mathrm{ar}[\\alpha\\eta_t^2+\\beta]}{1-\\lambda} \\right) \\kappa_\\eta. $$ As $0\u0026lt;\\lambda\u0026lt;1$ (the upper bound is needed to guarantee the existence of $\\mathbb{E}[y_t^4]$), this formula stresses the fact that $\\kappa_y\\geq \\kappa_\\eta$. The marginal distribution of $y_t$ thus has thicker tails than the distribution of the innovations. Using $\\mathbb{V}\\mathrm{ar}[\\alpha\\eta_t^2+\\beta]=\\alpha^2(\\kappa_\\eta-1)$ and $\\lambda=(\\alpha+\\beta)^2 + \\alpha^2(\\kappa_\\eta -1)$, the expression for $\\kappa_y$ in terms of $\\alpha$ and $\\beta$ reads $$ \\kappa_y = \\frac{1-(\\alpha+\\beta)^2}{1-(\\alpha+\\beta)^2 - \\alpha^2(\\kappa_\\eta -1)} \\kappa_\\eta. $$\nAutocorrelations of the Squares of a GARCH(1,1) Process For any $k\u0026gt;1$, using stationarity, the $k$th autocorrelation of the square of a GARCH(1,1) process is \\begin{equation} \\begin{aligned} \\rho_k \u0026amp;= \\mathbb{C}\\mathrm{or}[y_t^2,y_{t-k}^2] = \\frac{\\mathbb{C}\\mathrm{ov}[y_t^2,y_{t-k}^2]}{\\sqrt{\\mathbb{V}\\mathrm{ar}[y_t^2] , \\mathbb{V}\\mathrm{ar}[y_{t-k}^2] }} \\\\ \u0026amp;= \\frac{\\mathbb{C}\\mathrm{ov}[y_t^2,y_{t-k}^2]}{\\mathbb{V}\\mathrm{ar}[y_t^2] } = \\frac{\\mathbb{E}[y_t^2 y_{t-k}^2] - \\big( \\mathbb{E}[y_t^2] \\big)^2 }{ \\mathbb{E}[y_t^4] - \\big( \\mathbb{E}[y_t^2] \\big)^2 }. \\end{aligned} \\label{eq:autocorrelationcoef} \\tag{6} \\end{equation} Having derived $ \\mathbb{E}[\\sigma_t^4]$ in \\eqref{eq:sigmat4}, the denominator of \\eqref{eq:autocorrelationcoef} is easiest to simplify. We have $$ \\begin{aligned} \\mathbb{E}[y_t^4] - \\big( \\mathbb{E}[y_t^2] \\big)^2 \u0026amp;= \\mathbb{E}[\\sigma_t^4] \\kappa_\\eta - \\big( \\mathbb{E}[\\sigma_t^2] \\big)^2 = \\big( \\mathbb{E}[\\sigma_t^2] \\big)^2\\big(\\kappa_y -1\\big). \\end{aligned} $$ The numerator is evaluated using a recursion similar to \\eqref{eq:sigma2expr}. Only this time we stop the substitution as soon as we have related $\\sigma_t^2$ to $\\sigma_{t-k}^2$, that is $$ \\sigma_t^2 = \\omega\\left( 1 + \\sum_{j=1}^{k-1} \\prod_{i=1}^j \\big(\\alpha \\eta_{t-i}^2 + \\beta\\big) \\right) + \\sigma_{t-k}^2 \\prod_{i=1}^k \\big(\\alpha \\eta_{t-i}^2 + \\beta\\big). $$ The expression above allows us to evaluate $\\mathbb{E}[y_t^2 y_{t-k}^2]$. We repeatedly use the law of iterated expectations and find $$ \\begin{aligned} \\mathbb{E}[y_t^2 y_{t-k}^2] \u0026amp;= \\mathbb{E}[\\sigma_t^2 \\sigma_{t-k}^2 \\eta_{t-k}^2] \\\\ \u0026amp;= \\mathbb{E}\\left[\\Bigg\\{ \\omega\\Bigg( 1 + \\sum_{j=1}^{k-1} \\prod_{i=1}^j \\big(\\alpha \\eta_{t-i}^2 + \\beta\\big) \\Bigg) + \\sigma_{t-k}^2 \\prod_{i=1}^k \\big(\\alpha \\eta_{t-i}^2 + \\beta\\big) \\Bigg\\} \\sigma_{t-k}^2 \\eta_{t-k}^2 \\right] \\\\ \u0026amp;= \\omega \\mathbb{E}\\big[\\sigma_{t-k}^2\\big] \\mathbb{E}\\left[ \\Bigg( 1 + \\sum_{j=1}^{k-1} \\prod_{i=1}^j \\big(\\alpha \\eta_{t-i}^2 + \\beta\\big) \\Bigg) \\right] + \\mathbb{E}\\big[\\sigma_{t-k}^4\\big] \\mathbb{E}\\left[ \\prod_{i=1}^{k-1} \\big(\\alpha \\eta_{t-i}^2 + \\beta\\big) \\right] \\mathbb{E}\\left[ \\alpha \\eta_{t-k}^4 + \\beta \\eta_{t-k}^2 \\right] \\\\ \u0026amp;= \\omega \\mathbb{E}\\big[ \\sigma_{t-k}^2\\big]\\left(1+\\sum_{j=1}^{k-1} \\zeta^j \\right) + \\mathbb{E}\\big[\\sigma_{t-k}^4\\big] \\zeta^{k-1} (\\alpha \\kappa_\\eta + \\beta) \\\\ \u0026amp;= \\big( \\mathbb{E}[\\sigma_t^2] \\big)^2 \\left( 1 - \\zeta^k \\right) + \\big( \\mathbb{E}[\\sigma_t^2] \\big)^2 \\frac{\\kappa_y}{\\kappa_\\eta} , \\zeta^{k-1} , (\\alpha \\kappa_\\eta + \\beta) \\end{aligned} $$ where the last equality exploits $\\mathbb{E}[\\sigma_t^2] = \\omega/(1-\\zeta)$, intermediate results from \\eqref{eq:sigmat4}, and stationarity. As $ \\mathbb{E}[y_t^2] = \\mathbb{E}[\\sigma_t^2]$, the numerator in \\eqref{eq:autocorrelationcoef} is $$ \\mathbb{E}[y_t^2 y_{t-k}^2] - \\big( \\mathbb{E}[y_t^2] \\big)^2 = \\zeta^{k-1} \\big( \\mathbb{E}[\\sigma_t^2] \\big)^2 \\left(\\frac{\\kappa_y}{\\kappa_\\eta} (\\alpha \\kappa_\\eta + \\beta) - \\zeta \\right) $$ and a simple division finally produces $$ \\rho_k = \\frac{\\frac{\\kappa_y}{\\kappa_\\eta} (\\alpha \\kappa_\\eta + \\beta) - \\zeta}{\\kappa_y -1} \\zeta^{k-1} = \\rho_1(\\boldsymbol \\theta) \\zeta^{k-1} . $$\nReferences T. Bollerslev (1986), Generalized Autoregressive Conditional Heteroskedasticity, Journal of Econometrics\nS. Darolles, C. Francq and S. Laurent (2018), Asymptotics of Cholesky GARCH Models and Time-varying Conditional Betas, Journal of Econometrics\nR.F. Engle and K. Kroner (1995), Multivariate Simultaneous GARCH, Econometric Theory atom://tree-view C. Francq and J.-M. Zakoian (2019), GARCH Models: Structure, Statistical Inference and Financial Applications, John Wiley \u0026amp; Sons\nP.R. Hansen and A. Lunde (2005), A Forecast Comparison of Volatility Models: Does Anything Beat a GARCH(1,1)?, Journal of Applied Econometrics\nNotes The process ${X_t}$ is said to be strictly stationary if the random vectors $(X_1,\\ldots,X_k)$\u0026rsquo; and $(X_{1+h},\\ldots,X_{k+h})$\u0026rsquo; have the same joint distribution for any $k\\in\\mathbb{N}$ and $h\\in\\mathbb{Z}$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe natural logarithm is a concave function. Jensen\u0026rsquo;s inequality implies that $\\gamma = \\mathbb{E}\\big[\\log(\\alpha \\eta_t^2 + \\beta) \\big]\\leq \\log\\big( \\alpha \\mathbb{E}[\\eta_t^2] + \\beta \\big)=\\log(\\alpha+\\beta)$. In other words, if $\\alpha+\\beta\u0026lt;1$ holds, then \\eqref{eq:stationaritycondition} must hold as well.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA total of 57 missing observations (1.06% of all data points) were linearly interpolated.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe representation here is somewhat of a simplification of the original model because I take $K=1$. The model can be made more flexible by allowing $K\u0026gt;1$. The current specification was selected because it makes the link with the univariate GARCH(1,1) model most explicit.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe exposition here is once again a simplification. Possible generalisations of this Cholesky GARCH model include: (1) more complicated processes for $v_{it}$ ($i=1,\\ldots,m$), and (2) making the parameters in the matrix $\\boldsymbol L$ time-varying. Details can be found in Darolles, Francq and Laurent (2018).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe nonlinear optimisation was started with the true parameter vector as initial guess. The Matlab function fmincon() typically does a better job at finding a nearby (possibly only local) optimum. The optimisers from Python and R show a tendency to wander around more before settling at a local optimum. I only imposed a lower bound on $\\omega$, $\\alpha$ and $\\beta$ during optimisation. Computational times will differ if more elaborate parameter constraints are imposed during estimation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1652140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652140800,"objectID":"bf222bad5a049d2d3f67bf8a35139e2e","permalink":"https://HannoReuvers.github.io/post/garch11/","publishdate":"2022-05-10T00:00:00Z","relpermalink":"/post/garch11/","section":"post","summary":"Benchmark stochastic volatility models","tags":[],"title":"The GARCH(1,1) model and its extensions","type":"post"},{"authors":["Hanno Reuvers and Etinne Wijler"],"categories":null,"content":"","date":1640908920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640908920,"objectID":"d40f1e199ddf74fea26e1f424267d646","permalink":"https://HannoReuvers.github.io/publication/teun-paper/","publishdate":"2021-12-31T00:02:00Z","relpermalink":"/publication/teun-paper/","section":"publication","summary":" We consider a high-dimensional model in which variables are observed over time and on a spatial grid. The model takes the form of a spatio-temporal regression containing time lags and a spatial lag of the dependent variable. Unlike classical spatial autoregressive models, we do not rely on a predetermined spatial interaction matrix but infer all spatial interactions from the data. That is, assuming sparsity, we estimate the spatial and temporal dependence in a fully data-driven way by penalizing a set of Yule-Walker equations. This regularization can be left unstructured but we also propose more customized shrinkage procedures that follow intuitively when observations originate from spatial grids (e.g. satellite images). Finite sample error bounds are derived and estimation consistency is established in an asymptotic framework wherein the sample size and the number of spatial units diverge jointly. A simulation exercise shows strong finite sample performance compared to competing procedures. As an empirical application, we model satellite measured NO2 concentrations in London. Our approach delivers forecast improvements over a competitive benchmark and we discover evidence for strong spatial interactions. ","tags":null,"title":"Sparse Generalized Yule-Walker Estimation of Large Spatio-temporal Models with an Application to NO2 Satellite Data","type":"publication"},{"authors":["Yicong Lin and Hanno Reuvers"],"categories":null,"content":"","date":1639353720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639353720,"objectID":"443a72793919cf000b8f5a8dc3558d07","permalink":"https://HannoReuvers.github.io/publication/flexible-trend/","publishdate":"2021-12-13T00:02:00Z","relpermalink":"/publication/flexible-trend/","section":"publication","summary":"The environmental Kuznets curve predicts an inverted U-shaped relationship between environmental pollution and economic growth. Current analyses frequently employ models which restrict nonlinearities in the data to be explained by the economic growth variable only. We propose a Generalized Cointegrating Polynomial Regression (GCPR) to allow for an alternative source of nonlinearity. More specifically, the GCPR is a seemingly unrelated regression with (1) integer powers of deterministic and stochastic trends for the individual units, and (2) a common flexible global trend. We estimate this GCPR by nonlinear least squares and derive its asymptotic distribution. Endogeneity of the regressors will introduce nuisance parameters into the limiting distribution but a simulation-based approach nevertheless enables us to conduct valid inference. A multivariate subsampling KPSS test is proposed to verify the correct specification of the cointegrating relation. Our simulation study shows good performance of the simulated inference approach and subsampling KPSS test. We illustrate the GCPR approach using data for Austria, Belgium, Finland, the Netherlands, Switzerland, and the UK. A single global trend accurately captures all nonlinearities leading to a linear cointegrating relation between GDP and CO2 for all countries. This suggests that the environmental improvement of the last years is due to economic factors different from GDP.","tags":null,"title":"Cointegrating Polynomial Regressions with Power Law Trends: Environmental Kuznets Curve or Omitted Time Effects?","type":"publication"},{"authors":[],"categories":null,"content":"The Vasiek model is an interest rate model which specifies the short rate $r(t)$ under the risk-neutral dynamics (or $\\mathbb{Q}$-dynamics) as \\begin{equation} dr(t) = \\kappa \\big( \\theta -r(t)\\big) dt + \\sigma dW(t), \\tag{1} \\label{eq:Vasicek_dif_eq} \\end{equation} with initial condition $r(0) = r_0$ and $W(t)$ denoting a standard Brownian motion driving the stochastic differential equation. An explicit expression for $r(t)$ can be derived using It calculus (see, e.g. Mikosch (1998); Chapter 3). To solve \\eqref{eq:Vasicek_dif_eq}, we try the solution $f\\big(r(t),t\\big) = r(t) e^{\\kappa t}$. The It lemma implies \\begin{equation} \\begin{aligned} df\\big(r(t),t\\big) \u0026amp;= \\kappa r(t) e^{\\kappa t} dt + e^{\\kappa t} dr(t) \\\\ \u0026amp;= \\kappa r(t) e^{\\kappa t} dt + e^{\\kappa t}\\Big[ \\kappa \\big( \\theta -r(t)\\big) dt + \\sigma dW(t)\\Big] \\\\ \u0026amp;= \\kappa \\theta e^{\\kappa t} dt + \\sigma e^{\\kappa t} dW(t). \\end{aligned} \\tag{2} \\label{eq:ItoLemmaSolved} \\end{equation} The RHS of \\eqref{eq:ItoLemmaSolved} no longer depends on $r(t)$ and we can thus integrate to find expressions for $f\\big(r(t),t\\big)$. This result is important for two reasons. First, we can integrate over the range $[0,t]$ to find $$ \\begin{aligned} f\\big( r(t), t \\big) \u0026amp;- f\\big( r(0),0\\big) = r(t) e^{\\kappa t} -r_0 = \\int_0^t df\\big( r(s),s\\big) \\\\ \u0026amp;= \\int_0^t \\kappa \\theta e^{\\kappa s} ds + \\int_0^t \\sigma e^{\\kappa s} dW(s). \\end{aligned} $$ After some rearrangments the explicit solution for $r(t)$ is \\begin{equation} r(t) = r_0 e^{-\\kappa t} + e^{-\\kappa t} \\int_0^t \\kappa \\theta e^{\\kappa s} ds + e^{-\\kappa t} \\int_0^t \\sigma e^{\\kappa s} dW(s). \\label{eq:rtexplicit} \\tag{3} \\end{equation} With this explicit expression for $r(t)$ we can calculate its mean and variance. Since stochastic integrals have zero mean we have $$ \\begin{aligned} \\mathbb{E}\\big[ r(t) \\big] \u0026amp;= r_0 e^{-\\kappa t} + \\theta e^{-\\kappa t} \\left[ e^{\\kappa t} -1 \\right] = r_0 e^{-\\kappa t} + \\theta \\left[ 1-e^{-\\kappa t} \\right] \\\\ \u0026amp;=: r_0 e^{-\\kappa t} + \\theta \\kappa \\Lambda(t), \\end{aligned} $$ where we defined $\\Lambda(t) = \\int_0^t e^{-\\kappa s} ds = \\frac{1}{\\kappa}\\left[ 1 - e^{-\\kappa t} \\right]$. The long-run mean of the process is $\\lim_{t\\to\\infty} \\mathbb{E}\\big[ r(t) \\big] = \\theta$. Moreover, by It isometry the variance of the process is $$ \\mathbb{V}\\text{ar}\\big[r(t)\\big] = \\sigma^2 e^{-2\\kappa t} \\int_0^t e^{2\\kappa s} ds = \\tfrac{\\sigma^2}{2\\kappa} \\left[ 1- e^{-2\\kappa t} \\right] = \\tfrac{1}{2} \\sigma^2 \\Lambda(2t). $$ Second, \\eqref{eq:ItoLemmaSolved} is the starting point to derive an exact discretization of $r(t)$. Such a discretization will allow us to simulate paths of $r(t)$ that can later be used to value interest rate derivatives. Given a time step $h$, we now integrate \\eqref{eq:ItoLemmaSolved} over the interval $[t-h,t]$ to obtain $$ \\begin{aligned} f \u0026amp;\\big( r(t), t \\big) - f\\big( r(t-h),t-h\\big) = \\int_{t-h}^t df\\big( r(s),s\\big) \\\\ \u0026amp;= \\theta \\big( e^{\\kappa t} - e^{\\kappa(t-h)}\\big) + \\int_{t-h}^t \\sigma e^{\\kappa s} dW(s) \\\\ \u0026amp; \\iff r(t) = \\theta \\big(1-e^{-\\kappa h}\\big) +e^{-\\kappa h}r(t-h) + e^{-\\kappa t} \\int_{t-h}^t \\sigma e^{\\kappa s} dW(s). \\end{aligned} $$ The discretized process of $r(t)$ thus follows an AR(1) model with intercept $\\theta \\big(1-e^{-\\kappa h}\\big)$, autoregressive coefficient $e^{-\\kappa h}$, and innovation process $e^{-\\kappa t} \\int_{t-h}^t \\sigma e^{\\kappa s} dW(s)$. These innovations have some interesting properties. First, note how the integration intervals of subsequent steps of the discretization do not overlap. For example, $\\int_{t-h}^t \\sigma e^{\\kappa s} dW(s)$ depends on $ \\{ W(s) : t-h \\leq s \\leq t \\}$, whereas $\\int_{t}^{t+h} \\sigma e^{\\kappa s} dW(s)$ depends on $\\{W(s) : t \\leq s \\leq t+h \\}$. With increments of Brownian motions being independent, we must conclude that these stochastic integrals are independent. Second, stochastic integrals are normally distributed and mean zero. The distributions of the innovations is thus fully specified after computing its variance, or $$ \\begin{aligned} \\mathbb{V} \u0026amp;\\text{ar}\\left[ e^{-\\kappa t} \\int_{t-h}^t \\sigma e^{\\kappa s} dW(s) \\right] \\\\ \u0026amp;= \\sigma^2 e^{-2\\kappa t} \\int_{t-h}^t e^{2\\kappa s} ds = \\tfrac{\\sigma^2}{2\\kappa }\\left[ 1 - e^{-2\\kappa h} \\right] = \\sigma^2 h \\left[ \\frac{e^{-2\\kappa h}-1}{-2\\kappa h} \\right] \\\\ \u0026amp;=: \\sigma^2 h \\alpha\\big( -2\\kappa h \\big), \\end{aligned} $$ where we defined $\\alpha(x)=\\tfrac{e^x-1}{x} \\text{ with }\\alpha(0)=0$. Overall, we can simulate data from the Vasiek model using $r(0)=r_0$ as the starting value and moving forward according to the recursion \\begin{equation} r(t) = \\theta \\kappa \\Lambda(h) + e^{-\\kappa h} r(t-h) + u_t^{(h)},\\; u_t^{(h)}\\stackrel{i.i.d.}{\\sim}\\mathcal{N}\\left(0, \\sigma^2 h \\alpha\\big( -2\\kappa h \\big)\\right). \\label{eq:vasiceksteps} \\end{equation} Some sample paths of the Vasiek model are displayed in Figure 1.\nFigure 1: An illustration of 5 sample paths for the Vasiek model (grey lines), the mean function (blue line), and 95% (pointwise) confidence intervals (light blue area). Risk-neutral valuation of a zero-coupon bond Let $X$ denote a contingent claim with maturity date $T$. According to the risk-neutral valuation formula (cf. Proposition 8.1.2 in Bingham and Kiesel (2004)), the price at time $t$ of this claim can be computed as $\\Pi_X(t) = \\mathbb{E}_{\\mathbb{Q}}\\left.\\left[ X e^{-\\int_t^T r(s) ds}\\right| \\mathcal{F}_t \\right]$. Since the zero-coupon bond, or $T$-bond, promises a cash payment of 1 at maturity, its time-$t$ price is given by\n\\begin{equation} P(t,T) = \\mathbb{E}_{\\mathbb{Q}}\\left.\\left[e^{-\\int_t^T r(s) ds}\\right| \\mathcal{F}_t \\right], \\label{eq:zerocouponbond} \\tag{4} \\end{equation} where $\\mathcal{F}_t$ is the $\\sigma$-algebra containing all information up to time $t$. In this section we will evaluated \\eqref{eq:zerocouponbond} analytically. That is, starting from the explicit expression for $r(t)$ in \\eqref{eq:rtexplicit}, we first derive the distribution of $\\left. \\int_t^T r(s) ds\\right| \\mathcal{F}_t$ and subsequently evaluated the conditional expectation. The first step is rather tedious and explained in detail in the Appendix. It turns out that $\\left. \\int_t^T r(s) ds\\right| \\mathcal{F}_t$ is normally distributed with mean $$ r(t) \\Lambda(T-t) + \\theta\\Big[(T-t) - \\Lambda(T-t) \\Big] $$ and variance $$ \\tfrac{\\sigma^2}{\\kappa^2} \\Big[ (T-t) -2 \\Lambda(T-t) + \\tfrac{1}{2} \\Lambda \\big(2(T-t) \\big) \\Big]. $$ The expectation in \\eqref{eq:zerocouponbond} can be calculated rather quickly using moment generating functions. For a random variable $Y$, its moment generating function is defined as $M_Y(t) = \\mathbb{E}\\left[ e^{tY} \\right]$. If $Y$ is normally distributed, say $Y\\sim\\mathcal{N}(\\mu,\\sigma^2)$, then $M_Y(t) = e^{\\mu t + \\frac{1}{2} \\sigma^2 t^2}$. To evaluate \\eqref{eq:zerocouponbond}, we use this result for $t=-1$ and find that the time-$t$ price of a zero-coupon bond with maturity $T$ equals\n\\begin{equation} \\begin{aligned} P(t,T) \u0026amp;= \\exp\\left\\{ -r(t) \\Lambda(T-t) - \\theta\\Big[(T-t) - \\Lambda(T-t) \\Big]+ \\tfrac{\\sigma^2}{2\\kappa^2} \\left[ (T-t) -2\\Lambda(T-t) + \\frac{1}{2} \\Lambda\\Big(2(T-t) \\Big) \\right] \\right\\} \\\\ \u0026amp;= \\exp\\left\\{ -r(t) \\Lambda(T-t) + \\left( \\tfrac{\\sigma^2}{2\\kappa^2} -\\theta \\right) (T-t) + \\left(\\theta -\\tfrac{\\sigma^2}{\\kappa^2} \\right) \\Lambda(T-t) + \\tfrac{\\sigma^2}{4 \\kappa^2} \\Lambda\\Big(2(T-t) \\Big)\\right\\}. \\end{aligned} \\label{eq:zerocouponbondprice} \\tag{5} \\end{equation}\nWe can translate these zero-coupon bond prices into yields using $$ y(t,T) = - \\tfrac{1}{T-t} \\log P(t,T) = \\left( \\theta - \\tfrac{\\sigma^2}{2\\kappa^2}\\right) + \\tfrac{1}{T-t}\\left[r(t)\\Lambda(T-t)+ \\left(\\tfrac{\\sigma^2}{\\kappa^2} -\\theta\\right) \\Lambda(T-t) - \\tfrac{\\sigma^2}{4 \\kappa^2} \\Lambda\\Big(2(T-t)\\Big)\\right]. $$ At long maturities, as $T\\to\\infty$, the yield converges to $\\theta - \\tfrac{\\sigma^2}{2\\kappa^2}$. Visualisations of the complete yield curve are shown in the section entitled Verification by Monte Carlo simulation.\nFeynman-Kac formula: solving the PDE Consider a short-rate model with $\\mathbb{Q}$-dynamics given by $$ d r(t) = a\\big(t,r(t)\\big) dt + b\\big(t,r(t) \\big) dW(t) $$ and write $P(t,T) = F(t,r(t);T)$ to explicitly indicate the dependence on $r(t)$. For brevity, we will sometimes in this section omit the function arguments, e.g. write $F$ instead of $F(t,r(t);T)$. The Feynman-Kac formula (see, e.g. Bingham and Kiesel (2004); Proposition 8.2.2) stipulates that $F(t,r(t);T)$ solves the partial differential equation (PDE)\n\\begin{equation} \\frac{\\partial F}{\\partial t} + a \\frac{\\partial F}{\\partial r} + \\frac{b^2}{2} \\frac{\\partial^2 F}{\\partial r^2} - r F = 0, \\label{eq:FeynmanKac} \\tag{6} \\end{equation}\nwith terminal condition $F(T,r;T)=1$ for all $r\\in\\mathbb{R}$. We make two observations. First, we have $a\\big(t,r(t) \\big)=\\kappa(\\theta-r(t))$ and $b\\big(t,r(t) \\big)=\\sigma$ for the Vasiek model. Second, it is hard (or sometimes even impossible) to solve \\eqref{eq:FeynmanKac} analytically. We are however in the lucky situation where $a\\big(t,r(t) \\big)$ and $b\\big(t,r(t) \\big)$ are linear in $r(t)$. It can be shown (cf. Filipovi (2009); Proposition 5.2) that this leads to an affine term structure, that is the solution $F(t,r;T)$ must take the form $$ F(t,r;T) = \\exp\\Big[ -A(t,T) - B(t,T) r \\Big], $$ for appropriate $A(t,T)$ and $B(t,T)$.\nWe can now solve the PDE by inserting this specific functional form into \\eqref{eq:FeynmanKac} and see what this implies for $A$ and $B$. Since $\\frac{\\partial F}{\\partial t}= \\left( -\\frac{\\partial A}{\\partial t} - \\frac{\\partial B}{\\partial t} r \\right)F$, $\\frac{\\partial F}{\\partial r}= -B F$ and $\\frac{\\partial^2 F}{\\partial r^2}= B^2 F$, we find $$ \\left( -\\frac{\\partial A}{\\partial t} - \\frac{\\partial B}{\\partial t} r \\right)F-\\kappa(\\theta-r)B F + \\frac{\\sigma^2}{2}B^2 F - r F = 0 $$ or equivalently after collecting terms $$ F\\left[\\Big(-\\frac{\\partial A}{\\partial t}-\\kappa \\theta B+ \\frac{\\sigma^2}{2} B^2 \\Big) + \\Big( -\\frac{\\partial B}{\\partial t}+\\kappa B -1\\Big) r \\right] = 0. $$ The boundary condition is $F(T,r;T) = \\exp[-A(T,T)-B(T,T)r]=1$ or $A(T,T)+B(T,T)r=0$. If these relations need to hold for all $r\\in\\mathbb{R}$, then intercept terms should be zero as well as the expressions proportional to $r$. The PDE for $F(T,r(t);T)$ is now seen to reduce to a set of coupled ordinary differential equations (ODEs):\n\\begin{equation} \\begin{aligned} \u0026amp; -\\frac{\\partial A}{\\partial t}-\\kappa \\theta B+ \\frac{\\sigma^2}{2} B^2=0,\\\\ \u0026amp; -\\frac{\\partial B}{\\partial t}+\\kappa B -1=0,\\\\ \u0026amp;A(T,T) = B(T,T) = 0. \\end{aligned} \\label{eq:ODEs} \\tag{7} \\end{equation}\nThe second equation in \\eqref{eq:ODEs} completely specifies $B$. With some rewriting, we have $\\frac{\\partial B}{\\partial t}-\\kappa B = e^{\\kappa t} \\frac{\\partial}{\\partial t}\\left[e^{-\\kappa t} B \\right]= -1$. Integrating over $[t,T]$ gives $$ e^{-\\kappa T} B(T,T) - e^{-\\kappa t} B(t,T)= - \\int_t^T e^{-\\kappa s} ds. $$ Together with the boundary condition $B(T,T)=0$ we conclude that $B(t,T) = \\int_t^T e^{-\\kappa(s-t)}ds = \\frac{1}{\\kappa}\\left[ 1 - e^{-\\kappa (T-t)} \\right] = \\Lambda(T-t)$. Having found the explicit solution for $B(t,T)$, we can complete the derivations by $$ \\begin{aligned} \u0026amp;A(t,T) = -\\big[A(T,T)-A(t,T)\\big] =\\int_t^T -\\frac{\\partial A}{\\partial s} ds \\\\ \u0026amp;= \\int_t^T \\Big[ \\kappa \\theta B(s,T) - \\frac{\\sigma^2}{2} \\big[B(s,T)\\big]^2 \\Big]ds \\\\ \u0026amp;= \\theta \\left[(T-t) - \\int_t^T e^{-\\kappa (T-s)} ds \\right] - \\tfrac{\\sigma^2}{2\\kappa^2} \\int_t^T \\big[1 -2 e^{-\\kappa(T-s)}+e^{-2\\kappa(T-s)} \\big]ds , \\\\ \u0026amp;= \\theta \\Big[(T-t) - \\Lambda(T-t) \\Big] - \\tfrac{\\sigma^2}{2\\kappa^2}\\left[ (T-t) -2 \\Lambda(T-t) + \\frac{1}{2} \\Lambda\\big( 2(T-t)\\big) \\right] \\\\ \u0026amp;= \\left( \\theta - \\tfrac{\\sigma^2}{2\\kappa^2} \\right) (T-t) +\\left( \\tfrac{\\sigma^2}{\\kappa^2} - \\theta\\right) \\Lambda(T-t) - \\tfrac{\\sigma^2}{4 \\kappa^2}\\Lambda\\big( 2(T-t)\\big), \\end{aligned} $$ where we used $\\int_t^T e^{-\\kappa (T-s)} ds = \\tfrac{1}{\\kappa}\\left[ 1 - e^{-\\kappa (T-t)} \\right] = \\Lambda(T-t)$ and $\\int_t^T e^{-2\\kappa(T-s)}ds= \\frac{1}{2} \\Lambda \\big( 2(T-t)\\big)$. The overall expression for $P(t,T) = F(t,r(t);T)$ coincides with the result from the previous section.\nVerification by Monte Carlo simulation If we like to avoid extensive algebraic computations, then we can opt for a simulation approach. That is, we approximate the expectation in \\eqref{eq:zerocouponbond} by Monte Carlo simulation. Our example is $P(0,T)=\\mathbb{E}_{\\mathbb{Q}}\\big[e^{-\\int_0^T r(s) ds} \\big]$. The steps are as follows:\nPartition the interval $[0,T]$ in $n$ intervals of equal length. For $j=0,1,\\ldots,n$, the implied grid points are $t_j = j\\tfrac{T}{n}$. Start from $r(0)=r(t_0)=r_0$ and use the AR(1) recursion with stepsize $h=\\frac{T}{n}$ to simulate $N_{sim}$ sample paths of the Vasiek model. We use $r^{(i)}(t_j)$ to denote the realised value of the $i^{\\text{th}}$ path at grid point $t_j$. Approximate $P(0,T)$ by \\begin{equation} \\frac{1}{N_{sim}} \\sum_{i=1}^{N_{sim}} e^{-h \\sum_{j=1}^n r^{(i)}(t_j)} . \\label{eq:approx} \\tag{8} \\end{equation} The computation of \\eqref{eq:approx} requires choices for $n$ and $N_{sim}$. We like both these quantities to be large. A large number of simulated paths is needed because the expectation in $\\mathbb{E}{\\mathbb{Q}} \\big[e^{-\\int_0^T r(s) ds} \\big]$ is being replaced by a sample average across the simulated paths. The large value for $n$ should ensure that the integral $\\int_0^T r(s) ds$ is sufficiently well approximated by the Riemann sum $\\sum{j=1}^n r^{(i)}(t_j)\\big[ t_j - t_{j-1} \\big] = h \\sum_{j=1}^n r^{(i)}(t_j)$.\nFigure 2: Zero-coupon bond prices for various maturities. Analytical bond prices are depicted in red and simulated bond prices are shown by the black dots. The error bars are $\\pm 1.96 \\hat{\\sigma}$ with $\\hat\\sigma$ denoting the standard error among $50$ replications of the bond price Monte Carlo simulation. All calculations in this figure are based on $n=100$ grid points. Visual evidence for this simulated approach is available in Figures 2\u0026ndash;3. We have taken $r_0=6%$, $\\theta=0.08$, $\\kappa=0.86$, and $\\sigma=0.01$. The (estimated) zero-coupon bond prices from \\eqref{eq:approx} are converted into yields. Figure 2 shows that: (1) differences between simulated and exact yields are small, and (2) variability between simulated yields decreases when $N_{sim}$ increases from 50 to 500. The influence of $n$ is portrayed in Figure 3. In practice, we can use these kind of graphs to decide on suitable choices for $n$ and $N_{sim}$. Simply select a pair $(n,N_{sim})$ and verify whether the computed quantity is insensitive to changes therein.\nFigure 3: The simulated bond price for a maturity of 5 years. The true bond price, $P(0,5)\\approx 7.54%$, is independent of $n$ (red). The black dots are obtained by Monte Carlo simulation. The error bars are $\\pm 1.96 \\hat{\\sigma}$ with $\\hat\\sigma$ denoting the standard error among $50$ replications. Appendix Recall $\\Lambda(t) = \\int_0^t e^{-\\kappa s} ds = \\frac{1}{\\kappa}\\left[ 1 - e^{-\\kappa t} \\right]$ and note how it implies \\begin{equation} \\begin{aligned} \\int_x^T e^{-\\kappa(s-x)} ds \u0026amp;= e^{\\kappa x} \\Big[ \\Lambda(T) - \\Lambda(x) \\Big] = \\tfrac{1}{\\kappa} e^{\\kappa x} \\left[ e^{-\\kappa x} - e^{-\\kappa T}\\right] \\\\ \u0026amp; = \\Lambda(T-x). \\end{aligned} \\label{eq:MyINT} \\tag{A1} \\end{equation} The integral in this last equation will be used at several occasions in the derivations below. Using the expression for $r(t)$, we have $$ \\begin{aligned} \\int_t^T r(s) ds \u0026amp;= \\int_t^T e^{-\\kappa s} r(0) ds \\\\ \u0026amp; \\quad + \\int_t^T \\int_0^s e^{-\\kappa(s-x)} \\kappa \\theta dx ds + \\int_t^T \\int_0^s e^{-\\kappa (s-x)}\\sigma dW(x) ds \\\\ \u0026amp;=: I+II+III. \\end{aligned} $$ We will develop these three contributions separately. Term $I$ is easiest. Using \\eqref{eq:MyINT}, we find $$ I = \\int_t^T e^{-\\kappa s} r(0) ds = r(0) e^{-\\kappa t} \\int_t^T e^{ -\\kappa(s-t) }ds = r(0) e^{-\\kappa t} \\Lambda(T-t). $$\nFigure 4: The separation of the area of integration into two parts. Term $II$ requires a change in the order of integration. Inspecting the area of integration in Figure 4, we arrive at the following integral relation $$ \\begin{aligned} II \u0026amp;= \\int_t^T \\int_0^s e^{-\\kappa(s-x)} \\kappa \\theta dx ds \\\\ \u0026amp;= \\int_0^t \\int_t^T e^{-\\kappa(s-x)} \\kappa \\theta ds dx + \\int_t^T \\int_x^T e^{-\\kappa(s-x)} \\kappa \\theta ds dx \\\\ \u0026amp;= \\int_0^t \\int_t^T e^{-\\kappa(s-t+t-x)} \\kappa \\theta ds dx + \\kappa \\theta \\int_t^T \\Lambda(T-x) dx \\\\ \u0026amp;= \\int_0^t e^{-\\kappa(t-x)}\\kappa \\theta dx , \\Lambda(T-t) + \\kappa \\theta \\int_t^T \\Lambda(T-x) dx. \\end{aligned} $$ We apply exactly the same change in the order of integration to term $III$. The result is $$ \\begin{aligned} III \u0026amp;= \\int_t^T \\int_0^s e^{-\\kappa (s-x)}\\sigma dW(x) ds \\\\ \u0026amp;= \\int_0^t \\int_t^T e^{-\\kappa (s-x)}\\sigma ds dW(x) + \\int_t^T \\int_x^T e^{-\\kappa (s-x)}\\sigma ds dW(x) \\\\ \u0026amp;= \\int_0^t \\int_t^T e^{-\\kappa (s-t+t-x)}\\sigma ds dW(x) + \\sigma \\int_t^T \\Lambda(T-x)dW(x) \\\\ \u0026amp;= \\int_0^t e^{-\\kappa(t-x)} \\sigma dW(x) , \\Lambda(T-t) + \\sigma \\int_t^T \\Lambda(T-x)dW(x). \\end{aligned} $$ If terms $I$\u0026ndash;$III$ are added together, then we see that $\\int_x^T e^{-\\kappa(s-x)} ds$ is an affine transformation of the prevailing short rate $r(t)$, i.e. $$ \\begin{aligned} \\int_t^T \u0026amp;r(s) ds \\\\ \u0026amp; = I+II+III \\\\ \u0026amp;= \\left[ r(0) e^{-\\kappa t} + \\int_0^t e^{-\\kappa(t-x)}\\kappa \\theta dx+\\int_0^t e^{-\\kappa(t-x)} \\sigma dW(x) \\right] \\Lambda(T-t) \\\\ \u0026amp;\\quad+ \\kappa \\theta \\int_t^T \\Lambda(T-x) dx+\\sigma \\int_t^T \\Lambda(T-x)dW(x) \\\\ \u0026amp;= r(t) \\Lambda(T-t)+\\kappa \\theta \\int_t^T \\Lambda(T-x) dx+\\sigma \\int_t^T \\Lambda(T-x)dW(x). \\end{aligned} $$ Conditional on all information up to time t, i.e. conditional on $\\mathcal{F}_t$, the first two terms are deterministic. Moreover, since (1) increments of Brownian motions are independent of the current value and (2) stochastic integrals are normally distributed, we know that $ \\left.\\int_t^T r(s) ds\\right| \\mathcal{F}_t$ is normally distributed with mean $$ \\begin{aligned} r(t) \\Lambda(T-t) \u0026amp;+\\kappa \\theta \\int_t^T \\Lambda(T-x) dx \\\\ \u0026amp;= r(t) \\Lambda(T-t)+\\kappa \\theta \\int_0^{T-t} \\Lambda(x) dx \\\\ \u0026amp;= r(t) \\Lambda(T-t) + \\theta\\Big[(T-t) - \\Lambda(T-t) \\Big] \\end{aligned} $$ and by It isometry a variance of $$ \\begin{aligned} \\sigma^2 \\int_t^T \u0026amp;\\big[ \\Lambda(T-x) \\big]^2 dx = \\sigma^2 \\int_0^{T-t} \\big[ \\Lambda(x) \\big]^2 dx \\\\ \u0026amp;= \\tfrac{\\sigma^2}{\\kappa^2} \\int_0^{T-t} \\left( 1-2 e^{-\\kappa x} + e^{-2\\kappa x} \\right) dx \\\\ \u0026amp;= \\tfrac{\\sigma^2}{\\kappa^2} \\left[ (T-t) -2 \\int_0^{T-t} e^{-\\kappa x} dx + \\tfrac{1}{2} \\int_0^{2(T-t)} e^{-\\kappa x} dx \\right] \\\\ \u0026amp;= \\tfrac{\\sigma^2}{\\kappa^2} \\left[ (T-t) -2 \\Lambda(T-t) + \\tfrac{1}{2} \\Lambda\\Big(2(T-t) \\Big) \\right]. \\end{aligned} $$\nReferences N. H. Bingham and R. Kiesel (2004), Risk-neutral Valuation, Springer Finance\nD. Filipovi (2009), Term-structure Models, Springer Finance\nT. Mikosch (1998), Elementary Stochastic Calculus with Finance in View, World Scientific\n","date":1616371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616371200,"objectID":"7f724a7810a9fd9566bf67a7fcb032bc","permalink":"https://HannoReuvers.github.io/post/vasicek/","publishdate":"2021-03-22T00:00:00Z","relpermalink":"/post/vasicek/","section":"post","summary":"Zero-coupon bond pricing by risk-neutral valuation, Feynman-Kac, and Monte Carlo simulation","tags":[],"title":"The Vasiek short-rate model","type":"post"},{"authors":["Yicong Lin and Hanno Reuvers"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"94432b662be6869e43e108049167b776","permalink":"https://HannoReuvers.github.io/publication/ekc-paper/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/ekc-paper/","section":"publication","summary":"This paper develops the asymptotic theory of a Fully Modified Generalized Least Squares estimator for multivariate cointegrating polynomial regressions. Such regressions allow for deterministic trends, stochastic trends and integer powers of stochastic trends to enter the cointegrating relations. Our fully modified estimator incorporates: (1) the direct estimation of the inverse autocovariance matrix of the multidimensional errors, and (2) second order bias corrections. The resulting estimator has the intuitive interpretation of applying a weighted least squares objective function to filtered data series. Moreover, the required second order bias corrections are convenient byproducts of our approach and lead to standard asymptotic inference. We also study several multivariate KPSS-type of tests for the null of cointegration. A comprehensive simulation study shows good performance of the FM-GLS estimator and the related tests. As a practical illustration, we reinvestigate the Environmental Kuznets Curve (EKC) hypothesis for six early industrialized countries as in Wagner et al. (2020).","tags":null,"title":"Efficient Estimation by Fully Modified GLS with an Application to the Environmental Kuznets Curve","type":"publication"},{"authors":["Marina Friedrich, Eric Beutner, Hanno Reuvers, Stephan Smeekes, Jean-Pierre Urbain, Whitney Bader, Bruno Franco, Bernard Lejeune, and Emmanuel Mahieu"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"aed8dd42cafb875117da93f207302cd8","permalink":"https://HannoReuvers.github.io/publication/liege-paper/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/liege-paper/","section":"publication","summary":"Ethane is the most abundant non-methane hydrocarbon in the Earth's atmosphere and an important precursor of tropospheric ozone through various chemical pathways. Ethane is also an indirect greenhouse gas (global warming potential), influencing the atmospheric lifetime of methane through the consumption of the hydroxyl radical (OH). Understanding the development of trends and identifying trend reversals in atmospheric ethane is therefore crucial. Our dataset consists of four series of daily ethane columns. As with many other decadal time series, our data are characterized by autocorrelation, heteroskedasticity, and seasonal effects. Additionally, missing observations due to instrument failure or unfavorable measurement conditions are common in such series. The goal of this paper is therefore to analyze trends in atmospheric ethane with statistical tools that correctly address these data features. We present selected methods designed for the analysis of time trends and trend reversals. We consider bootstrap inference on broken linear trends and smoothly varying nonlinear trends. In particular, for the broken trend model, we propose a bootstrap method for inference on the break location and the corresponding changes in slope. For the smooth trend model we construct simultaneous confidence bands around the nonparametrically estimated trend. Our autoregressive wild bootstrap approach, combined with a seasonal filter, is able to handle all issues mentioned above.","tags":null,"title":"A Statistical Analysis of Time Trends in Atmospheric Ethane","type":"publication"},{"authors":["Jan Lohmeyer, Franz Palm, Hanno Reuvers, and Jean-Pierre Urbain"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"787f64717bdb93c7ad157f67a2916977","permalink":"https://HannoReuvers.github.io/publication/fic-paper/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/fic-paper/","section":"publication","summary":"This paper investigates the focused information criterion and plug-in average for vector autoregressive models with local-to-zero misspecification. These methods have the advantage of focusing on a quantity of interest rather than aiming at overall model fit. Any (suciently regular) function of the parameters can be used as a quantity of interest. We determine the asymptotic properties and elaborate on the role of the locally misspecified parameters. In particular, we show that the inability to consistently estimate locally misspecified parameters translates into suboptimal selection and averaging. We apply this framework to impulse response analysis. A Monte Carlo simulation study supports our claims.","tags":null,"title":"Focused Information Criterion for Locally Misspecified Vector Autoregressive Models","type":"publication"},{"authors":[],"categories":null,"content":"Short intro We will first have to understand the origins of global warming before we can attempt to model the past and predict the future. Two key players are displayed in Figure 1. We see how both the global temperature and the global carbon dioxide1 ($\\text{CO}_2$) concentration have been rising steadily over the last 40 years. The connection between these two graphs is commonly known as the \u0026ldquo;greenhouse effect\u0026rdquo;.\nFigure 1: Yearly temperature anomalies and carbon dioxide concentrations for the time period 1980-2019. (a) The temperature anomaly is the temperature difference with respect to the 1951-1980 global average (source: https://climate.nasa.gov) . (b) Global $\\text{CO}_2$ concentrations as obtained from NOAA Global Monitoring Laboratory. How exactly are these tiny $\\text{CO}_2$ molecules affecting the temperature on earth? The answer to this question is the topic of this post. We will look at two models: the bare-rock model and the layer model.2 Both models should be viewed as toy models as they describe a very much simplified version of reality. Indeed, the earth will be treated as a perfect sphere with the same temperature everywhere. The heat flows and spatial variation on our planet, e.g. ocean currents and altitude differences, are simply ignored. Notwithstanding their simplicity, these models will still provide all necessary ingredients to understand the origins of the greenhouse effect.\nThe bare-rock model The bare-rock model is a stylized model to explain the temperature on earth. This earth is represented by a large ball with a single uniform temperature. An energy balance between incoming sunlight and heat loss to space determines the temperature on our planet.\nEnergy received by the earth When taking a walk on a beautiful summer day, you will feel the warmth of the sun. Similarly, the earth is continuously heated by the sun. How much energy is our spherical earth receiving from the sun? Let $I_{sun}$ denote the energy influx of sunlight, i.e. the amount of incoming energy per second and per $\\text{m}^2$. To calculate the amount of energy absorbed per second, we have to multiply $I_{sun}$ by the area of the earth which is absorbing sunlight. An inspection of Figure 2 shows that the relevant area equals $\\pi r_{earth}^2$. We additionally have to account for the fact that snow, ice, and clouds will reflect a fraction of the incoming sunlight back to space. This fraction is called the albedo and is generally denoted by $\\alpha$. If a fraction $\\alpha$ is reflected, then there remains a fraction $1-\\alpha$ to be absorbed. Overall, the effective incoming flux of solar energy is: \\begin{equation} \\label{eq1} F_{in} [\\text{Watt}]= I_{sun} \\times \\pi r_{earth}^2 \\times (1-\\alpha). \\end{equation} The units of this energy flux is Watt (also abbreviated by W).3\nFigure 2: The earth will leave a circular shadow on a screen placed behind it. The area of this circle is $\\pi r_{earth}^2$, where $r_{earth}^{}$ is the radius of our spherical planet. Emitted energy The earth is not only absorbing energy, it is also emitting energy. The emitted energy is in the form of so-called black-body radiation. All warm objects emit this type of radiation and both the dominant wavelengths of the emitted light and the total energy flux depend on the body\u0026rsquo;s temperature. Warmer objects emit radiation of shorter wavelength. This explains why we see a shining sun and a glow on the iron that has just been taken out of the fire. Our earth is much colder than either of these objects and is thus radiating light of much longer wavelength. These wavelenghts are too long for the human eye to observe.\nOf particular interest for our bare-rock model is the energy flux from a black-body radiator. The amount of energy emitted per second and per $\\text{m}^2$ is governed by the Stefan-Boltzmann law: \\begin{equation} I_{black\\text{ }body} = \\sigma T^4, \\end{equation} where $\\sigma$ is the Stefan-Boltzmann constant (a constant of nature) and $T$ denotes the temperature in Kelvins.4 Each square meter of surface area can emit black-body radiation. Given that the total surface area of the earth is $4 \\pi r_{Earth}^2$, the outgoing energy flux of our planet is \\begin{equation} F_{out} [\\text{Watt}] = \\sigma T^4\\times 4 \\pi r_{earth}^2. \\end{equation}\nFinding a balance\u0026hellip; Depending on the magnitudes of $F_{in}$ and $F_{out}$, we find ourselves in one of the following three situations:\n$F_{in} \u0026gt; F_{out}$: The earth is receiving more energy than it is sending back to space. The accumulation of energy will cause the earth to heat up. $F_{in} \u0026lt; F_{out}$: The nett heat flux is negative, that is, our spherical earth is steadily losing energy and hence cooling down. $F_{in} = F_{out}$: The incoming and outgoing energy flux are exactly equal. There is neither heat gain nor heat loss and the temperature will remain constant. If $F_{in} = F_{out}$, then our earth is in equilibrium \u0026ndash; or in steady-state \u0026ndash; and this provides a good indication of the earth\u0026rsquo;s temperature according to our bare-rock model. Inserting the expressions for $F_{in}$ and $F_{out}$, this equilibrium temperature turns out to be: $$ T_{equilibrium} = \\left( \\frac{ (1-\\alpha) I_{sun}}{4 \\sigma} \\right)^{1/4}. $$ If we plug in some representative numbers,5 then this ball-shaped earth will have an equilibrium temperature of 254 K (or -19 $^\\circ$C). This earth is too cold\u0026hellip; we better add a blanket!\nFigure 3: A schematic representation of the layer model. Sunlight shines unhindered through the layer and warms the earth. The radiation from the earth is trapped by the layer and re-emitted towards both the earth and the universe. The layer model Our blanket is an additional layer placed between the earth and the sun, see Figure 3. This layer is special. The incoming light from the sun will pass straight through it, but the outgoing radiation from the earth is absorbed and subsequently re-emitted. Our new task is to determine the resulting equilibrium temperature of the earth. We again match the incoming and outgoing energy fluxes but now we should do so for the earth and the layer simulaneously. Additionally, we set the sun\u0026rsquo;s energy flux per unit of area equal to $I_{sunlight}= \\frac{1}{4} (1-\\alpha) I_{sun}$.6 The equilibrium equations are $$ I_{sunlight} + I_{layer,down} = I_{earth} \\iff \\frac{1}{4} (1-\\alpha) I_{sun} + \\sigma T_{layer}^4 = \\sigma T_{earth}^4 $$ and $$ I_{earth} = I_{layer,up} + I_{layer,down} \\iff \\sigma T_{earth}^4 = \\sigma T_{layer}^4 + \\sigma T_{layer}^4 $$ for the earth and layer, respectively. We can solve these two equations and find the earth\u0026rsquo;s temperature in this layer model: $$ T_{earth} = \\left( \\frac{ 2 (1-\\alpha) I_{sun}}{4 \\sigma} \\right)^{1/4}. $$ A quick comparison with $T_{equilibrium}$ shows that the new temperature $T_{earth}$ is a factor $2^{1/4}\\approx 1.1892$ higher. The new temperature on earth is now 302 K (or about 29 $^\\circ$C). The current temperature is admittedly a bit too high but we will also see that $\\text{CO}_2$ is not behaving as extreme as our layer. That is, this greenhouse gas does not absorb all the radiation coming from earth. Most importantly, we can now understand why the additional layer works as a blanket. It creates an additional energy flow $I_{layer,down}$ back to earth that causes the earth to warm up.\nThe analogy between carbon dioxide and the layer Carbon dioxide behaves similarly to our layer. The incoming sunlight from our warm sun has a relatively short wavelenght and this light will not (or hardly) interact with the $\\text{CO}_2$ molecules in the air. As the earth is much colder than the sun, it will emit black body radiation with longer wavelength. This radiation is partly(!) absorbed by the carbon dioxide molecules in the air and released into any arbitrary direction. This process creates an energy flow back to earth much the same as $I_{layer,down}$ in Figure 3. The higher the $\\text{CO}_2$ concentration in the atmosphere, the more energy is channeled back to earth, and the more global warming. Finally, do note that there is nothing special about carbon dioxide. The warming mechanism behind the other greenhouse gasses is the same!\nNotes Of course, there are also other greenhouse gasses (e.g. water vapor, methane, nitrous oxide, and ozone). Feel free to read \u0026ldquo;greenhouse gasses\u0026rdquo; instead of \u0026ldquo;carbon dioxide\u0026rdquo; throughout this post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe discussions of the bare-rock and layer model are inspired by chapters 2-4 from Global Warming: Understanding the Forecast by David Archer.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n1 Watt is equal to 1 Joule per second.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt is easy to convert a temperature from degree Celsius to Kelvin, just add 273.15. For example, water freezes at 0 $^\\circ$C or equivalently 273.15 K.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe numerical value of the Stefan-Boltzmann constant is $\\sigma = 5.670\\times 10^{-8} \\text{ J} \\text{ s}^{-1} \\text{ m}^{-2} \\text{ K}^{-4}$. For $\\alpha$ and $I_{sun}$, we follow the book by David Archer, and use $\\alpha=0.3$ and $I_{sun}= 1350 \\text{ W m}^{-2}$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe factor $(1-\\alpha)$ accounts for the earth\u0026rsquo;s albedo. Subsequently recall that: (1) a surface area of $\\pi r_{earth}^2$ is facing the sun and thus absorbing energy, and (2) that the total surface area of the earth is $4 \\pi r_{earth}^2$. The effective energy influx per unit of surface area thus requires a factor $\\frac{1}{4}$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596067200,"objectID":"cd06dc6bff1376218973a7739ae48b44","permalink":"https://HannoReuvers.github.io/post/climate-econometrics-1/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/post/climate-econometrics-1/","section":"post","summary":"Understanding global warming","tags":[],"title":"The greenhouse effect","type":"post"},{"authors":[],"categories":null,"content":"Short intro Let us compare the performance of C++, Matlab, Python and R when performing typical econometrical/statistical tasks. The word typical is quite ambiguous since computational requirements can vary substantially between subfields. As such, I will consider a setting that is standard yet also computationally demanding: bootstrap inference on the Durbin-Watson test statistic. This setting is easy to understand and the computational performance of the programming languages is probably representative for a wide range of Monte Carlo (MC) simulations. MC simulations are very often needed in research when relying on frequentist statistics.\nThe Durbin-Watson test statistic Consider a standard multivariate regression model $y_t^{} = \\mathbf{x}_t\u0026rsquo; \\boldsymbol \\beta + u_t^{}$ for $t = 1, 2, . . . , T$. The presence of serial correlation in $u_t$ will render standard ordinary least squares (OLS) inference invalid. The Durbin-Watson test statistic was one of the first test statistics to test for the presence of serial correlation.1 It is computed as follows:\nCalculate the OLS estimator $\\widehat{\\boldsymbol \\beta}=(\\mathbf{X}\u0026rsquo;\\mathbf{X})^{-1} \\mathbf{X}\u0026rsquo;\\mathbf{y}$. Obtain the residuals $\\widehat{u}_t=y_t- \\mathbf{x}_t\u0026rsquo;\\widehat{\\boldsymbol \\beta}$ for $t = 1, 2, . . . , T$. The Durbin-Watson test statisic is now computed as $DW= \\frac{\\sum_{t=2}^T \\left(\\widehat{u}_t-\\widehat{u}_{t-1}\\right)^2 }{\\sum_{t=1}^T \\widehat{u}_t^2}$. Simple algebraic manipulations show that the Durbin-Watson statistic can be expressed in terms of the first order sample autocorrelation $\\widehat{\\rho}= \\frac{\\sum_{t=2}^T \\widehat{u}_t\\widehat{u}_{t-1} }{\\sum_{t=1}^T \\widehat{u}_t^2}$, namely $DW=2(1-\\widehat{\\rho})-\\frac{\\widehat{u}_1^2+\\widehat{u}_T^2}{\\sum_{t=1}^T \\widehat{u}_t^2}$. Based on this result we would expect outcomes close to the value 2 if first order autocorrelation is absent. Values different from 2 indicate serial correlation. Using the Durbin-Watson statistic to carry out a formal hypothesis test is more complicated because its (asymptotic) distribution is difficult to derive. This motivates the use of a bootstrap approach.2 The specific steps are:\nEstimate $\\boldsymbol \\beta$ by OLS and compute the residual series ${\\widehat{u}_t }$ as well as the Durbin-Watson statistic $DW$. Compute $\\widehat{\\rho}$ (see the definition above) and $\\widehat{\\varepsilon,}_t=\\widehat{u}_t- \\widehat{\\rho} \\widehat{u}_{t-1}$ for $t=2,3,\\ldots,T$. Draw bootstrap errors $\\varepsilon_t^\\star$ by resampling with replacement from $\\{\\widehat{\\varepsilon}_2,\\widehat{\\varepsilon,}_3,\\ldots,\\widehat{\\varepsilon,}_T \\}$. Store these in the vector $\\boldsymbol \\varepsilon^\\star=[\\varepsilon_1^\\star,\\varepsilon_2^\\star,\\ldots,\\varepsilon_T^\\star]\u0026rsquo; $. Set $\\mathbf{u}^\\star=\\boldsymbol \\varepsilon^\\star$ (to impose the null hypothesis in the bootstrap sample), construct the bootstrap sample $\\mathbf{y}^\\star=\\mathbf{X}\\widehat{\\boldsymbol \\beta}+\\mathbf{u}^\\star$, and compute the DW statistic from $\\mathbf{y}^\\star$. Repeat steps 3-4 $B$ times and store these outcomes in a list $\\left\\{ DW_b^\\star \\right\\}_{b=1}^B$. For a given significance level $\\alpha$, we define $q_{\\alpha/2}^\\star$ and $q_{1-\\alpha/2}^\\star$ as the $\\alpha/2$ and $1-\\alpha/2$ empirical quantiles of $\\left\\{ DW_b^\\star \\right\\}_{b=1}^B$. We reject the null hypothesis of no serial correlation if either $DW \u0026lt; q_{\\alpha/2}^\\star$ or if $DW\u0026gt;q_{1-\\alpha/2}^\\star$. The simulation setting We consider a simple simulation setting to compare the different programming environments, namely\n$$ \\begin{aligned} y_t \u0026amp;= \\mathbf{x}_t\u0026rsquo;\\boldsymbol{\\beta}+u_t \\\\\\ u_t \u0026amp;= \\rho u_{t-1}+\\varepsilon_t, \\qquad \\qquad t=1,2,\\ldots,T, \\end{aligned} $$\nwhere $T=\\{100,250,500\\}$. The fixed regressor matrix $\\mathbf{X}$ contains a column of ones and a column that repeats the sequence $0,1,0,-1$ (orthogonal design). The error terms ${\\epsilon_t}$ are standard normal and a presample of 50 observations is used to remove the influence of the initial values. We set $\\boldsymbol \\beta=[1,2]\u0026rsquo; $ and vary the value for $\\rho$ over the grid $[0,0.025,\\ldots,0.5]\u0026rsquo; $ (21 grid points). The resulting power curves are depicted in Figure 1. Overall, we use 1000 Monte Carlo replications where each replication uses $B=499$ bootstrap resamples.\nResults and discussion Simulations are performed on a Macbook with a 2.6 GHz Intel Core i5 processor. I used serial computing. The computational times are reported in Table 1.\nTable 1: Absolute and relative (using C++ as the benchmark) computational times in seconds. The conclusions from this table are unambiguous. The C++ implementation is significantly faster than the scientific programming environments of Matlab and R. However, this should be balanced against the fact that the programming itself is much easier/faster in the user-friendly environments offered by either of these languages. The results do not promote the use of Python. The computational times are longest and coding is less comfortable because matrix algebra requires an additional library (such as numpy). My conclusions are qualitatively the same as those in Boraan Aruoba and Fernndez-Villaverde (2015).3\nReferences J. Durbin and G. S. Watson (1971), Testing for Serial Correlation in Least Squares Regression, Biometrika\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Jeong and S. Chung (2001), Bootstrap Tests for Autocorrelation, Computational Statistics and Data Analysis\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Boraan Aruoba and J. Fernndez-Villaverde (2015), A Comparison of Programming Languages in Macroeconomics, Journal of Economic Dynamics \u0026amp; Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"53ddb2b1af9173dcac5d24d95688dc91","permalink":"https://HannoReuvers.github.io/post/which-language/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/post/which-language/","section":"post","summary":"A comparison of C++, Matlab, Python and R for Monte Carlo simulations","tags":[],"title":"Which computational language to choose?","type":"post"}]