<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hanno Reuvers</title>
    <link>https://HannoReuvers.github.io/</link>
      <atom:link href="https://HannoReuvers.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Hanno Reuvers</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 22 Mar 2021 00:00:00 +0100</lastBuildDate>
    <image>
      <url>https://HannoReuvers.github.io/img/icon-192.png</url>
      <title>Hanno Reuvers</title>
      <link>https://HannoReuvers.github.io/</link>
    </image>
    
    <item>
      <title>The Vasiček short-rate model</title>
      <link>https://HannoReuvers.github.io/post/vasicek/</link>
      <pubDate>Mon, 22 Mar 2021 00:00:00 +0100</pubDate>
      <guid>https://HannoReuvers.github.io/post/vasicek/</guid>
      <description>&lt;p&gt;The Vasiček model is an interest rate model which specifies the short rate $r(t)$ under the risk-neutral dynamics (or $\mathbb{Q}$-dynamics) as
\begin{equation}
dr(t) = \kappa \big( \theta -r(t)\big) dt + \sigma dW(t),
\tag{1}
\label{eq:Vasicek_dif_eq}
\end{equation}
with initial condition $r(0) = r_0$ and $W(t)$ denoting a standard Brownian motion driving the stochastic differential equation. An explicit expression for $r(t)$ can be derived using Itô calculus (see, e.g.  Mikosch(1998); Chapter 3). To solve \eqref{eq:Vasicek_dif_eq}, we try the solution $f\big(r(t),t\big) = r(t) e^{\kappa t}$. The Itô lemma implies
\begin{equation}
\begin{aligned}
df\big(r(t),t\big)
&amp;amp;= \kappa r(t) e^{\kappa t} dt + e^{\kappa t} dr(t) \\&lt;br&gt;
&amp;amp;= \kappa r(t) e^{\kappa t} dt + e^{\kappa t}\Big[  \kappa \big( \theta -r(t)\big) dt + \sigma dW(t)\Big] \\&lt;br&gt;
&amp;amp;= \kappa \theta e^{\kappa t} dt + \sigma e^{\kappa t} dW(t).
\end{aligned}
\tag{2}
\label{eq:ItoLemmaSolved}
\end{equation}
The RHS of \eqref{eq:ItoLemmaSolved} no longer depends on $r(t)$ and we can thus integrate to find expressions for $f\big(r(t),t\big)$. This result is important for two reasons. First, we can integrate over the range $[0,t]$ to find
$$
\begin{aligned}
f\big( r(t), t \big) &amp;amp;- f\big( r(0),0\big)
= r(t) e^{\kappa t} -r_0
= \int_0^t df\big( r(s),s\big) \\&lt;br&gt;
&amp;amp;= \int_0^t \kappa \theta e^{\kappa s} ds + \int_0^t \sigma e^{\kappa s} dW(s).
\end{aligned}
$$
After some rearrangments the explicit solution for $r(t)$ is
\begin{equation}
r(t) = r_0 e^{-\kappa t} + e^{-\kappa t} \int_0^t \kappa \theta e^{\kappa s} ds + e^{-\kappa t}  \int_0^t \sigma e^{\kappa s} dW(s).
\label{eq:rtexplicit}
\tag{3}
\end{equation}
With this explicit expression for $r(t)$ we can calculate its mean and variance. Since stochastic integrals have zero mean we have
$$
\begin{aligned}
\mathbb{E}\big[ r(t) \big]
&amp;amp;= r_0 e^{-\kappa t} + \theta e^{-\kappa t} \left[ e^{\kappa t} -1 \right]
= r_0 e^{-\kappa t} + \theta  \left[ 1-e^{-\kappa t} \right] \\&lt;br&gt;
&amp;amp;=: r_0 e^{-\kappa t} + \theta \kappa \Lambda(t),
\end{aligned}
$$
where we defined $\Lambda(t) = \int_0^t e^{-\kappa s} ds = \frac{1}{\kappa}\left[ 1 - e^{-\kappa t} \right]$. The long-run mean of the process is $\lim_{t\to\infty} \mathbb{E}\big[ r(t) \big] = \theta$. Moreover, by Itô isometry the variance of the process is
$$
\mathbb{V}\text{ar}\big[r(t)\big]
= \sigma^2 e^{-2\kappa t} \int_0^t e^{2\kappa s} ds
= \tfrac{\sigma^2}{2\kappa} \left[ 1- e^{-2\kappa t} \right] = \tfrac{1}{2} \sigma^2 \Lambda(2t).
$$
Second, \eqref{eq:ItoLemmaSolved} is the starting point to derive an exact discretization of $r(t)$. Such a discretization will allow us to simulate paths of $r(t)$ that can later be used to value interest rate derivatives. Given a time step $h$, we now integrate \eqref{eq:ItoLemmaSolved} over the interval $[t-h,t]$ to obtain
$$
\begin{aligned}
f &amp;amp;\big( r(t), t \big) - f\big( r(t-h),t-h\big)
= \int_{t-h}^t df\big( r(s),s\big) \\
&amp;amp;= \theta \big( e^{\kappa t} - e^{\kappa(t-h)}\big) + \int_{t-h}^t \sigma e^{\kappa s} dW(s) \\&lt;br&gt;
&amp;amp; \iff r(t) = \theta \big(1-e^{-\kappa h}\big) +e^{-\kappa h}r(t-h) + e^{-\kappa t} \int_{t-h}^t \sigma e^{\kappa s} dW(s).
\end{aligned}
$$
The discretized process of $r(t)$ thus follows an AR(1) model with intercept $\theta \big(1-e^{-\kappa h}\big)$, autoregressive coefficient $e^{-\kappa h}$, and innovation process $e^{-\kappa t} \int_{t-h}^t \sigma e^{\kappa s} dW(s)$. These innovations have some interesting properties. First, note how the integration intervals of subsequent steps of the discretization do not overlap. For example, $\int_{t-h}^t \sigma e^{\kappa s} dW(s)$ depends on $ \{ W(s) : t-h \leq s \leq t \}$, whereas $\int_{t}^{t+h} \sigma e^{\kappa s} dW(s)$ depends on $\{W(s) : t \leq s \leq t+h \}$. With increments of Brownian motions being independent, we must conclude that these stochastic integrals are independent. Second, stochastic integrals are normally distributed and mean zero. The distributions of the innovations is thus fully specified after computing its variance, or
$$
\begin{aligned}
\mathbb{V} &amp;amp;\text{ar}\left[ e^{-\kappa t} \int_{t-h}^t \sigma e^{\kappa s} dW(s) \right] \\&lt;br&gt;
&amp;amp;= \sigma^2 e^{-2\kappa t} \int_{t-h}^t e^{2\kappa s} ds
= \tfrac{\sigma^2}{2\kappa }\left[ 1 - e^{-2\kappa h} \right]
= \sigma^2 h \left[ \frac{e^{-2\kappa h}-1}{-2\kappa h} \right] \\&lt;br&gt;
&amp;amp;=: \sigma^2 h \alpha\big( -2\kappa h \big),
\end{aligned}
$$
where we defined $\alpha(x)=\tfrac{e^x-1}{x} \text{ with }\alpha(0)=0$. Overall, we can simulate data from the Vasiček model using $r(0)=r_0$ as the starting value and moving forward according to the recursion
\begin{equation}
r(t) = \theta \kappa \Lambda(h) + e^{-\kappa h} r(t-h) + u_t^{(h)},\; u_t^{(h)}\stackrel{i.i.d.}{\sim}\mathcal{N}\left(0, \sigma^2 h \alpha\big( -2\kappa h \big)\right).
\label{eq:vasiceksteps}
\end{equation}
Some sample paths of the Vasiček model are displayed in Figure 1.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;paths_overview.png&#34; data-caption=&#34;Figure 1: An illustration of 5 sample paths for the Vasiček model (grey lines), the mean function (blue line), and 95% (pointwise) confidence intervals (light blue area).&#34;&gt;
&lt;img data-src=&#34;paths_overview.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 1: An illustration of 5 sample paths for the Vasiček model (grey lines), the mean function (blue line), and 95% (pointwise) confidence intervals (light blue area).
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;risk-neutral-valuation-of-a-zero-coupon-bond&#34;&gt;Risk-neutral valuation of a zero-coupon bond&lt;/h2&gt;
&lt;p&gt;Let $X$ denote a contingent claim with maturity date $T$. According to the risk-neutral valuation formula (cf. Proposition 8.1.2 in Bingham and Kiesel (2004)), the price at time $t$ of this claim can be computed as $\Pi_X(t) = \mathbb{E}_{\mathbb{Q}}\left.\left[ X e^{-\int_t^T r(s) ds}\right| \mathcal{F}_t \right]$. Since the zero-coupon bond, or $T$-bond, promises a cash payment of 1 at maturity, its time-$t$ price is given by&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(t,T) = \mathbb{E}_{\mathbb{Q}}\left.\left[e^{-\int_t^T r(s) ds}\right| \mathcal{F}_t \right],
\label{eq:zerocouponbond}
\tag{4}
\end{equation}
where $\mathcal{F}_t$ is the $\sigma$-algebra containing all information up to time $t$. In this section we will evaluated \eqref{eq:zerocouponbond} analytically. That is, starting from the explicit expression for $r(t)$ in \eqref{eq:rtexplicit}, we first derive the distribution of $\left. \int_t^T r(s) ds\right| \mathcal{F}_t$ and subsequently evaluated the conditional expectation. The first step is rather tedious and explained in detail in the Appendix. It turns out that $\left. \int_t^T r(s) ds\right| \mathcal{F}_t$ is normally distributed with mean
$$
r(t) \Lambda(T-t) + \theta\Big[(T-t) - \Lambda(T-t) \Big]
$$
and variance
$$
\tfrac{\sigma^2}{\kappa^2} \Big[  (T-t) -2 \Lambda(T-t) + \tfrac{1}{2} \Lambda \big(2(T-t) \big) \Big].
$$
The expectation in \eqref{eq:zerocouponbond} can be calculated rather quickly using moment generating functions. For a random variable $Y$, its moment generating function is defined as $M_Y(t) = \mathbb{E}\left[ e^{tY} \right]$. If $Y$ is normally distribution, say $Y\sim\mathcal{N}(\mu,\sigma^2)$, then $M_Y(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}$. To evaluate \eqref{eq:zerocouponbond}, we use this result for $t=-1$ and find that the time-$t$ price of a zero-coupon bond with maturity $T$ equals&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
P(t,T)
&amp;amp;= \exp\left\{  -r(t) \Lambda(T-t) - \theta\Big[(T-t) - \Lambda(T-t) \Big]+ \tfrac{\sigma^2}{2\kappa^2} \left[  (T-t) -2\Lambda(T-t) + \frac{1}{2} \Lambda\Big(2(T-t) \Big) \right] \right\} \\&lt;br&gt;
&amp;amp;= \exp\left\{ -r(t) \Lambda(T-t) + \left( \tfrac{\sigma^2}{2\kappa^2} -\theta \right) (T-t) + \left(\theta -\tfrac{\sigma^2}{\kappa^2}  \right)
\Lambda(T-t) + \tfrac{\sigma^2}{4 \kappa^2}  \Lambda\Big(2(T-t) \Big)\right\}.
\end{aligned}
\label{eq:zerocouponbondprice}
\tag{5}
\end{equation}&lt;/p&gt;
&lt;p&gt;We can translate these zero-coupon bond prices into yields using
$$
y(t,T) = - \tfrac{1}{T-t} \log P(t,T) = \left( \theta - \tfrac{\sigma^2}{2\kappa^2}\right) + \tfrac{1}{T-t}\left[r(t)\Lambda(T-t)+ \left(\tfrac{\sigma^2}{\kappa^2}  -\theta\right) \Lambda(T-t) - \tfrac{\sigma^2}{4 \kappa^2}  \Lambda\Big(2(T-t)\Big)\right].
$$
At long maturities, as $T\to\infty$, the yield converges to $\theta - \tfrac{\sigma^2}{2\kappa^2}$. Visualisations of the complete yield curve are shown in the section entitled &lt;em&gt;Verification by Monte Carlo simulation&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;feynman-kac-formula-solving-the-pde&#34;&gt;Feynman-Kac formula: solving the PDE&lt;/h2&gt;
&lt;p&gt;Consider a short-rate model with $\mathbb{Q}$-dynamics given by
$$
d r(t) = a\big(t,r(t)\big) dt + b\big(t,r(t) \big) dW(t)
$$
and write $P(t,T) = F(t,r(t);T)$ to explicitly indicate the dependence on $r(t)$. For brevity, we will sometimes in this section omit the function arguments, e.g. write $F$ instead of $F(t,r(t);T)$. The Feynman-Kac formula (see, e.g. ) stipulates that $F(t,r(t);T)$ solves the partial differential equation (PDE)&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial F}{\partial t} + a \frac{\partial F}{\partial r} + \frac{b^2}{2} \frac{\partial^2 F}{\partial r^2} - r F = 0,
\label{eq:FeynmanKac}
\tag{6}
\end{equation}&lt;/p&gt;
&lt;p&gt;with terminal condition $F(T,r;T)=1$ &lt;em&gt;for all&lt;/em&gt; $r\in\mathbb{R}$. We make two observations. First, we have $a\big(t,r(t) \big)=\kappa(\theta-r(t))$ and $b\big(t,r(t) \big)=\sigma$ for the Vasiček model. Second, it is hard (or sometimes even impossible) to solve \eqref{eq:FeynmanKac} analytically. We are however in the lucky situation where $a\big(t,r(t) \big)$ and $b\big(t,r(t) \big)$ are linear in $r(t)$. It can be shown (cf. Filipović (2009) ; Proposition 5.2) that this leads to an affine term structure, that is the solution $F(t,r;T)$ must take the form
$$
F(t,r;T) = \exp\Big[ -A(t,T) - B(t,T) r \Big],
$$
for appropriate $A(t,T)$ and $B(t,T)$.&lt;/p&gt;
&lt;p&gt;We can now solve the PDE by inserting this specific functional form into \eqref{eq:FeynmanKac} and see what this implies for $A$ and $B$. Since $\frac{\partial F}{\partial t}= \left( -\frac{\partial A}{\partial t} - \frac{\partial B}{\partial t} r \right)F$, $\frac{\partial F}{\partial r}= -B F$ and $\frac{\partial^2 F}{\partial r^2}= B^2 F$, we find
$$
\left( -\frac{\partial A}{\partial t} - \frac{\partial B}{\partial t} r \right)F-\kappa(\theta-r)B F + \frac{\sigma^2}{2}B^2 F - r F = 0
$$
or equivalently after collecting terms
$$
F\left[\Big(-\frac{\partial A}{\partial t}-\kappa \theta B+ \frac{\sigma^2}{2} B^2 \Big) + \Big( -\frac{\partial B}{\partial t}+\kappa B -1\Big) r \right] = 0.
$$
The boundary condition is $F(T,r;T) = \exp[-A(T,T)-B(T,T)r]=1$ or $A(T,T)+B(T,T)r=0$. If these relations need to hold for all $r\in\mathbb{R}$, then intercept terms should be zero as well as the expressions proportional to $r$. The PDE for $F(T,r(t);T)$ is now seen to reduce to a set of coupled ordinary differential equations (ODEs):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{aligned}
&amp;amp; -\frac{\partial A}{\partial t}-\kappa \theta B+ \frac{\sigma^2}{2} B^2=0,\\&lt;br&gt;
&amp;amp; -\frac{\partial B}{\partial t}+\kappa B -1=0,\\&lt;br&gt;
&amp;amp;A(T,T) = B(T,T) = 0.
\end{aligned}
\label{eq:ODEs}
\tag{7}
\end{equation}&lt;/p&gt;
&lt;p&gt;The second equation in \eqref{eq:ODEs} completely specifies $B$.  With some rewriting, we have $\frac{\partial B}{\partial t}-\kappa B = e^{\kappa t} \frac{\partial}{\partial t}\left[e^{-\kappa t} B \right]= -1$. Integrating over $[t,T]$ gives
$$
e^{-\kappa T} B(T,T) - e^{-\kappa t} B(t,T)=  - \int_t^T e^{-\kappa s} ds.
$$
Together with the boundary condition $B(T,T)=0$ we conclude that $B(t,T) = \int_t^T e^{-\kappa(s-t)}ds = \frac{1}{\kappa}\left[ 1 - e^{-\kappa (T-t)} \right] = \Lambda(T-t)$. Having found the explicit solution for $B(t,T)$, we can complete the derivations by
$$
\begin{aligned}
&amp;amp;A(t,T)
= -\big[A(T,T)-A(t,T)\big]
=\int_t^T -\frac{\partial A}{\partial s} ds \\&lt;br&gt;
&amp;amp;= \int_t^T \Big[ \kappa \theta B(s,T) - \frac{\sigma^2}{2} \big[B(s,T)\big]^2 \Big]ds \\&lt;br&gt;
&amp;amp;= \theta \left[(T-t) - \int_t^T e^{-\kappa (T-s)} ds  \right] - \tfrac{\sigma^2}{2\kappa^2} \int_t^T \big[1 -2 e^{-\kappa(T-s)}+e^{-2\kappa(T-s)} \big]ds , \\&lt;br&gt;
&amp;amp;=  \theta \Big[(T-t) - \Lambda(T-t)  \Big] - \tfrac{\sigma^2}{2\kappa^2}\left[ (T-t) -2 \Lambda(T-t) + \frac{1}{2} \Lambda\big( 2(T-t)\big) \right] \\&lt;br&gt;
&amp;amp;= \left( \theta - \tfrac{\sigma^2}{2\kappa^2}  \right) (T-t) +\left( \tfrac{\sigma^2}{\kappa^2} - \theta\right) \Lambda(T-t) - \tfrac{\sigma^2}{4 \kappa^2}\Lambda\big( 2(T-t)\big),
\end{aligned}
$$
where we used $\int_t^T e^{-\kappa (T-s)} ds = \tfrac{1}{\kappa}\left[ 1 - e^{-\kappa (T-t)} \right] = \Lambda(T-t)$ and $\int_t^T e^{-2\kappa(T-s)}ds= \frac{1}{2} \Lambda \big( 2(T-t)\big)$. The overall expression for $P(t,T) = F(t,r(t);T)$ coincides with the result from the previous section.&lt;/p&gt;
&lt;h2 id=&#34;verification-by-monte-carlo-simulation&#34;&gt;Verification by Monte-Carlo simulation&lt;/h2&gt;
&lt;p&gt;If we like to avoid extensive algebraic computations, then we can opt for a simulation approach. That is, we approximate the expectation in \eqref{eq:zerocouponbond} by Monte Carlo simulation. Our example is $P(0,T)=\mathbb{E}_{\mathbb{Q}}\big[e^{-\int_0^T r(s) ds} \big]$. The steps are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Partition the interval $[0,T]$ in $n$ intervals of equal length. For $j=0,1,\ldots,n$, the implied grid points are $t_j =  j\tfrac{T}{n}$.&lt;/li&gt;
&lt;li&gt;Start from $r(0)=r(t_0)=r_0$ and use the AR(1) recursion with stepsize $h=\frac{T}{n}$ to simulate $N_{sim}$ sample paths of the Vasiček model. We use $r^{(i)}(t_j)$ to denote the realised value of the $i^{\text{th}}$ path at grid point $t_j$.&lt;/li&gt;
&lt;li&gt;Approximate $P(0,T)$ by
\begin{equation}
\frac{1}{N_{sim}} \sum_{i=1}^{N_{sim}} e^{-h \sum_{j=1}^n r^{(i)}(t_j)} .
\label{eq:approx}
\tag{8}
\end{equation}&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The computation of \eqref{eq:approx} requires choices for $n$ and $N_{sim}$. We like both these quantities to be large. A large number of simulated paths is needed because the expectation in $\mathbb{E}_{\mathbb{Q}} \big[e^{-\int_0^T r(s) ds} \big]$ is being replaced by a sample average across the simulated paths. The large value for $n$ should ensure that the integral $\int_0^T r(s) ds$ is sufficiently well approximated  by the Riemann sum $\sum_{j=1}^n r^{(i)}(t_j)\big[ t_j - t_{j-1} \big] = h \sum_{j=1}^n r^{(i)}(t_j)$.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;Nsim_sensitivity.png&#34; data-caption=&#34;Figure 2: Zero-coupon bond prices for various maturities. Analytical bond prices are depicted in red and simulated bond prices are shown by the black dots. The error bars are $\pm 1.96 \hat{\sigma}$ with $\hat\sigma$ denoting the standard error among $50$ replications of the bond price Monte Carlo simulation. All calculations in this figure are based on $n=100$ grid points.&#34;&gt;
&lt;img data-src=&#34;Nsim_sensitivity.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 2: Zero-coupon bond prices for various maturities. Analytical bond prices are depicted in red and simulated bond prices are shown by the black dots. The error bars are $\pm 1.96 \hat{\sigma}$ with $\hat\sigma$ denoting the standard error among $50$ replications of the bond price Monte Carlo simulation. All calculations in this figure are based on $n=100$ grid points.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Visual evidence for this simulated approach is available in Figures 2&amp;ndash;3. We have taken $r_0=6%$, $\theta=0.08$, $\kappa=0.86$, and $\sigma=0.01$. The (estimated) zero-coupon bond prices from \eqref{eq:approx} are converted into yields. Figure 2 shows that: (1) differences between simulated and exact yields are small, and (2) variability between simulated yields decreases when $N_{sim}$ increases from 50 to 500. The influence of $n$ is portrayed in Figure 3. In practice, we can use these kind of graphs to decide on suitable choices for $n$ and $N_{sim}$. Simply select a pair $(n,N_{sim})$ and verify whether the computed quantity is insensitive to changes therein.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;YieldT5_various_n.png&#34; data-caption=&#34;Figure 3: The simulated bond price for a maturity of 5 years. The true bond price, $P(0,5)\approx 7.54%$, is independent of $n$ (red). The  black dots are obtained by Monte Carlo simulation. The error bars are $\pm 1.96 \hat{\sigma}$ with $\hat\sigma$ denoting the standard error among $50$ replications.&#34;&gt;
&lt;img data-src=&#34;YieldT5_various_n.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 3: The simulated bond price for a maturity of 5 years. The true bond price, $P(0,5)\approx 7.54%$, is independent of $n$ (red). The  black dots are obtained by Monte Carlo simulation. The error bars are $\pm 1.96 \hat{\sigma}$ with $\hat\sigma$ denoting the standard error among $50$ replications.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;p&gt;Recall $\Lambda(t) = \int_0^t e^{-\kappa s} ds = \frac{1}{\kappa}\left[ 1 - e^{-\kappa t} \right]$ and note how it implies
\begin{equation}
\begin{aligned}
\int_x^T e^{-\kappa(s-x)} ds
&amp;amp;= e^{\kappa x} \Big[ \Lambda(T) - \Lambda(x) \Big]
= \tfrac{1}{\kappa} e^{\kappa x} \left[ e^{-\kappa x} - e^{-\kappa T}\right] \\&lt;br&gt;
&amp;amp; = \Lambda(T-x).
\end{aligned}
\label{eq:MyINT}
\tag{A1}
\end{equation}
The integral in this last equation will be used at several occasions in the derivations below. Using the expression for $r(t)$, we have
$$
\begin{aligned}
\int_t^T r(s) ds
&amp;amp;= \int_t^T e^{-\kappa s} r(0) ds \\&lt;br&gt;
&amp;amp; \quad + \int_t^T \int_0^s e^{-\kappa(s-x)} \kappa \theta dx ds + \int_t^T \int_0^s e^{-\kappa (s-x)}\sigma dW(x) ds \\&lt;br&gt;
&amp;amp;=: I+II+III.
\end{aligned}
$$
We will develop these three contributions separately. Term $I$ is easiest. Using \eqref{eq:MyINT}, we find
$$
I = \int_t^T e^{-\kappa s} r(0) ds = r(0) e^{-\kappa t} \int_t^T e^{ -\kappa(s-t) }ds = r(0) e^{-\kappa t} \Lambda(T-t).
$$&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;integrationarea.png&#34; data-caption=&#34;Figure 4: The separation of the area of integration into two parts.&#34;&gt;
&lt;img data-src=&#34;integrationarea.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 4: The separation of the area of integration into two parts.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Term $II$ requires a change in the order of integration. Inspecting the area of integration in Figure 4, we arrive at the following integral relation
$$
\begin{aligned}
II &amp;amp;= \int_t^T \int_0^s e^{-\kappa(s-x)} \kappa \theta dx ds \\&lt;br&gt;
&amp;amp;= \int_0^t \int_t^T e^{-\kappa(s-x)} \kappa \theta ds dx + \int_t^T \int_x^T e^{-\kappa(s-x)} \kappa \theta ds dx \\&lt;br&gt;
&amp;amp;= \int_0^t \int_t^T e^{-\kappa(s-t+t-x)} \kappa \theta ds dx  + \kappa \theta \int_t^T \Lambda(T-x) dx \\&lt;br&gt;
&amp;amp;= \int_0^t e^{-\kappa(t-x)}\kappa \theta dx , \Lambda(T-t) + \kappa \theta \int_t^T \Lambda(T-x) dx.
\end{aligned}
$$
We apply exactly the same change in the order of integration to term $III$. The result is
$$
\begin{aligned}
III &amp;amp;= \int_t^T \int_0^s e^{-\kappa (s-x)}\sigma dW(x) ds \\&lt;br&gt;
&amp;amp;= \int_0^t \int_t^T e^{-\kappa (s-x)}\sigma ds dW(x) + \int_t^T \int_x^T e^{-\kappa (s-x)}\sigma ds dW(x) \\&lt;br&gt;
&amp;amp;= \int_0^t \int_t^T e^{-\kappa (s-t+t-x)}\sigma ds dW(x) + \sigma \int_t^T \Lambda(T-x)dW(x) \\&lt;br&gt;
&amp;amp;= \int_0^t e^{-\kappa(t-x)} \sigma dW(x) , \Lambda(T-t) + \sigma \int_t^T \Lambda(T-x)dW(x).
\end{aligned}
$$
If terms $I$&amp;ndash;$III$ are added together, then we see that $\int_x^T e^{-\kappa(s-x)} ds$ is an affine transformation of the prevailing short rate $r(t)$, i.e.
$$
\begin{aligned}
\int_t^T &amp;amp;r(s) ds \\&lt;br&gt;
&amp;amp; = I+II+III \\&lt;br&gt;
&amp;amp;= \left[ r(0) e^{-\kappa t} +  \int_0^t e^{-\kappa(t-x)}\kappa \theta dx+\int_0^t e^{-\kappa(t-x)} \sigma dW(x) \right] \Lambda(T-t) \\&lt;br&gt;
&amp;amp;\quad+ \kappa \theta \int_t^T \Lambda(T-x) dx+\sigma \int_t^T \Lambda(T-x)dW(x) \\&lt;br&gt;
&amp;amp;= r(t) \Lambda(T-t)+\kappa \theta \int_t^T \Lambda(T-x) dx+\sigma \int_t^T \Lambda(T-x)dW(x).
\end{aligned}
$$
Conditional on all information up to time t, i.e. conditional on $\mathcal{F}_t$, the first two terms are deterministic. Moreover, since (1) increments of Brownian motions are independent of the current value and (2) stochastic integrals are normally distributed, we know that $ \left.\int_t^T r(s) ds\right| \mathcal{F}_t$ is normally distributed with mean
$$
\begin{aligned}
r(t) \Lambda(T-t) &amp;amp;+\kappa \theta \int_t^T \Lambda(T-x) dx \\&lt;br&gt;
&amp;amp;= r(t) \Lambda(T-t)+\kappa \theta \int_0^{T-t} \Lambda(x) dx \\&lt;br&gt;
&amp;amp;= r(t) \Lambda(T-t) + \theta\Big[(T-t) - \Lambda(T-t) \Big]
\end{aligned}
$$
and by Itô isometry a variance of
$$
\begin{aligned}
\sigma^2 \int_t^T &amp;amp;\big[ \Lambda(T-x) \big]^2 dx = \sigma^2 \int_0^{T-t} \big[ \Lambda(x) \big]^2 dx \\&lt;br&gt;
&amp;amp;= \tfrac{\sigma^2}{\kappa^2} \int_0^{T-t} \left( 1-2 e^{-\kappa x} + e^{-2\kappa x} \right) dx \\&lt;br&gt;
&amp;amp;= \tfrac{\sigma^2}{\kappa^2} \left[ (T-t) -2 \int_0^{T-t} e^{-\kappa x} dx + \tfrac{1}{2} \int_0^{2(T-t)} e^{-\kappa x} dx  \right] \\&lt;br&gt;
&amp;amp;= \tfrac{\sigma^2}{\kappa^2} \left[  (T-t) -2 \Lambda(T-t) + \tfrac{1}{2} \Lambda\Big(2(T-t) \Big) \right].
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;N. H. Bingham and R. Kiesel (2004), &lt;em&gt;Risk-neutral Valuation&lt;/em&gt;, Springer Finance.&lt;/p&gt;
&lt;p&gt;D. Filipović (2009), &lt;em&gt;Term-structure Models&lt;/em&gt;, Springer Finance.&lt;/p&gt;
&lt;p&gt;T. Mikosch (1998), &lt;em&gt;Elementary Stochastic Calculus with Finance in View&lt;/em&gt;, World Scientific.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Generalized Yule–Walker Estimation for Spatio-temporal Models</title>
      <link>https://HannoReuvers.github.io/publication/teun-paper/</link>
      <pubDate>Tue, 01 Sep 2020 00:02:00 +0200</pubDate>
      <guid>https://HannoReuvers.github.io/publication/teun-paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cointegrating Polynomial Regressions with Power Law Trends: A New Angle on the Environmental Kuznets Curve</title>
      <link>https://HannoReuvers.github.io/publication/flexible-trend/</link>
      <pubDate>Sat, 01 Aug 2020 00:02:00 +0200</pubDate>
      <guid>https://HannoReuvers.github.io/publication/flexible-trend/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient Estimation by Fully Modified GLS with an Application to the Environmental Kuznets Curve</title>
      <link>https://HannoReuvers.github.io/publication/ekc-paper/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0200</pubDate>
      <guid>https://HannoReuvers.github.io/publication/ekc-paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Statistical Analysis of Time Trends in Atmospheric Ethane</title>
      <link>https://HannoReuvers.github.io/publication/liege-paper/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0200</pubDate>
      <guid>https://HannoReuvers.github.io/publication/liege-paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Focused Information Criterion for Locally Misspecified Vector Autoregressive Models</title>
      <link>https://HannoReuvers.github.io/publication/fic-paper/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0100</pubDate>
      <guid>https://HannoReuvers.github.io/publication/fic-paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Climate Econometrics 1: the greenhouse effect</title>
      <link>https://HannoReuvers.github.io/post/climate-econometrics-1/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0200</pubDate>
      <guid>https://HannoReuvers.github.io/post/climate-econometrics-1/</guid>
      <description>&lt;h2 id=&#34;short-intro&#34;&gt;Short intro&lt;/h2&gt;
&lt;p&gt;We will first have to understand the origins of global warming before we can attempt to model the past and predict the future. Two key players are displayed in Figure 1. We see how both the global temperature and the global carbon dioxide&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; ($\text{CO}_2$) concentration have been rising steadily over the last 40 years. The connection between these two graphs is commonly known as the &amp;ldquo;greenhouse effect&amp;rdquo;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;tempCO2.png&#34; data-caption=&#34;Figure 1: Yearly temperature anomalies and carbon dioxide concentrations for the time period 1980-2019. (a) The temperature anomaly is the temperature difference with respect to the 1951-1980 global average (source: https://climate.nasa.gov) . (b) Global $\text{CO}_2$ concentrations as obtained from NOAA Global Monitoring Laboratory.&#34;&gt;
&lt;img data-src=&#34;tempCO2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 1: Yearly temperature anomalies and carbon dioxide concentrations for the time period 1980-2019. (a) The temperature anomaly is the temperature difference with respect to the 1951-1980 global average (source: &lt;a href=&#34;https://climate.nasa.gov&#34;&gt;https://climate.nasa.gov&lt;/a&gt;) . (b) Global $\text{CO}_2$ concentrations as obtained from &lt;a href=&#34;https://www.esrl.noaa.gov/gmd/ccgg/trends/global.html&#34;&gt;NOAA Global Monitoring Laboratory&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;How exactly are these tiny $\text{CO}_2$ molecules affecting the temperature on earth? The answer to this question is the topic of this post. We will look at two models: the bare-rock model and the layer model.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; Both models should be viewed as toy models as they describe a very much simplified version of reality. Indeed, the earth will be treated as a perfect sphere with the same temperature everywhere. The heat flows and spatial variation on our planet, e.g. ocean currents and altitude differences, are simply ignored. Notwithstanding their simplicity, these models will still provide all necessary ingredients to understand the origins of the greenhouse effect.&lt;/p&gt;
&lt;h2 id=&#34;the-bare-rock-model&#34;&gt;The bare-rock model&lt;/h2&gt;
&lt;p&gt;The bare-rock model is a stylized model to explain the temperature on earth. This earth is represented by a large ball with a single uniform temperature. An energy balance between incoming sunlight and heat loss to space determines the temperature on our planet.&lt;/p&gt;
&lt;h4 id=&#34;energy-received-by-the-earth&#34;&gt;Energy received by the earth&lt;/h4&gt;
&lt;p&gt;When taking a walk on a beautiful summer day, you will feel the warmth of the sun. Similarly, the earth is continuously heated by the sun. How much energy is our spherical earth receiving from the sun? Let $I_{sun}$ denote the energy influx of sunlight, i.e. the amount of incoming energy per second and per $\text{m}^2$. To calculate the amount of energy absorbed per second, we have to multiply $I_{sun}$ by the area of the earth which is absorbing sunlight. An inspection of Figure 2 shows that the relevant area equals $\pi r_{earth}^2$. We additionally have to account for the fact that snow, ice, and clouds will reflect a fraction of the incoming sunlight back to space. This fraction is called the albedo and is generally denoted by $\alpha$. If a fraction $\alpha$ is reflected, then there remains a fraction $1-\alpha$ to be absorbed. Overall, the effective incoming flux of solar energy is:
\begin{equation}
\label{eq1}
F_{in} [\text{Watt}]= I_{sun} \times \pi r_{earth}^2 \times (1-\alpha).
\end{equation}
The units of this energy flux is Watt (also abbreviated by W).&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;earthwithshadow.png&#34; data-caption=&#34;Figure 2: The earth will leave a circular shadow on a screen placed behind it. The area of this circle is $\pi r_{earth}^2$, where $r_{earth}^{}$ is the radius of our spherical planet.&#34;&gt;
&lt;img data-src=&#34;earthwithshadow.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 2: The earth will leave a circular shadow on a screen placed behind it. The area of this circle is $\pi r_{earth}^2$, where $r_{earth}^{}$ is the radius of our spherical planet.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;emitted-energy&#34;&gt;Emitted energy&lt;/h4&gt;
&lt;p&gt;The earth is not only absorbing energy, it is also emitting energy. The emitted energy is in the form of so-called black-body radiation. All warm objects emit this type of radiation and both the dominant wavelengths of the emitted light and the total energy flux depend on the body&#39;s temperature. Warmer objects emit radiation of shorter wavelength. This explains why we see a shining sun and a glow on the iron that has just been taken out of the fire. Our earth is much colder than either of these objects and is thus radiating light of much longer wavelength. These wavelenghts are too long for the human eye to observe.&lt;/p&gt;
&lt;p&gt;Of particular interest for our bare-rock model is the energy flux from a black-body radiator. The amount of energy emitted per second and per $\text{m}^2$ is governed by the Stefan-Boltzmann law:
\begin{equation}
I_{black\text{ }body} = \sigma T^4,
\end{equation}
where $\sigma$ is the Stefan-Boltzmann constant (a constant of nature) and $T$ denotes the temperature in Kelvins.&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; Each square meter of surface area can emit black-body radiation. Given that the total surface area of the earth is $4 \pi r_{Earth}^2$, the outgoing energy flux of our planet is
\begin{equation}
F_{out} [\text{Watt}] = \sigma T^4\times 4 \pi r_{earth}^2.
\end{equation}&lt;/p&gt;
&lt;h4 id=&#34;finding-a-balance&#34;&gt;Finding a balance&amp;hellip;&lt;/h4&gt;
&lt;p&gt;Depending on the magnitudes of $F_{in}$ and $F_{out}$, we find ourselves in one of the following three situations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$F_{in} &amp;gt; F_{out}$: The earth is receiving more energy than it is sending back to space. The accumulation of energy will cause the earth to heat up.&lt;/li&gt;
&lt;li&gt;$F_{in} &amp;lt; F_{out}$: The nett heat flux is negative, that is, our spherical earth is steadily losing energy and hence cooling down.&lt;/li&gt;
&lt;li&gt;$F_{in} = F_{out}$: The incoming and outgoing energy flux are exactly equal. There is neither heat gain nor heat loss and the temperature will remain constant.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If $F_{in} = F_{out}$, then our earth is in equilibrium &amp;ndash; or in steady-state &amp;ndash; and this provides a good indication of the earth&#39;s temperature according to our bare-rock model. Inserting the expressions for $F_{in}$ and $F_{out}$, this equilibrium temperature turns out to be:
$$
T_{equilibrium} = \left(  \frac{ (1-\alpha) I_{sun}}{4 \sigma} \right)^{1/4}.
$$
If we plug in some representative numbers,&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; then this ball-shaped earth will have an equilibrium temperature of 254 K (or -19 $^\circ$C). This earth is too cold&amp;hellip; we better add a blanket!&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;layermodel.png&#34; data-caption=&#34;Figure 3: A schematic representation of the layer model. Sunlight shines unhindered through the layer and warms the earth. The radiation from the earth is trapped by the layer and re-emitted towards both the earth and the universe.&#34;&gt;
&lt;img data-src=&#34;layermodel.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 3: A schematic representation of the layer model. Sunlight shines unhindered through the layer and warms the earth. The radiation from the earth is trapped by the layer and re-emitted towards both the earth and the universe.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;the-layer-model&#34;&gt;The layer model&lt;/h2&gt;
&lt;p&gt;Our blanket is an additional layer placed between the earth and the sun, see Figure 3. This layer is special. The incoming light from the sun will pass straight through it, but the outgoing radiation from the earth is absorbed and subsequently re-emitted. Our new task is to determine the resulting equilibrium temperature of the earth. We again match the incoming and outgoing energy fluxes but now we should do so for the earth and the layer simulaneously. Additionally, we set the sun&#39;s energy flux per unit of area equal to $I_{sunlight}= \frac{1}{4} (1-\alpha) I_{sun}$.&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; The equilibrium equations are
$$
I_{sunlight} + I_{layer,down} = I_{earth} \iff \frac{1}{4} (1-\alpha) I_{sun} + \sigma T_{layer}^4 = \sigma T_{earth}^4
$$
and
$$
I_{earth} = I_{layer,up} + I_{layer,down} \iff \sigma T_{earth}^4 =  \sigma T_{layer}^4 +  \sigma T_{layer}^4
$$
for the earth and layer, respectively. We can solve these two equations and find the earth&#39;s temperature in this layer model:
$$
T_{earth} =  \left(  \frac{ 2 (1-\alpha) I_{sun}}{4 \sigma} \right)^{1/4}.
$$
A quick comparison with $T_{equilibrium}$ shows that the new temperature $T_{earth}$ is a factor $2^{1/4}\approx 1.1892$ higher. The new temperature on earth is now 302 K (or about 29 $^\circ$C). The current temperature is admittedly a bit too high but we will also see that $\text{CO}_2$ is not behaving as extreme as our layer. That is, this greenhouse gas does not absorb all the radiation coming from earth. Most importantly, we can now understand why the additional layer works as a blanket. It creates an additional energy flow $I_{layer,down}$ back to earth that causes the earth to warm up.&lt;/p&gt;
&lt;h2 id=&#34;the-analogy-between-carbon-dioxide-and-the-layer&#34;&gt;The analogy between carbon dioxide and the layer&lt;/h2&gt;
&lt;p&gt;Carbon dioxide behaves similarly to our layer. The incoming sunlight from our warm sun has a relatively short wavelenght and this light will not (or hardly) interact with the $\text{CO}_2$ molecules in the air. As the earth is much colder than the sun, it will emit black body radiation with longer wavelength. This radiation is partly(!) absorbed by the carbon dioxide molecules in the air and released into any arbitrary direction. This process creates an energy flow back to earth much the same as $I_{layer,down}$ in Figure 3. The higher the $\text{CO}_2$ concentration in the atmosphere, the more energy is channeled back to earth, and the more global warming. Finally, do note that there is nothing special about carbon dioxide. The warming mechanism behind the other greenhouse gasses is the same!&lt;/p&gt;
&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Of course, there are also other greenhouse gasses (e.g. water vapor, methane, nitrous oxide, and ozone). Feel free to read &amp;ldquo;greenhouse gasses&amp;rdquo; instead of &amp;ldquo;carbon dioxide&amp;rdquo; throughout this post. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The discussions of the bare-rock and layer model are inspired by chapters 2-4 from &lt;i&gt;Global Warming: Understanding the Forecast&lt;/i&gt; by &lt;a href=&#34;https://geosci.uchicago.edu/people/david-archer/&#34;&gt;David Archer&lt;/a&gt;. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;1 Watt is equal to 1 Joule per second. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;It is easy to convert a temperature from degree Celsius to Kelvin, just add 273.15. For example, water freezes at 0 $^\circ$C or equivalently 273.15 K. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The numerical value of the Stefan-Boltzmann constant is $\sigma = 5.670\times 10^{-8} \text{ J} \text{ s}^{-1} \text{ m}^{-2} \text{ K}^{-4}$. For $\alpha$ and $I_{sun}$, we follow the book by David Archer, and use $\alpha=0.3$ and $I_{sun}= 1350 \text{ W m}^{-2}$. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The factor $(1-\alpha)$ accounts for the earth&#39;s albedo. Subsequently recall that: (1) a surface area of $\pi r_{earth}^2$ is facing the sun and thus absorbing energy, and (2) that the total surface area of the earth is $4 \pi r_{earth}^2$. The effective energy influx &lt;i&gt;per unit of surface area&lt;/i&gt; thus requires a factor $\frac{1}{4}$. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Which computational language to choose?</title>
      <link>https://HannoReuvers.github.io/post/which-language/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0200</pubDate>
      <guid>https://HannoReuvers.github.io/post/which-language/</guid>
      <description>&lt;h2 id=&#34;short-intro&#34;&gt;Short intro&lt;/h2&gt;
&lt;p&gt;Let us compare the performance of C++, Matlab, Python and R when performing typical econometrical/statistical tasks. The word ‘typical’ is quite ambiguous since computational requirements can vary substantially between subfields. As such, I will consider a setting that is standard yet also computationally demanding: &lt;em&gt;bootstrap inference on the Durbin-Watson test statistic&lt;/em&gt;. This setting is easy to understand and the computational performance of the programming languages is probably representative for a wide range of Monte Carlo (MC) simulations. MC simulations are very often needed in research when relying on frequentist statistics.&lt;/p&gt;
&lt;h2 id=&#34;the-durbin-watson-test-statistic&#34;&gt;The Durbin-Watson test statistic&lt;/h2&gt;
&lt;p&gt;Consider a standard multivariate regression model $y_t^{} = \mathbf{x}_t&amp;rsquo; \boldsymbol \beta + u_t^{}$ for $t = 1, 2, . . . , T$. The presence of serial correlation in $u_t$ will render standard ordinary least squares (OLS) inference invalid. The Durbin-Watson test statistic was one of the first test statistics to test for the presence of serial correlation.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; It is computed as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Calculate the OLS estimator $\widehat{\boldsymbol \beta}=(\mathbf{X}&#39;\mathbf{X})^{-1} \mathbf{X}&#39;\mathbf{y}$.&lt;/li&gt;
&lt;li&gt;Obtain the residuals $\widehat{u}_t=y_t- \mathbf{x}_t&amp;rsquo;\widehat{\boldsymbol \beta}$ for $t = 1, 2, . . . , T$.&lt;/li&gt;
&lt;li&gt;The Durbin-Watson test statisic is now computed as $DW= \frac{\sum_{t=2}^T \left(\widehat{u}_t-\widehat{u}_{t-1}\right)^2  }{\sum_{t=1}^T \widehat{u}_t^2}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Simple algebraic manipulations show that the Durbin-Watson statistic can be expressed in terms of the first order sample autocorrelation $\widehat{\rho}= \frac{\sum_{t=2}^T \widehat{u}_t\widehat{u}_{t-1} }{\sum_{t=1}^T \widehat{u}_t^2}$, namely $DW=2(1-\widehat{\rho})-\frac{\widehat{u}_1^2+\widehat{u}_T^2}{\sum_{t=1}^T \widehat{u}_t^2}$. Based on this result we would expect outcomes close to the value 2 if first order autocorrelation is absent. Values different from 2 indicate serial correlation. Using the Durbin-Watson statistic to carry out a formal hypothesis test is more complicated because its (asymptotic) distribution is difficult to derive. This motivates the use of a bootstrap approach.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; The specific steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate $\boldsymbol \beta$ by OLS and compute the residual series ${\widehat{u}_t }$ as well as the Durbin-Watson statistic $DW$.&lt;/li&gt;
&lt;li&gt;Compute $\widehat{\rho}$ (see the definition above) and $\widehat{\varepsilon,}_t=\widehat{u}_t- \widehat{\rho} \widehat{u}_{t-1}$ for $t=2,3,\ldots,T$.&lt;/li&gt;
&lt;li&gt;Draw bootstrap errors $\varepsilon_t^\star$ by resampling with replacement from $\{\widehat{\varepsilon}_2,\widehat{\varepsilon,}_3,\ldots,\widehat{\varepsilon,}_T \}$. Store these in the vector $\boldsymbol \varepsilon^\star=[\varepsilon_1^\star,\varepsilon_2^\star,\ldots,\varepsilon_T^\star]&amp;rsquo; $.&lt;/li&gt;
&lt;li&gt;Set $\mathbf{u}^\star=\boldsymbol \varepsilon^\star$ (to impose the null hypothesis in the bootstrap sample), construct the bootstrap sample $\mathbf{y}^\star=\mathbf{X}\widehat{\boldsymbol \beta}+\mathbf{u}^\star$, and compute the DW statistic from $\mathbf{y}^\star$.&lt;/li&gt;
&lt;li&gt;Repeat steps 3-4 $B$ times and store these outcomes in a list $\left\{ DW_b^\star \right\}_{b=1}^B$.&lt;/li&gt;
&lt;li&gt;For a given significance level $\alpha$, we define $q_{\alpha/2}^\star$ and $q_{1-\alpha/2}^\star$ as the $\alpha/2$ and $1-\alpha/2$ empirical quantiles of $\left\{ DW_b^\star \right\}_{b=1}^B$. We reject the null hypothesis of no serial correlation if either $DW &amp;lt; q_{\alpha/2}^\star$ or if $DW&amp;gt;q_{1-\alpha/2}^\star$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;the-simulation-setting&#34;&gt;The simulation setting&lt;/h2&gt;
&lt;p&gt;We consider a simple simulation setting to compare the different programming environments, namely&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
y_t &amp;amp;= \mathbf{x}_t&amp;rsquo;\boldsymbol{\beta}+u_t \\\&lt;br&gt;
u_t &amp;amp;= \rho u_{t-1}+\varepsilon_t, \qquad \qquad t=1,2,\ldots,T,
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $T=\{100,250,500\}$. The fixed regressor matrix $\mathbf{X}$ contains a column of ones and a column that repeats the sequence $0,1,0,-1$ (orthogonal design). The error terms ${\epsilon_t}$ are standard normal and a presample of 50 observations is used to remove the influence of the initial values. We set $\boldsymbol \beta=[1,2]&amp;rsquo; $ and vary the value for $\rho$ over the grid $[0,0.025,\ldots,0.5]&amp;rsquo; $ (21 grid points). The resulting power curves are depicted in Figure 1. Overall, we use 1000 Monte Carlo replications where each replication uses $B=499$ bootstrap resamples.&lt;/p&gt;
&lt;h2 id=&#34;results-and-discussion&#34;&gt;Results and discussion&lt;/h2&gt;
&lt;p&gt;Simulations are performed on a Macbook with a 2.6 GHz Intel Core i5 processor. I used serial computing. The computational times are reported in Table 1.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;comp_time_table.png&#34; data-caption=&#34;Table 1: Absolute and relative (using C&amp;#43;&amp;#43; as the benchmark) computational times in seconds.&#34;&gt;
&lt;img data-src=&#34;comp_time_table.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Table 1: Absolute and relative (using C++ as the benchmark) computational times in seconds.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The conclusions from this table are unambiguous. The C++ implementation is significantly faster than the scientific programming environments of Matlab and R. However, this should be balanced against the fact that the programming itself is much easier/faster in the user-friendly environments offered by either of these languages. The results do not promote the use of Python. The computational times are longest and coding is less comfortable because matrix algebra requires an additional library (such as numpy). My conclusions are qualitatively the same as those in Borağan Aruoba and Fernández-Villaverde (2015).&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;J. Durbin and G. S. Watson (1971), &lt;em&gt;Testing for Serial Correlation in Least Squares Regression&lt;/em&gt;, Biometrika &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;J. Jeong and S. Chung (2001), &lt;em&gt;Bootstrap Tests for Autocorrelation&lt;/em&gt;, Computational Statistics and Data Analysis &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;S.  Borağan Aruoba and J. Fernández-Villaverde (2015), &lt;em&gt;A Comparison of Programming Languages in Macroeconomics&lt;/em&gt;, Journal of Economic Dynamics &amp;amp; Control &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
